{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Batwing Machine Learning Library","text":"<p>Most ML work doesn\u2019t fail because models are hard. It fails because the data-to-model pipeline is messy, inconsistent, and constantly rewritten across notebooks, repos, and teams.</p> <p>Batwing-ML is a lightweight toolkit that fixes that. It gives you a clean, repeatable workflow for tabular ML experiments \u2014 the sort of structure every project needs, but almost nobody has the patience to build from scratch.</p> <p>Think of it as the missing middle layer between raw pandas code and a full MLOps stack: fast to set up, transparent to debug, and disciplined enough to scale.</p>"},{"location":"#what-problem-does-it-solve","title":"What problem does it solve?","text":"<p>If you work with tabular data long enough, you know the story:</p> <ul> <li>Every dataset arrives messy.  </li> <li>Every experiment needs its own preprocessing.  </li> <li>Feature engineering gets copy-pasted from older notebooks.  </li> <li>Model tuning turns into a maze of duplicated scripts.  </li> <li>Evaluation becomes whatever the last person wrote at 2AM.</li> </ul> <p>Teams end up with inconsistent pipelines, hidden data leakage, undocumented transformations, ad-hoc hyperparameter searches, and results that can\u2019t be reproduced a month later.</p> <p>Batwing-ML gives you a way out by providing:</p> <ul> <li>A single, predictable flow from raw DataFrame \u2192 validated features \u2192 tuned model \u2192 evaluated results.  </li> <li>Structured metadata that records every transformation and choice.  </li> <li>Notebook-first utilities for quick EDA, exploration, and diagnostics.  </li> <li>Composable functions that stay faithful to sklearn and pandas instead of inventing a new ecosystem.  </li> </ul> <p>It\u2019s the workflow you wish you had the first time you shipped a model.</p>"},{"location":"#who-is-it-for","title":"Who is it for?","text":"<p>Batwing-ML is built for:</p> <ul> <li>Data scientists who want fast experiments without drowning in boilerplate.  </li> <li>ML engineers who want reproducible steps, clean audit trails, and debuggable pipelines.  </li> <li>Students and researchers who want to run rigorous experiments without reinventing preprocessing or tuning logic.  </li> <li>Teams who want consistency across notebooks and contributors.</li> </ul> <p>If your work touches tabular ML, you\u2019ll probably feel at home.</p>"},{"location":"#why-batwing-ml-feels-different","title":"Why Batwing-ML feels different","text":"<p>Here\u2019s the thing: most \u201chelper\u201d libraries try to abstract everything away. Batwing-ML does the opposite \u2014 it\u2019s transparent by design.</p> <ul> <li>Every function returns data + metadata so you know exactly what happened.  </li> <li>Nothing is hidden behind classes or magic fit/transform pipelines.  </li> <li>You always keep control of your DataFrame.  </li> <li>Everything is powered by standard tools: pandas, numpy, sklearn, Optuna.</li> </ul> <p>You get structure without losing flexibility.</p>"},{"location":"#whats-inside","title":"What\u2019s inside","text":""},{"location":"#1-data-validation","title":"1. Data validation","text":"<p>Catch schema issues, missing columns, broken dtypes, unexpected ranges, and category mismatches \u2014 with a clean audit report you can log or inspect.</p>"},{"location":"#2-eda-diagnostics","title":"2. EDA &amp; diagnostics","text":"<p>Quick summaries, feature distributions, correlations, leakage signals, and light model probes.</p>"},{"location":"#3-preprocessing","title":"3. Preprocessing","text":"<p>Imputation, encoding, scaling, deduping, junk-column detection, and clean previews built for notebooks.</p>"},{"location":"#4-feature-engineering","title":"4. Feature engineering","text":"<p>Automatic selection (variance, MI, model-based), synthetic features, transformations, PCA, and per-step metadata.</p>"},{"location":"#5-hyperparameter-tuning","title":"5. Hyperparameter tuning","text":"<p>Optuna-powered tuning across classification, multiclass, and regression \u2014 with clean result dictionaries and optional fast-mode sampling.</p>"},{"location":"#6-nested-cross-validation","title":"6. Nested cross-validation","text":"<p>Honest model comparison with minimal code.</p>"},{"location":"#7-evaluation","title":"7. Evaluation","text":"<p>Consistent metrics, plots, summaries, and export helpers for final model reporting.</p> <p>Each part works alone, but together they form a complete experimentation loop.</p>"},{"location":"#a-quick-end-to-end-example","title":"A quick end-to-end example","text":"<pre><code>import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom batwing_ml import (\n    validate_and_clean_data,\n    preprocess_dataframe,\n    feature_engineering,\n    hyperparameter_tuning_classification,\n)\n\ndf = pd.read_csv(\"data.csv\")\n\n# 1) Validate &amp; clean\ndf_clean, audit = validate_and_clean_data(df)\n\n# 2) Preprocess\nX_processed, prep_steps = preprocess_dataframe(\n    df_clean.drop(columns=[\"label\"]),\n    encode=\"onehot\",\n    scale=\"standard\",\n    return_steps=True,\n)\n\ny = df_clean[\"label\"]\n\n# 3) Engineer features\nX_fe, fe_meta = feature_engineering(X_processed)\n\n# 4) Tune a model\nresults = hyperparameter_tuning_classification(\n    X=X_fe,\n    y=y,\n    model_class=RandomForestClassifier,\n    param_grid={\n        \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),\n        \"max_depth\":    lambda t: t.suggest_int(\"max_depth\", 3, 20),\n    },\n    scoring=\"f1_macro\",\n)\n\nprint(results[\"best_params\"])\n\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through a complete workflow with Batwing-ML \u2013 starting from a raw CSV and ending with a tuned, evaluated model.</p> <p>You can copy this into a notebook and run it top to bottom, or adapt each step into your own pipeline.</p>"},{"location":"getting-started/#1-installation","title":"1. Installation","text":"<p>First, install Batwing-ML from PyPI.</p> <pre><code>pip install batwing-ml\n</code></pre> <p>Batwing-ML builds on the usual Python data stack. If you\u2019re in a fresh environment, it\u2019s safe to also install:</p> <pre><code>pip install pandas numpy scikit-learn optuna\n</code></pre> <ul> <li>pandas / numpy \u2013 DataFrame and numerical operations.</li> <li>scikit-learn \u2013 Models, metrics, and utilities.</li> <li>optuna \u2013 Used under the hood for hyperparameter tuning.</li> </ul>"},{"location":"getting-started/#2-load-your-data","title":"2. Load your data","text":"<p>Start with your dataset as a pandas DataFrame. This can come from CSV, SQL, Parquet, etc.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\n</code></pre> <p>For this walkthrough, we\u2019ll assume your target column is called <code>label</code> and all other columns are features.</p> <ul> <li>You don\u2019t need to clean anything manually yet.</li> <li>Missing values, mixed types, and messy columns are expected at this stage.</li> </ul>"},{"location":"getting-started/#3-validate-clean","title":"3. Validate &amp; clean","text":"<p>Before modeling, it\u2019s important to know what\u2019s wrong with your data and to fix simple issues systematically.</p> <pre><code>from batwing_ml import validate_and_clean_data\n\ndf_clean, audit = validate_and_clean_data(df)\n</code></pre> <p>What this does:</p> <ul> <li>Standardizes dtypes where possible.</li> <li>Flags and optionally drops obviously broken columns (empty, constant, etc.).</li> <li>Handles basic range / consistency checks.</li> <li> <p>Returns two things:</p> </li> <li> <p><code>df_clean</code> \u2013 the cleaned DataFrame you will use downstream.</p> </li> <li><code>audit</code> \u2013 a structured dict describing what changed.</li> </ul> <p>You can inspect <code>audit</code> to see, for example:</p> <ul> <li>Which columns were dropped or fixed.</li> <li>How many rows were affected by cleaning.</li> <li>Any schema issues that might need manual review.</li> </ul> <p>This makes your preprocessing auditable instead of \u201cwe tweaked stuff in a notebook and forgot what happened.\u201d</p>"},{"location":"getting-started/#4-quick-eda","title":"4. Quick EDA","text":"<p>Next, get a compact overview of your cleaned data. Batwing-ML includes helpers that produce notebook-friendly summaries.</p> <pre><code>from batwing_ml import summary_dataframe\n\nsummary = summary_dataframe(df_clean)\nsummary  # renders nicely in notebooks\n</code></pre> <p>Typical things you\u2019ll see in <code>summary</code>:</p> <ul> <li>Column types (numeric, categorical, datetime, etc.).</li> <li>Missing value counts and proportions.</li> <li>Basic stats (min, max, mean, std) for numeric columns.</li> <li>Cardinality and top values for categoricals.</li> </ul> <p>This gives you a quick sanity check before you dive into modeling.</p>"},{"location":"getting-started/#5-preprocess-and-encode","title":"5. Preprocess and encode","text":"<p>Now we turn the cleaned DataFrame into a model-ready feature matrix: imputed, encoded, and scaled.</p> <pre><code>from batwing_ml import preprocess_dataframe\n\nX_processed, prep_steps = preprocess_dataframe(\n    df_clean.drop(columns=[\"label\"]),\n    encode=\"onehot\",\n    scale=\"standard\",\n    drop_missing_thresh=0.3,\n    return_steps=True,\n)\n\ny = df_clean[\"label\"]\n</code></pre> <p>What each argument does:</p> <ul> <li><code>df_clean.drop(columns=[\"label\"])</code> \u2013 use all non-target columns as features.</li> <li><code>encode=\"onehot\"</code> \u2013 one-hot encode categorical features.</li> <li><code>scale=\"standard\"</code> \u2013 standardize numeric features (zero mean, unit variance).</li> <li><code>drop_missing_thresh=0.3</code> \u2013 drop columns with more than 30% missing values.</li> <li><code>return_steps=True</code> \u2013 return a metadata object describing what preprocessing was applied.</li> </ul> <p>Outputs:</p> <ul> <li><code>X_processed</code> \u2013 a numerical feature matrix ready for modeling.</li> <li><code>prep_steps</code> \u2013 a record of encoders, scalers, dropped columns, etc. (useful for debugging or exporting your pipeline).</li> </ul>"},{"location":"getting-started/#6-feature-engineering","title":"6. Feature engineering","text":"<p>With a clean feature matrix, you can optionally apply feature engineering and selection.</p> <pre><code>from batwing_ml import feature_engineering\n\nX_fe, fe_meta = feature_engineering(\n    X_processed,\n    target=None,\n    mode=\"both\",\n)\n</code></pre> <p>Typical things this step can do (depending on configuration):</p> <ul> <li>Remove low-variance or redundant features.</li> <li>Apply supervised selection strategies (e.g., mutual information, model-based importance) when a target is provided.</li> <li>Create transformed or composite features.</li> </ul> <p>In this minimal example:</p> <ul> <li><code>mode=\"both\"</code> tells Batwing-ML to run both selection and transformation logic (where applicable).</li> <li><code>X_fe</code> is the final engineered feature matrix.</li> <li><code>fe_meta</code> records what was selected or generated, so you can inspect or log it later.</li> </ul> <p>If you prefer to keep things simple at first, you can skip this step and use <code>X_processed</code> directly.</p>"},{"location":"getting-started/#7-train-and-evaluate-a-model","title":"7. Train and evaluate a model","text":"<p>Finally, we split the data, tune a model, and evaluate it using Batwing-ML\u2019s tuning and evaluation helpers.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom batwing_ml import (\n    hyperparameter_tuning_classification,\n    evaluate_classification_model,\n)\n\n# 1) Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_fe, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y,\n)\n\n# 2) Hyperparameter tuning\n\ntuning = hyperparameter_tuning_classification(\n    X=X_train,\n    y=y_train,\n    model_class=RandomForestClassifier,\n    param_grid={\n        \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),\n        \"max_depth\":    lambda t: t.suggest_int(\"max_depth\", 3, 20),\n    },\n    scoring=\"f1_macro\",\n    fast_mode=True,\n)\n\nbest_model = tuning[\"best_model\"]\n\n# 3) Final evaluation\n\nmetrics = evaluate_classification_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    return_dict=True,\n)\n\nprint(metrics)\n</code></pre> <p>What\u2019s happening here:</p> <ol> <li>Train/test split \u2013 We keep 20% of the data as a hold-out test set and use stratification to preserve label distribution.</li> <li> <p>Hyperparameter tuning \u2013</p> </li> <li> <p><code>hyperparameter_tuning_classification</code> wraps Optuna.</p> </li> <li><code>param_grid</code> defines the search space (here, number of trees and max depth for a random forest).</li> <li><code>scoring=\"f1_macro\"</code> optimizes macro-averaged F1, which is good for imbalanced multiclass problems.</li> <li><code>fast_mode=True</code> can enable smaller subsets or fewer folds for quicker iteration.</li> <li>Evaluation \u2013 <code>evaluate_classification_model</code> computes metrics on both train and test sets and can optionally generate plots (confusion matrix, ROC/PR curves, etc., depending on configuration).</li> </ol> <p>The printed <code>metrics</code> dict gives you a compact summary you can log or compare across runs.</p>"},{"location":"getting-started/#8-where-to-go-next","title":"8. Where to go next","text":"<p>From here, you can:</p> <ul> <li>Swap in different models (e.g., gradient boosting, linear models, etc.).</li> <li>Use the regression or multiclass workflows if your task isn\u2019t simple binary classification.</li> <li> <p>Explore the Guides section for:</p> </li> <li> <p>A deeper classification workflow.</p> </li> <li>Regression pipelines.</li> <li>Handling large datasets and using fast-mode effectively.</li> <li>Notebook-focused visual previews and reporting.</li> </ul> <p>Batwing-ML is meant to be composable: you can keep this exact structure or plug individual steps into your own experiment scripts, pipelines, or MLflow runs.</p>"},{"location":"api/classification/","title":"Binary Classification API","text":"<p>This page documents the main entry points for binary classification experiments in Batwing-ML.</p> <ul> <li>Optuna-based hyperparameter tuning</li> <li>Nested cross-validation for unbiased model comparison</li> <li>Rich evaluation utilities (metrics + plots)</li> </ul> <p>If mkdocstrings fails to import something, adjust the dotted paths below to match your actual module layout.</p>"},{"location":"api/classification/#hyperparameter-tuning-binary-classification","title":"Hyperparameter tuning (binary classification)","text":"<pre><code>from batwing_ml import hyperparameter_tuning_classification\n</code></pre>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification","title":"<code>hyperparameter_tuning_classification(X=None, y=None, model_class=None, param_grid=None, scoring='accuracy', n_trials=50, cv_folds=5, stratified=True, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83d\udd27 Hyperparameter Tuning for Classification (via Optuna)</p> <p>Optimize hyperparameters of any binary classification model using Optuna\u2019s efficient  sampling. This function supports cross-validation, custom scoring, stratified sampling,  reproducibility, and returns the best trial summary and optionally the best fitted model.</p> <p>Ideal for automated model selection pipelines, leaderboard tuning, or experimentation  in research and production ML workflows.</p>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix with shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector with binary labels (0 or 1).</p> callable <p>A scikit-learn-style classifier class (e.g., <code>RandomForestClassifier</code>, <code>LogisticRegression</code>). Not an instance \u2013 must be the class itself.</p> dict <p>Dictionary mapping hyperparameter names to Optuna sampling functions. Example:     {         \"C\": lambda t: t.suggest_float(\"C\", 0.01, 10, log=True),         \"penalty\": lambda t: t.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])     }</p> str or callable, default='accuracy' <p>Metric to optimize during cross-validation. Supports: - Any string from sklearn (e.g., 'f1', 'roc_auc', 'log_loss') - A custom callable with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of optimization trials to run.</p> int, default=5 <p>Number of folds in cross-validation to evaluate each hyperparameter setting.</p> bool, default=True <p>If True, use StratifiedKFold for cross-validation (preserves class balance).</p> {'maximize', 'minimize'}, default='maximize' <p>Whether to maximize or minimize the scoring function.</p> bool, default=True <p>If True, prints trial progress, parameter table, and best result summary.</p> bool, default=True <p>If True, fits and returns the best model using the entire dataset.</p> int, default=42 <p>Random seed for reproducible folds and results.</p> float or None, optional <p>If provided, samples a random fraction (e.g., 0.1 = 10%) of the dataset.</p> int or None, optional <p>If provided, uses only the first N rows of the data.</p> bool, default=False <p>If True, reduces <code>n_trials</code> to 10, disables logs, and optimizes speed. Use for quick tests or large-scale experiments.</p>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--returns","title":"Returns","text":"<p>dict     A dictionary containing:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.Study     - 'best_model' : fitted model (if return_model=True)</p>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--raises","title":"Raises","text":"<p>ValueError     If required arguments are missing or incompatible.</p>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use: - You're comparing models or trying to find optimal settings for one. - You want to replace GridSearchCV with faster, smarter search. - You want control over cross-validation, scoring, and sampling. - You\u2019re tuning on large datasets and want quick feedback via fast_mode or subsampling.</p> <p>\ud83d\udccc Core Concepts:</p> <p>\u2022 Optuna Trials:     Each trial samples a different set of parameters using Optuna\u2019s intelligent search strategy     (Tree Parzen Estimator) and evaluates them using cross-validation.</p> <p>\u2022 param_grid:     Define hyperparameter ranges via lambdas \u2014 much more flexible than grid search.     Supports <code>suggest_float</code>, <code>suggest_int</code>, <code>suggest_categorical</code>, and log-scale sampling.</p> <p>\u2022 scoring:     Choose a string (e.g., 'f1', 'accuracy', 'roc_auc') or define a custom scoring function     that returns a float. Examples:         - 'neg_log_loss': minimizes log loss (automatically handled)         - custom_cost(estimator, X, y): returns cost/loss based on predictions</p> <p>\u2022 StratifiedKFold:     Recommended for binary classification, especially if classes are imbalanced.</p> <p>\u2022 fast_mode:     Use fast_mode=True to reduce trials to 10, turn off prints, and return results quickly.     Ideal for initial tests or iterative tuning on large data.</p> <p>\u2022 Sampling Subsets:     - <code>use_fraction=0.1</code>: randomly sample 10% of the data     - <code>use_n_samples=5000</code>: use only the first 5000 rows     - You can pass both; <code>use_n_samples</code> is applied after <code>use_fraction</code></p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import RandomForestClassifier results = hyperparameter_tuning_classification(         X=X, y=y,         model_class=RandomForestClassifier,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 15),             \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),         },         scoring='f1',         n_trials=50,         stratified=True,         fast_mode=False,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) best_model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use fewer trials with small data or fast models (e.g., LogisticRegression). - Tune for <code>log_loss</code> when probabilistic accuracy matters (e.g., fraud detection). - Always use <code>return_model=True</code> in production workflows to get the fitted model. - Set <code>fast_mode=True</code> when running in a loop or with large datasets. - Pair with <code>evaluate_classification_model()</code> for post-tuning performance analysis.</p>"},{"location":"api/classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--see-also","title":"See Also","text":"<p>evaluate_classification_model : Full evaluation suite for binary classifiers. GridSearchCV : Traditional brute-force alternative (slower, less efficient). Optuna : https://optuna.org</p>"},{"location":"api/classification/#nested-cross-validation-binary-classification","title":"Nested cross-validation (binary classification)","text":"<pre><code>from batwing_ml import run_nested_cv_classification\n</code></pre> <p>Run nested cross-validation for multiple classification models with optional tuning strategies and scaling.</p> <p>This function evaluates one or more classification models using nested cross-validation \u2014  an approach that combines robust performance estimation with internal hyperparameter tuning.  It supports both GridSearchCV and RandomizedSearchCV, allows sub-sampling for large datasets,  and offers a simplified or silent fast mode for production or large-scale runs.</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix (n_samples, n_features). y : pd.Series or np.ndarray     Binary classification target variable (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn classifiers.</p> dict of str \u2192 dict <p>Dictionary mapping model names to hyperparameter grids in sklearn format.</p> list of str or callables, default=['accuracy'] <p>List of scoring metrics to evaluate during nested CV (e.g., ['accuracy', 'roc_auc']).</p> int, default=3 <p>Number of folds in the outer CV loop (used for final performance estimation).</p> int, default=3 <p>Number of folds in the inner CV loop (used for hyperparameter tuning).</p> {'grid', 'random'}, default='grid' <p>Search strategy to use for tuning: - 'grid': exhaustive parameter search (GridSearchCV) - 'random': random search with <code>n_iter</code> (RandomizedSearchCV)</p> int, optional <p>Only used when <code>search_method='random'</code>. Number of parameter settings sampled.</p> int, optional <p>If provided, limits the number of rows used for fitting to this count (e.g., 100_000).</p> float, optional <p>If provided, randomly samples this fraction of rows (e.g., 0.1 for 10%).</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function to determine if a given model should use <code>StandardScaler</code>.</p> sklearn-compatible transformer, optional <p>If provided, replaces built-in scaling logic with a user-defined transformer or pipeline.</p> bool, default=True <p>If True, prints progress, hyperparameter tables, and detailed summaries.</p> bool, default=False <p>If True, disables visual output and reduces outer CV folds to speed up runtime.</p> bool, default=False <p>If True, returns a dictionary with summaries, best parameters, and matplotlib figures.</p> {'tabulate', 'rich'}, default='tabulate' <p>Controls how summary tables are printed (colorful or ASCII).</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--returns","title":"Returns","text":"<p>dict, optional     Only if <code>return_results=True</code>. Includes:     - 'summary': pd.DataFrame of all model-metric combinations     - 'results': detailed CV results and best parameters     - 'best_params': dict of best params for each model and metric     - 'figures': optional comparison plots (if fast_mode=False)</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--raises","title":"Raises","text":"<p>ValueError     If any of the required inputs (<code>X</code>, <code>y</code>, <code>model_dict</code>, <code>param_grids</code>) are missing.</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--examples","title":"Examples","text":"<p>from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier</p> <p>models = {         'logistic': LogisticRegression(),         'rf': RandomForestClassifier()     }</p> <p>grids = {         'logistic': {'C': [0.1, 1, 10]},         'rf': {'n_estimators': [50, 100]}     }</p> <p>results = run_nested_cv_classification(         X=X, y=y,         model_dict=models,         param_grids=grids,         scoring_list=['accuracy', 'f1'],         fast_mode=True,         search_method='random',         n_iter=5,         max_samples=10000,         return_results=True     )</p> <p>print(results['summary'])</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--user-guide","title":"User Guide","text":"<p>\ud83e\uddea What is Nested CV?     - Nested cross-validation helps prevent overfitting in model selection by using an inner loop        for hyperparameter tuning and an outer loop for estimating generalization performance.</p> <p>\ud83d\udccc Common Use Cases:     - Benchmarking many classifiers fairly.     - Avoiding bias from tuning and testing on the same split.     - Handling imbalanced or high-cardinality feature spaces.</p> <p>\u2699\ufe0f When to Use Which Search Strategy:     - Use <code>'grid'</code> for small hyperparameter spaces where you want exact control.     - Use <code>'random'</code> when the space is large or you're optimizing speed vs. accuracy.</p> <p>\u26a1 When to Use <code>fast_mode=True</code>:     - During large-scale experiments (e.g., 10M+ rows).     - If visual plots or verbose logging are unnecessary.     - In production or headless environments.</p> <p>\ud83d\udd0d Interpreting the Output:     - Mean/Std Dev: Represents model stability across folds.     - Best Params: Optimal hyperparameters found by inner CV.     - Figures: Comparison plots for accuracy, F1, AUC, etc.</p> <p>\ud83d\udcc9 Scaling Rules:     - By default, tree-based models skip scaling.     - You can override this using <code>use_scaling()</code> or provide a custom <code>preprocessor</code>.</p> <p>\ud83d\udeab Memory Tips for Big Data:     - Use <code>max_samples</code> or <code>sample_frac</code> to downsample safely.     - Enable <code>fast_mode</code> to avoid expensive operations like seaborn/Matplotlib rendering.</p>"},{"location":"api/classification/#batwing_ml.run_nested_cv_classification--see-also","title":"See Also","text":"<ul> <li>GridSearchCV, RandomizedSearchCV (from sklearn)</li> <li>evaluate_classification_model() for post-training model diagnostics</li> </ul>"},{"location":"api/classification/#evaluation-utilities-binary-classification","title":"Evaluation utilities (binary classification)","text":"<pre><code>from batwing_ml import evaluate_classification_model\n</code></pre>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model","title":"<code>evaluate_classification_model(model=None, X_train=None, y_train=None, X_test=None, y_test=None, cv=5, cost_fn=None, cost_fp=None, validation_params=None, scoring_curve='accuracy', verbose=True, return_dict=False, return_model_only=False, export_model=False, extra_plots=None, sample_fraction=None, sample_size=None, fast_mode=False)</code>","text":"<p>Evaluate a binary classification model with diagnostics, metrics, plots, and model handling.</p> <p>This function evaluates a scikit-learn-compatible classifier using standard and advanced metrics. It supports both pretrained and unfitted models, can train during evaluation, and provides plots for calibration, threshold tuning, learning/validation curves, and more. It's useful for audits, experimentation, and production diagnostics.</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn classifier. If unfitted, the function will train it using provided data.</p> array-like, optional <p>Training feature matrix. Required if model is not already fitted.</p> array-like, optional <p>Training labels.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing labels.</p> int, default=5 <p>Number of folds for cross-validation in learning/validation curves.</p> float, optional <p>Cost for false negatives in misclassification cost calculation.</p> float, optional <p>Cost for false positives.</p> dict, optional <p>Dictionary of hyperparameter names to lists of values for validation curve plotting.</p> str, default='accuracy' <p>Scoring metric used in learning and validation curves.</p> bool, default=True <p>If True, prints metrics and plots.</p> bool, default=False <p>If True, returns metrics as a dictionary.</p> bool, default=False <p>If True, returns the trained model only (no metrics).</p> bool, default=False <p>If True, returns (metrics_dict, trained_model) tuple.</p> list of str, optional <p>Options include:     - \"threshold\": Precision/Recall/F1 vs threshold     - \"calibration\": Reliability of predicted probabilities     - \"ks\": KS separation statistic     - \"lift\": Lift curve     - \"det\": Detection Error Tradeoff curve</p> float, optional <p>Fraction of data to use for train/test sets (e.g., 0.1 = 10%).</p> int, optional <p>Number of rows to use from training and testing sets.</p> bool, default=False <p>If True, skips plots and printouts for faster processing.</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Fitted model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>export_model=True</code></p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--raises","title":"Raises","text":"<p>ValueError     If necessary inputs are missing or misaligned.</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--examples","title":"Examples","text":"<p>evaluate_classification_model(         model=clf,         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         cost_fn=10, cost_fp=1,         validation_params={'max_depth': [2, 4, 6]},         scoring_curve='f1',         extra_plots=['threshold', 'ks'],         return_dict=True     )</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--save-a-newly-trained-model","title":"Save a newly trained model","text":"<p>model = evaluate_classification_model(         model=RandomForestClassifier(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         return_model_only=True     )</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--evaluate-a-pretrained-model","title":"Evaluate a pretrained model","text":"<p>from joblib import load model = load(\"model.joblib\") evaluate_classification_model(model=model, X_test=Xt, y_test=yt)</p>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You have a binary classifier (trained or not) and want a full evaluation report.     \u2022 You want to generate visual diagnostics to understand performance beyond raw metrics.     \u2022 You're comparing different models and need standardized summaries and visual feedback.     \u2022 You want to include cost-sensitive evaluation logic (e.g., FP/FN tradeoffs in fraud, medical diagnosis).     \u2022 You plan to export a trained model right after evaluation to reuse or save for deployment.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 Accuracy:         \u2192 Proportion of correct predictions overall.         \u2192 Works well when classes are balanced, but misleading with imbalanced classes.</p> <pre><code>\u2022 Precision:\n    \u2192 Of all predicted positives, how many were actually correct?\n    \u2192 Use when false positives are costly (e.g., spam detection).\n\n\u2022 Recall:\n    \u2192 Of all actual positives, how many did the model catch?\n    \u2192 Use when false negatives are costly (e.g., cancer diagnosis).\n\n\u2022 F1 Score:\n    \u2192 Harmonic mean of Precision and Recall.\n    \u2192 Best for imbalanced datasets where both FP and FN matter.\n\n\u2022 ROC AUC:\n    \u2192 Measures the model's ability to rank positives over negatives.\n    \u2192 Robust against imbalance. Closer to 1 = better.\n\n\u2022 Cost-sensitive Average Loss:\n    \u2192 Weighted loss calculation based on your domain-specific cost of False Positives and False Negatives.\n    \u2192 Useful in fraud detection, churn prediction, or medical triage where not all errors are equal.\n</code></pre> <p>\ud83d\udcc8 Optional Diagnostic Plots:     These are generated when <code>extra_plots</code> is set and <code>fast_mode=False</code>.</p> <pre><code>\u2022 Threshold Curve (`extra_plots=['threshold']`):\n    \u2192 Shows how Precision, Recall, and F1 change with different probability thresholds.\n    \u2192 Use it when you need to manually tune the decision boundary (e.g., prioritize recall over precision).\n\n\u2022 Calibration Curve (`extra_plots=['calibration']`):\n    \u2192 Compares predicted probabilities to observed outcomes.\n    \u2192 Helps determine if model outputs represent true probabilities (e.g., in credit scoring, risk modeling).\n\n\u2022 KS Statistic (`extra_plots=['ks']`):\n    \u2192 Plots the cumulative distribution of scores for each class and measures the maximum separation.\n    \u2192 A higher KS value (closer to 1) indicates good class separation.\n\n\u2022 Lift Curve (`extra_plots=['lift']`):\n    \u2192 Compares model performance against random guessing in terms of capturing true positives.\n    \u2192 Great for targeting top decile groups (e.g., marketing response modeling).\n\n\u2022 DET Curve (`extra_plots=['det']`):\n    \u2192 Plots False Positive Rate vs. False Negative Rate using logarithmic scale.\n    \u2192 Especially useful in imbalanced classification (e.g., rare disease detection, security event modeling).\n</code></pre> <p>\ud83d\udcda Learning &amp; Validation Curves (CV Required):     \u2022 Learning Curve:         \u2192 Plots model performance vs. training size.         \u2192 Helps detect underfitting (low train/val scores) or overfitting (train \u226b val).         \u2192 Useful to decide if more data will help your model.</p> <pre><code>\u2022 Validation Curve (`validation_params={'C': [...], 'max_depth': [...]}`):\n    \u2192 Shows how a single hyperparameter affects training and validation performance.\n    \u2192 Use to find optimal model complexity (e.g., tree depth, regularization).\n</code></pre> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skip all visual output and verbose logs. Ideal for CI/CD pipelines or large batch runs.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Use this if your model is not yet fitted and you want the trained object back after evaluation.\n\n\u2022 export_model=True:\n    \u2192 Returns both (metrics_dict, trained_model) to chain into pipelines, dashboards, or export routines.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Quickly prototype or test on large datasets without full evaluation time/cost.\n    \u2192 Useful when training/testing on full 10M+ rows is not practical.\n</code></pre>"},{"location":"api/classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--see-also","title":"See Also","text":"<ul> <li>preprocess_dataframe() : For feature preprocessing</li> <li>summary_dataframe() : For data overview</li> <li>run_nested_cv_classification() : For model comparison</li> </ul>"},{"location":"api/exploratory/","title":"Exploratory &amp; ETL API","text":"<p>This section documents the core utilities for data validation, sampling/splitting, EDA summaries, preprocessing, and feature engineering.</p> <p>Note: The <code>:::</code> blocks below are mkdocstrings hooks. If your module paths differ slightly, adjust the dotted paths (e.g. <code>batwing_ml.something.module</code>) to match your actual package.</p>"},{"location":"api/exploratory/#data-validation-etl","title":"Data validation &amp; ETL","text":"<p>High-level entry point for cleaning and validating raw tabular data.</p> <pre><code>from batwing_ml import validate_and_clean_data\n</code></pre> <p>Validate and clean a raw Pandas DataFrame with schema enforcement, common ETL transformations,  and full audit logging \u2014 designed for production-ready ML pipelines and interactive data workflows.</p> <p>This function performs all critical preprocessing validations: - Ensures column names and data types are as expected - Removes duplicates, trims and cleans strings - Parses date columns, applies range/category checks - Tracks actions in a structured report - Optionally saves audit report as HTML and JSON</p>"},{"location":"api/exploratory/#batwing_ml.validate_and_clean_data--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The raw input DataFrame to validate and clean. This should be the first step in your ML pipeline.</p> dict, optional <p>A dictionary mapping expected column names to their expected pandas-compatible data types (e.g., 'int', 'float'). Used to coerce types using <code>.astype()</code> and flag columns that don't match.</p> <p>Example:</p> <p>schema = {\"age\": \"int\", \"income\": \"float\", \"signup_date\": \"datetime64[ns]\"}</p> dict, optional <p>Dictionary to rename columns before validation. Useful when dealing with inconsistent column headers from different data sources.</p> <p>Example:</p> <p>rename_map = {\"SignUpDate\": \"signup_date\"}</p> bool, default=True <p>If True, removes duplicate rows using <code>df.drop_duplicates()</code>. Duplicates removed are logged in the report.</p> bool, default=True <p>If True, attempts to cast each column to the type specified in the <code>schema</code>. Safe conversions only; fails silently and skips invalid conversions.</p> bool, default=True <p>If True, automatically detects and parses columns that contain 'date' in their name using <code>pd.to_datetime()</code>.</p> dict, optional <p>A dictionary specifying numeric columns with acceptable (min, max) value ranges. Rows violating the range are not dropped but counted and reported.</p> <p>Example:</p> <p>enforce_ranges = {\"age\": (0, 120), \"income\": (0, 1_000_000)}</p> list of str, optional <p>Columns that must contain unique values (e.g., primary keys). Violations are logged, not dropped.</p> <p>Example:</p> <p>unique_columns = [\"email\", \"user_id\"]</p> dict, optional <p>Dictionary mapping column names to a list of allowed values. Invalid category entries are flagged in the report.</p> <p>Example:</p> <p>category_constraints = {\"gender\": [\"male\", \"female\", \"other\"]}</p> bool, default=True <p>If True, strips leading/trailing spaces and lowercases all object-type (string) columns.</p> bool, default=True <p>If True, converts all column names to <code>snake_case</code> for consistency across downstream ML workflows.</p> bool, default=True <p>If True, prints basic progress logs to console. Does not affect HTML output.</p> bool, default=True <p>If True, displays a beautiful scrollable HTML summary in notebooks and returns the full metadata dictionary.</p> str, optional <p>File path to save the report as an <code>.html</code> file. Uses the same style as your notebook HTML output.</p> <p>Example:</p> <p>report_path = \"validation_report.html\"</p> str, optional <p>If provided, saves the full validation report dictionary as a <code>.json</code> file for use in audit pipelines, logging systems, etc.</p> <p>Example:</p> <p>export_json = \"validation_metadata.json\"</p> list of str, optional <p>List of keywords (e.g., 'email', 'ssn', 'phone') to scan for in column names to flag potential PII fields.</p> <p>Example:</p> <p>pii_keywords = [\"email\", \"phone\", \"ssn\"]</p>"},{"location":"api/exploratory/#batwing_ml.validate_and_clean_data--returns","title":"Returns","text":"<p>df_clean : pd.DataFrame     The cleaned and type-coerced DataFrame, safe for downstream modeling or transformation.</p> dict <p>A structured dictionary summarizing all validation checks, transformation steps, and column-level actions. Contains keys such as: 'missing_values', 'coerced_columns', 'pii_suspects', etc.</p>"},{"location":"api/exploratory/#batwing_ml.validate_and_clean_data--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic usage with report:</p> <p>df_clean, report = validate_and_clean_data(df, schema={\"age\": \"int\", \"income\": \"float\"})</p> <p>\u25b6\ufe0f Add renaming and range checks:</p> <p>df_clean, report = validate_and_clean_data(         df,         rename_map={\"SignUpDate\": \"signup_date\"},         enforce_ranges={\"age\": (0, 100)}     )</p> <p>\u25b6\ufe0f Generate downloadable audit report:</p> <p>validate_and_clean_data(         df, schema=schema, report_path=\"validation.html\", export_json=\"report.json\"     )</p>"},{"location":"api/exploratory/#batwing_ml.validate_and_clean_data--notes","title":"Notes","text":"<ul> <li>Columns not in schema are ignored during type coercion</li> <li>Violations are logged but data is not dropped unless explicitly enabled</li> <li>You can use this before any ML function like <code>preprocess_dataframe()</code> or <code>feature_engineering()</code></li> <li>The audit report helps ensure trust, governance, and repeatability in data flows</li> </ul>"},{"location":"api/exploratory/#batwing_ml.validate_and_clean_data--related","title":"Related","text":"<p>\u2022 preprocess_dataframe() \u2013 for full-scale modeling prep \u2022 feature_exploration() \u2013 for checking feature quality \u2022 prepare_sample_split() \u2013 for reproducible sampling/splitting \u2022 pandas_profiling (external) \u2013 for deep statistical reports \u2022 great_expectations (optional) \u2013 for declarative expectations</p>"},{"location":"api/exploratory/#data-preparation-splitting","title":"Data preparation &amp; splitting","text":"<p>Helpers for sampling and preparing train/val/test splits.</p> <pre><code>from batwing_ml import prepare_sample_split\n</code></pre> <p>Sample and/or split your dataset for training workflows with optional stratification, reproducibility,  metadata tracking, and notebook-friendly summaries.</p> <p>This function is ideal for: - Efficiently working with large datasets by subsampling - Creating reproducible train/test splits with or without class balance - Getting audit-ready metadata (row counts, strategy used, seed) - Easily integrating into notebook-based data science pipelines</p>"},{"location":"api/exploratory/#batwing_ml.prepare_sample_split--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The full dataset from which sampling and/or splitting is performed. Each row is an observation.</p> str, optional <p>Column name to use for stratification (classification problems). Required only if <code>stratify=True</code>. Can be ignored for regression tasks or unsupervised workflows.</p> <p>Example:</p> <p>target='label'</p> int, optional <p>Number of rows to sample from the dataset before splitting. If None, the entire dataset is used.</p> <p>Example:</p> <p>sample_size=10000</p> bool, default=False <p>Whether to split the sampled/full dataset into train and test sets.</p> <p>If True, you get <code>(train_df, test_df)</code> or <code>(train_df, test_df, metadata)</code>. If False, only a single DataFrame is returned.</p> float, default=0.2 <p>Proportion of test set if <code>split=True</code>.</p> <p>Example:</p> <p>test_size=0.3  # 70/30 split</p> bool, default=False <p>If True, ensures that train/test (or sample) retains class balance by stratifying using the <code>target</code>.</p> <p>Recommended for classification tasks.</p> int, default=42 <p>Sets the random state for both sampling and splitting to ensure full reproducibility.</p> <p>Example:</p> <p>random_seed=123</p> bool, default=False <p>If True, returns the list of original indices of the sampled DataFrame.</p> <p>Example:</p> <p>df_sampled, idx = prepare_sample_split(df, sample_size=5000, return_indices=True)</p> bool, default=False <p>If True, returns a detailed dictionary summarizing: - number of rows before/after sampling - stratification status - train/test split sizes (if split=True) - random seed used</p> <p>Metadata is also displayed in scrollable HTML format when using notebooks.</p> bool, default=False <p>If True, prints internal logging messages about the sampling/splitting process.</p> callable, optional <p>Optional custom logging function (e.g., <code>logger.info</code>) to capture messages in logs or dashboards.</p> <p>Example:</p> <p>prepare_sample_split(df, log_callback=my_logger)</p>"},{"location":"api/exploratory/#batwing_ml.prepare_sample_split--returns","title":"Returns","text":"<p>pd.DataFrame or tuple     - If <code>split=False</code>: returns a sampled <code>pd.DataFrame</code>, or with indices/metadata if requested.     - If <code>split=True</code>: returns <code>(train_df, test_df)</code> or <code>(train_df, test_df, metadata)</code>.</p>"},{"location":"api/exploratory/#batwing_ml.prepare_sample_split--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic sampling:</p> <p>df_sampled = prepare_sample_split(df, sample_size=5000)</p> <p>\u25b6\ufe0f Sampling + splitting:</p> <p>train_df, test_df = prepare_sample_split(df, sample_size=10000, split=True)</p> <p>\u25b6\ufe0f Stratified split with metadata:</p> <p>train_df, test_df, meta = prepare_sample_split(         df, target='label', stratify=True, split=True, return_metadata=True     )</p> <p>\u25b6\ufe0f Sample with reproducibility:</p> <p>df_sampled = prepare_sample_split(df, sample_size=3000, random_seed=123)</p> <p>\u25b6\ufe0f With logging hook:</p> <p>prepare_sample_split(df, log_callback=lambda msg: print(f\"[LOG]: {msg}\"))</p>"},{"location":"api/exploratory/#batwing_ml.prepare_sample_split--notes","title":"Notes","text":"<ul> <li>Internally uses <code>sklearn.model_selection.train_test_split()</code> for splitting and stratification.</li> <li>Always resets index to prevent downstream issues with row alignment.</li> <li>Stratification only works with classification-style discrete targets.</li> </ul>"},{"location":"api/exploratory/#batwing_ml.prepare_sample_split--related","title":"Related","text":"<p>\u2022 feature_exploration() \u2014 run after sampling to analyze feature quality \u2022 evaluate_classification_model() \u2014 use after train/test split for performance metrics \u2022 feature_engineering() \u2014 to transform features post sampling/split \u2022 preprocess_dataframe() \u2014 clean the data before modeling</p>"},{"location":"api/exploratory/#dataframe-column-summaries","title":"DataFrame &amp; column summaries","text":"<p>Notebook-friendly EDA summaries for full DataFrames and individual columns.</p> <pre><code>from batwing_ml import summary_dataframe, summary_column\n</code></pre> <p>Generates a detailed summary report of an entire DataFrame for exploratory data analysis (EDA).</p> <p>This function provides a transparent, column-by-column overview of your dataset, including data types, missing value patterns, uniqueness, cardinality, and optionally deeper statistical insights like skewness, kurtosis, entropy, and correlation structure.</p> <p>It helps you understand the shape and quality of your data before modeling, and supports structured auditing via optional DataFrame outputs for downstream usage.</p> <p>Generates a detailed, human-readable summary of a single column in a DataFrame.</p> <p>This function helps you understand the nature of a specific column by providing summary metrics such as missingness, uniqueness, cardinality, entropy, skewness, and optional visualizations. It adapts intelligently to both numeric and categorical data and can highlight distribution issues, outliers, or missing trends over time.</p> <p>It is ideal for exploratory data analysis (EDA), column-wise diagnostics, and auditing feature quality before preprocessing or modeling.</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset to analyze. Must be a rectangular DataFrame with rows and columns.</p> bool, default=True <p>If True, prints human-readable summaries using styled tables, markdown, or HTML (in notebooks). If <code>fast_mode=True</code>, this will be ignored (no prints will be shown).</p> bool, default=False <p>If True, returns the core summary tables as pandas DataFrames for programmatic use.</p> bool, default=False <p>If True, enables additional statistics: - For numeric: skewness, kurtosis, and z-score-based outlier counts - For categorical: entropy (how evenly distributed the values are) - Also shows duplicate row and column analysis</p> bool, default=False <p>If True, computes a Pearson correlation matrix for all numeric features.</p> bool, default=False <p>If True, disables all visual and computationally expensive diagnostics: - Skips entropy, skewness, kurtosis, outliers, and duplicate detection - Skips correlation matrix computation - Skips verbose display/logging Use this mode when profiling large datasets (e.g., 1M+ rows) or in batch workflows.</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--returns","title":"Returns","text":"<p>tuple of pd.DataFrame, optional     Only returned if <code>return_dataframes=True</code>. Includes:     - summary : Core metadata (dtype, missing %, unique %, etc.)     - desc_numeric : Descriptive stats for all numeric columns     - desc_categorical : Descriptive stats for categorical/object columns     - correlation_matrix : Numeric correlation matrix (only if <code>correlation_matrix=True</code>)</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--raises","title":"Raises","text":"<p>ValueError     If the input DataFrame is empty or not valid.</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--examples","title":"Examples","text":"<p>summary_dataframe(df, detailing=True, correlation_matrix=True)</p> <p>summary, num_stats, cat_stats = summary_dataframe(df, return_dataframes=True, detailing=True)</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--notes","title":"Notes","text":"<ul> <li>The function is non-destructive: it reads from the input DataFrame without modifying it.</li> <li>If you're working with extremely large datasets, set <code>fast_mode=True</code> to avoid slow diagnostics.</li> <li>When <code>verbose=True</code>, this function uses IPython\u2019s HTML renderer for a notebook-friendly display.</li> </ul>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--see-also","title":"See Also","text":"<p>summary_column : Analyze a single column with detailed metrics and plots preprocess_dataframe : Prepare a dataset for modeling through scaling, encoding, and imputation preprocess_column : Clean a single column manually (e.g., outlier handling, transformation)</p>"},{"location":"api/exploratory/#batwing_ml.summary_dataframe--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When Should You Use This? - At the start of a project to assess data readiness. - Before feature engineering to identify columns to drop, fix, or transform. - During EDA or notebook exploration to communicate data quality. - In automated pipelines where you need programmatic summary outputs.</p> <p>\ud83d\udccc What You'll Learn: - Which columns have high missingness, low variance, or high cardinality - How many numeric/categorical features exist - Skewness or entropy in features (if detailing=True) - Whether your dataset has duplicated rows or columns - Correlation patterns among numeric features (optional)</p> <p>\u2699\ufe0f Recommended Usage Patterns:</p> <ol> <li> <p>Full EDA diagnostic (notebooks):</p> <p>summary_dataframe(df, detailing=True, correlation_matrix=True)</p> </li> <li> <p>For dashboards or programmatic reporting:</p> <p>summary, num_stats, cat_stats = summary_dataframe(df, return_dataframes=True)</p> </li> <li> <p>Batch analysis or large files:</p> <p>summary_dataframe(df, fast_mode=True)</p> </li> <li> <p>Minimal quick check (CLI or scripts):</p> <p>summary_dataframe(df, detailing=False, verbose=True)</p> </li> </ol> <p>\ud83d\udca1 Tips: - Use with <code>preprocess_dataframe()</code> to act on low-quality features you identify here. - <code>entropy</code> close to 0 \u2192 one category dominates (low information) - High skew/kurtosis \u2192 consider log or robust transformations - Z-score outliers &gt;10 \u2192 column likely needs clipping or scaling - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - Use <code>detailing=False</code> for a quick overview of the dataset without deep stats. - Use <code>correlation_matrix=False</code> to skip the correlation matrix. - Use <code>return_dataframes=True</code> to export summaries to reports or ML audit logs</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame containing the column to summarize.</p> str <p>The name of the column to analyze.</p> int, default=10 <p>Number of most frequent values to display in the frequency table for categorical features.</p> bool, default=True <p>If True, prints all summaries in formatted tables with headers. If <code>fast_mode=True</code>, this is ignored.</p> bool, default=False <p>If True, returns: - summary_table : key metrics (missing %, unique %, etc.) - desc_stats : descriptive stats (mean, std, IQR, etc.) - freq_dist : frequency counts of top values</p> bool, default=True <p>If True, enables additional diagnostics: - For numeric: skewness, kurtosis, z-score outliers - For categorical: entropy - Also enables visualizations if <code>plots</code> is specified</p> str, optional <p>If specified, enables a missing-value trend chart over time using this datetime column. Useful for temporal datasets and time-series analysis.</p> list of str, optional <p>List of plots to display: - \"histogram\": for numeric distribution - \"bar\": for top category counts - \"missing_trend\": for missing rate over time (requires <code>time_column</code>)</p> bool, default=False <p>If True, skips all optional visualizations, skew/entropy calculations, and pretty print tables. Recommended when analyzing very large datasets or when integrating into production pipelines.</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--returns","title":"Returns","text":"<p>tuple of pd.DataFrame, optional     If <code>return_dataframes=True</code>, returns:     - summary_table : base profile of the column     - desc_stats : statistical or categorical description     - freq_dist : top-N frequency breakdown</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--raises","title":"Raises","text":"<p>ValueError     If the specified column does not exist in the DataFrame.</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--examples","title":"Examples","text":"<p>summary_column(df, \"salary\", detailing=True, plots=[\"histogram\"])</p> <p>col_stats, desc, top_vals = summary_column(         df,         \"product_category\",         detailing=True,         plots=[\"bar\"],         return_dataframes=True     )</p> <p>summary_column(df, \"discount\", fast_mode=True)</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--notes","title":"Notes","text":"<ul> <li>The function detects whether the column is numeric or categorical and adapts its metrics accordingly.</li> <li>Outlier detection (z-score) is only applied to numeric features with sufficient variance.</li> <li>Plots are automatically skipped when <code>fast_mode=True</code>.</li> </ul>"},{"location":"api/exploratory/#batwing_ml.summary_column--see-also","title":"See Also","text":"<p>summary_dataframe : Summarizes all columns of a DataFrame at once. preprocess_column : Cleans and transforms a single column based on rules. preprocess_dataframe : End-to-end preprocessing pipeline for the entire DataFrame.</p>"},{"location":"api/exploratory/#batwing_ml.summary_column--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When Should You Use This? - You want to audit or explore one column in depth. - You're deciding how to impute, encode, or drop a specific column. - You want to visualize category frequency or numeric distribution interactively. - You're building an automated column-report pipeline (with return_dataframes=True).</p> <p>\u2699\ufe0f Recommended Use Cases</p> <ol> <li> <p>Understand a numeric column with outliers:</p> <p>summary_column(df, \"loan_amount\", detailing=True, plots=[\"histogram\"])</p> </li> <li> <p>Explore a categorical feature for feature engineering:</p> <p>summary_column(df, \"device_type\", top_n=5, detailing=True, plots=[\"bar\"])</p> </li> <li> <p>Check for seasonal missingness (e.g., sensors or logs):</p> <p>summary_column(df, \"temperature\", time_column=\"timestamp\", plots=[\"missing_trend\"])</p> </li> <li> <p>Automation or fast analysis at scale:</p> <p>summary_column(df, \"user_age\", fast_mode=True)</p> </li> </ol> <p>\ud83d\udca1 Tips: - Use with <code>preprocess_column()</code> to apply encoding or transformation after diagnosis. - If <code>entropy</code> is very low, the column may have little signal or be constant. - use 'fast_mode'=True<code>for large datasets to skip slow diagnostics. - Use</code>detailing=False<code>for a quick overview of the column without deep stats. - For many zero-variance columns, use</code>summary_dataframe()<code>for batch detection. - Always use</code>return_dataframes=True` if building custom reports or logging stats.</p>"},{"location":"api/exploratory/#preprocessing-helpers","title":"Preprocessing helpers","text":"<p>High-level preprocessing utilities that handle imputation, encoding, scaling, and column cleanup.</p> <pre><code>from batwing_ml import preprocess_dataframe, preprocess_column\n</code></pre> <p>Preprocess a tabular dataset for machine learning using a streamlined, transparent pipeline.</p> <p>This function performs automatic and configurable preprocessing on a pandas DataFrame,  applying common transformations such as missing value imputation, categorical encoding,  numeric feature scaling, and filtering of low-information or problematic features.</p> <p>It is designed to make raw datasets \"model-ready\" with minimal manual effort, while still allowing for full control over each preprocessing step.</p> <p>Preprocess a single column of data with smart transformations for modeling.</p> <p>This function allows targeted preprocessing of a single pandas Series \u2014 such as a feature column from a DataFrame \u2014 including missing value imputation, outlier treatment, encoding, scaling,  custom transformations, and optional inspection via plots or previews.</p> <p>It is useful for exploratory analysis, fine-tuned feature engineering, or inspecting column-specific cleaning steps outside of full-pipeline automation.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The raw input dataset to preprocess. Must be a standard, rectangular DataFrame.</p> bool, default=True <p>If True, fills missing values using the specified <code>numeric_method</code> and <code>categorical_method</code>.</p> {'mean', 'median', 'constant'}, default='mean' <p>Strategy to fill missing values in numeric columns: - 'mean': use column average - 'median': use column median - 'constant': fill with 0</p> {'mode', 'constant', 'drop'}, default='mode' <p>Strategy to fill missing values in categorical columns: - 'mode': fill with most frequent category - 'constant': fill with \"missing\" - 'drop': skip imputation for categoricals</p> float, default=0.3 <p>Drop columns with more than this proportion of missing values (e.g., 0.3 = 30%).</p> {'onehot', 'ordinal', None}, default='onehot' <p>How to encode categorical columns: - 'onehot': expand categories into binary columns (ideal for tree-based and linear models) - 'ordinal': convert categories into integers (compact but model-sensitive) - None: skip encoding</p> {'standard', 'minmax', 'robust', None}, default='standard' <p>Scaling method for numeric features: - 'standard': zero mean, unit variance (default for most ML models) - 'minmax': scales features to [0, 1] - 'robust': scales using median and IQR (more resistant to outliers) - None: skip scaling</p> bool, default=True <p>If True, drops columns where all values are the same \u2014 these provide no predictive value.</p> int or None, default=100 <p>If set, drops categorical columns with more than this many unique values. Useful to eliminate IDs or high-entropy features that aren't generalizable.</p> bool, default=False <p>If True, returns a second object (<code>steps</code>) summarizing the preprocessing pipeline steps. This is useful for auditing, documentation, or reproducing pipelines.</p> bool, default=False <p>If True, displays a styled scrollable HTML preview of the top rows of the transformed DataFrame. Intended for use inside notebooks (e.g., Jupyter, Colab).</p> bool, default=True <p>If True, prints human-readable logs describing each transformation as it is applied.</p> bool, default=False <p>If True, disables logging and previews to optimize performance for large datasets (1M+ rows). Recommended for production or batch processing pipelines.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--returns","title":"Returns","text":"<p>pd.DataFrame or Tuple[pd.DataFrame, dict]     - The transformed DataFrame, ready for use in ML models.     - If <code>return_steps=True</code>, also returns a dictionary-like object listing:         - dropped columns (due to high missingness, constant value, or high cardinality)         - columns encoded, scaled, or imputed         - final column names</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--examples","title":"Examples","text":"<p>df_clean = preprocess_dataframe(df)</p> <p>df_clean, steps = preprocess_dataframe(         df,         encode=\"ordinal\",         scale=\"robust\",         drop_missing_thresh=0.25,         return_steps=True,         preview=True     ) print(steps)</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--notes","title":"Notes","text":"<ul> <li>This function does not modify the input DataFrame (works on a copy).</li> <li>Designed to be flexible enough for prototyping, reproducible for experiments,   and fast enough for production workloads.</li> <li>Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops.</li> <li>For column-wise control, see <code>preprocess_column()</code>.</li> </ul>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--see-also","title":"See Also","text":"<p>preprocess_column : Clean and transform a single Series with similar options.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--user-guide","title":"User Guide","text":"<p>\ud83e\udded When Should You Use This Function? - You're starting with raw, messy tabular data that has missing values, mixed data types, or irrelevant columns. - You want to transform the dataset into a clean, numerical form ready for modeling \u2014 without hardcoding pipelines manually. - You're preparing data for machine learning algorithms (scikit-learn, XGBoost, LightGBM, etc.) and need a reproducible cleaning strategy. - You're in an experimentation phase and want fast iteration with logs, or you're moving toward deployment and need performance mode.</p> <p>\u2699\ufe0f Recommended Configurations (Use-Case Based)</p> <ol> <li>General-purpose ML modeling (balanced tabular data):    \u2192 Works well with logistic regression, SVMs, and shallow neural networks. <p>preprocess_dataframe(df, encode=\"onehot\", scale=\"standard\")</p> </li> </ol> <p>Why? One-hot encoding ensures categorical variables are treated independently. Standard scaling helps models converge.</p> <ol> <li>Tree-based models (RandomForest, XGBoost, LightGBM):    \u2192 These models handle ordinal input well and don\u2019t require scaling. <p>preprocess_dataframe(df, encode=\"ordinal\", scale=None)</p> </li> </ol> <p>Why? One-hot can add noise or bloat tree-based models. Ordinal + no scaling is faster and sufficient.</p> <ol> <li>High-cardinality datasets or sparse data (e.g., recommender systems): <p>preprocess_dataframe(df, encode=None, max_cardinality=50)</p> </li> </ol> <p>Why? Skip encoding and limit high-cardinality columns to avoid exploding the feature space.</p> <ol> <li>Production or big data batch preprocessing:    \u2192 Great for pipelines where performance &gt; visuals. <p>preprocess_dataframe(df, fast_mode=True)</p> </li> </ol> <p>Why? Disables all logging and display overhead \u2014 ideal for 1M+ rows.</p> <ol> <li>Auditing or debugging preprocessing behavior:    \u2192 You want to know exactly what was changed and why. <p>df_clean, steps = preprocess_dataframe(df, return_steps=True, verbose=True) print(steps)</p> </li> </ol> <p>Why? <code>steps</code> logs dropped columns, encoded features, and final output \u2014 great for versioning and reproducibility.</p> <p>\ud83d\udca1 Tips: - Use <code>preview=True</code> only in notebooks to visualize output cleanly. - Set <code>max_cardinality=None</code> to retain all categorical columns, even high-card ones. - <code>drop_constant=True</code> removes junk features automatically. - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - Use <code>preprocess_column()</code> for detailed tuning on a single feature.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_dataframe--related","title":"Related:","text":"<p>\u2022 preprocess_column(): Clean and transform a single Series with similar options. \u2022 summary_column(): View deep stats for a single column before preprocessing \u2022 summary_dataframe(): View deep stats for a DataFrame before preprocessing</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_column--parameters","title":"Parameters","text":"<p>col : pd.Series     The input column to clean. Can be numeric or categorical.</p> {'mean', 'median', 'mode', 'constant'} or scalar, optional <p>Strategy to fill missing values: - 'mean', 'median': numeric only - 'mode': most frequent value (works for both types) - 'constant': fill with 0 or \"missing\" - scalar: fill with a specific value (e.g., 0 or 'unknown')</p> {'standard', 'minmax', 'robust'}, optional <p>If provided, applies scaling (numeric only): - 'standard': zero mean, unit variance - 'minmax': rescale to [0, 1] - 'robust': scale based on median and IQR (for outliers)</p> {'ordinal', 'onehot'}, optional <p>If provided, applies encoding (categorical only): - 'ordinal': converts categories to integer codes - 'onehot': returns a new DataFrame with binary indicator columns</p> {'zscore', 'iqr'}, optional <p>Method for outlier detection and clipping (numeric only): - 'zscore': clips values with |Z| &gt; 3 - 'iqr': clips values outside 1.5 * IQR from Q1/Q3</p> tuple of float, default=(0.01, 0.99) <p>Lower and upper quantiles to cap values (a form of Winsorization). Applied only for numeric columns.</p> callable, optional <p>A custom function applied to every non-null element (e.g., <code>np.log1p</code>, <code>str.lower</code>). Useful for transformations like scaling, mapping, or normalization.</p> {'manual', 'auto'}, default='manual' <ul> <li>'manual': use the specified arguments only</li> <li>'auto': infer reasonable defaults based on column dtype and missing values</li> </ul> bool, default=False <p>If True, modifies the original Series inside a DataFrame. Otherwise works on a copy.</p> bool, default=False <p>If True, prints a small sample (head) of the cleaned column.</p> bool, default=False <p>If True, shows a histogram or bar plot (before/after if possible) to visualize the distribution.</p> bool, default=True <p>If True, logs steps taken (e.g., imputed with mean, encoded with ordinal).</p> bool, default=False <p>If True, disables plotting and previews for faster execution on large data.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_column--returns","title":"Returns","text":"<p>pd.Series or Tuple[pd.Series, dict]     - Cleaned Series (or one-hot encoded DataFrame if applicable).     - If used in unpacking (<code>col_clean, steps = ...</code>), also returns a dictionary       detailing the preprocessing actions taken.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_column--examples","title":"Examples","text":"<p>col_clean = preprocess_column(df[\"age\"], impute=\"mean\", scale=\"standard\")[0]</p> <p>cat_col, steps = preprocess_column(         df[\"gender\"],         impute=\"mode\",         encode=\"ordinal\",         preview=True,         verbose=True     ) print(steps)</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_column--user-guide","title":"User Guide","text":"<p>\ud83e\udded When Should You Use This? - You want fine-grained control over a single column in your dataset \u2014 e.g., apply different transformations for different features. - You\u2019re doing exploratory data analysis and want to inspect the impact of transformations before applying them in batch. - You\u2019re manually curating a feature set for a model and want to test encoding, scaling, or outlier treatment interactively. - You\u2019re building a custom preprocessing function per column for pipelines.</p> <p>\u2699\ufe0f Recommended Workflows (Based on Column Type)</p> <ol> <li>For numeric columns (e.g., age, income, price):    Apply standard cleaning + scaling + clipping: <p>col_clean, steps = preprocess_column(            df[\"income\"],            impute=\"mean\",            scale=\"standard\",            cap_outliers=\"zscore\",            cap_quantiles=(0.01, 0.99),            preview=True,            plot=True        )</p> </li> </ol> <p>Why? Numeric data benefits from scaling and outlier handling for better model convergence.</p> <ol> <li>For categorical columns (e.g., gender, city):    Apply imputation and ordinal encoding: <p>col_clean = preprocess_column(            df[\"gender\"],            impute=\"mode\",            encode=\"ordinal\"        )[0]</p> </li> </ol> <p>Why? Many models expect numeric input; ordinal encoding works well for tree models.</p> <ol> <li>When exploring transformation effects:    Use custom mapping functions or log transforms: <p>preprocess_column(df[\"price\"], custom_map=np.log1p, plot=True)</p> </li> </ol> <p>Why? Helps reduce skew in price-like data, which improves linear model performance.</p> <ol> <li>Quick automation:    Let the function auto-decide how to handle the column: <p>preprocess_column(df[\"feature_x\"], strategy=\"auto\")</p> </li> </ol> <p>Why? Saves time when you\u2019re cleaning many features quickly.</p> <p>\ud83d\udd0d Tips: - Use <code>preview=True</code> to view how the column looks after cleaning. - Use <code>plot=True</code> to visualize distributions before/after transformations. - Use <code>steps</code> output to document what happened to the column \u2014 especially useful in notebooks or reports. - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - One-hot encoding returns a DataFrame (not Series) \u2014 plan how you store or merge it back.</p>"},{"location":"api/exploratory/#batwing_ml.preprocess_column--related","title":"Related:","text":"<p>\u2022 preprocess_dataframe(): Clean an entire DataFrame in one call \u2022 summary_column(): View deep stats for a single column before preprocessing \u2022 summary_dataframe(): View deep stats for a DataFrame before preprocessing</p>"},{"location":"api/exploratory/#feature-exploration","title":"Feature exploration","text":"<p>Lightweight utilities to inspect feature-target relationships.</p> <pre><code>from batwing_ml import feature_exploration\n</code></pre>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration","title":"<code>feature_exploration(df, target=None, task=None, top_n=20, corr_threshold=0.95, skew_threshold=1.0, sample_size=None, fast_mode=False, export_path=None, tree_importance=True, perm_importance=True, show_preview=True, return_summary=False, heavy_ops_sample=5000, plots=None)</code>","text":"<p>Quickly analyze the quality and behavior of your dataset's features, with rich statistical summaries, automated warnings, optional modeling, and beautiful visualizations.</p> <p>This function helps you discover: - What features matter most for prediction? - Which features are redundant or constant? - Which columns may require transformation (e.g., skewed)? - How do categorical features relate to the target?</p> <p>Ideal for data scientists, analysts, and ML engineers who want to explore datasets before modeling.</p>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset. Each column is treated as a potential feature.</p> <pre><code>\u2705 Required: Must be a clean rectangular DataFrame (no nested structures).\n</code></pre> str, optional <p>The name of the target (dependent) variable in your dataset.</p> <p>Required for supervised diagnostics like: - Mutual Information (MI) - Correlation with target - Tree-based feature importance - Permutation importance - Grouped statistics for categorical features</p> <p>Example:</p> <p>target = \"SalePrice\"</p> {'regression', 'classification', 'multiclass'}, optional <p>Type of ML problem: - 'regression': Predicting continuous values (e.g., prices, temperature) - 'classification': Binary prediction (e.g., spam or not spam) - 'multiclass': More than 2 classes (e.g., sentiment = low/medium/high)</p> <p>\u2705 If not passed, the function will auto-detect using target column data type and uniqueness.</p> int, default=20 <p>How many top features to display in: - Summary table - Importance plots - Skewed distribution visualizations</p> <p>Tip: Increase to 50 or 100 for wide datasets with many features.</p> float, default=0.95 <p>Threshold above which two numeric features are considered highly correlated (redundant).</p> <ul> <li>Used to flag potential multicollinearity in the summary</li> <li>Helps you decide which features might be dropped</li> </ul> <p>Example: - A corr of 0.98 between 'age' and 'years_since_birth' may signal redundancy</p> float, default=1.0 <p>Skewness threshold for numeric features.</p> <ul> <li>Columns with absolute skew &gt; threshold are flagged</li> <li>These may benefit from transformation like <code>log(x+1)</code> or power scaling</li> </ul> <p>Tip: Highly skewed features can reduce model performance (especially linear models).</p> int, optional <p>If set, the DataFrame is sampled down to this number of rows before computing summaries.</p> <p>Useful for: - Large datasets (100k+ rows) - Speeding up exploration - Limiting memory usage</p> <p>Example:</p> <p>sample_size = 10000</p> bool, default=False <p>Turns off all compute-intensive operations: - Mutual information - Tree-based model fitting - Permutation importance - Plots</p> <p>\u2705 Use this for quick diagnostics on very large datasets or inside pipelines.</p> str, optional <p>If set, saves the summary table as a CSV.</p> <p>Example:</p> <p>export_path = \"feature_summary.csv\"</p> bool, default=True <p>If enabled and target is provided: - Fits a RandomForest model - Extracts feature importances - Displays top-N most predictive features</p> <p>\u26a0\ufe0f Ignored if <code>fast_mode=True</code>.</p> bool, default=True <p>If enabled: - Computes permutation-based feature importance (model-agnostic) - More stable than tree importances</p> <p>\u26a0\ufe0f Slower. Use only for small to mid-sized datasets or when needed.</p> bool, default=True <p>Displays the output table in a scrollable, styled HTML block (dark-theme compatible).</p> <p>Recommended for: - Jupyter Notebook - Google Colab - VSCode notebooks</p> bool, default=False <p>If True, the summary table (a DataFrame) is returned.</p> <p>Useful when you want to: - Save to Excel/CSV manually - Merge with other reports - Visualize in another tool</p> int, default=5000 <p>For compute-heavy steps like model fitting, mutual info, or permutation: - Only this many rows are sampled from the DataFrame</p> <p>Keeps everything fast and memory-efficient.</p> list of str, optional <p>List of visualizations to include. Choose any of:</p> <ul> <li>'importance': Bar plot of top features by tree or permutation importance</li> <li>'correlation': Heatmap of correlation between numeric features</li> <li>'skewed': Histogram of skewed numeric columns</li> <li>'grouped': Bar plots of mean target by category (only for regression)</li> </ul> <p>Example:</p> <p>plots = [\"importance\", \"correlation\", \"skewed\"]</p>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--returns","title":"Returns","text":"<p>pd.DataFrame or None     - If <code>return_summary=True</code>: Returns a summary DataFrame with suggestions     - If <code>return_summary=False</code>: Displays summary and plots only</p>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic usage:</p> <p>feature_exploration(df, target=\"SalePrice\", task=\"regression\")</p> <p>\u25b6\ufe0f With visuals and full scoring:</p> <p>feature_exploration(         df,         target=\"target\",         task=\"regression\",         plots=[\"importance\", \"correlation\", \"skewed\", \"grouped\"]     )</p> <p>\u25b6\ufe0f Fast mode scan:</p> <p>feature_exploration(df, fast_mode=True)</p> <p>\u25b6\ufe0f Export results:</p> <p>feature_exploration(df, target=\"target\", export_path=\"summary.csv\")</p> <p>\u25b6\ufe0f Capture summary in a variable:</p> <p>summary_df = feature_exploration(df, return_summary=True)</p>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--when-to-use","title":"When to Use","text":"<p>\u2705 Before modeling: to identify top features, poor features, or potential issues \u2705 After cleaning: to detect skew, high cardinality, or multicollinearity \u2705 In pipelines: to auto-generate feature insight reports \u2705 In dashboards: to track data quality in production  </p>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--notes","title":"Notes","text":"<ul> <li>Correlation = Pearson for numeric features</li> <li>Skewness is calculated using SciPy\u2019s <code>skew()</code></li> <li>Importance plots use RandomForest or Permutation models</li> <li>Grouped stats only apply to regression targets and categorical features</li> </ul>"},{"location":"api/exploratory/#batwing_ml.feature_exploration.feature_exploration--related","title":"Related","text":"<p>\u2022 feature_engineering() \u2013 to act on features after diagnosing \u2022 preprocess_dataframe() \u2013 for cleaning before feature exploration \u2022 summary_dataframe() \u2013 to get statistical overview of the full DataFrame \u2022 evaluate_classification_model() \u2013 to inspect how features affect model accuracy</p>"},{"location":"api/exploratory/#feature-engineering","title":"Feature engineering","text":"<p>Tools for feature selection, transformation, and dimensionality reduction.</p> <pre><code>from batwing_ml import feature_engineering\n</code></pre>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering","title":"<code>feature_engineering(df, target=None, mode='both', task=None, apply_changes=True, strategy='auto', top_k=None, fast_mode=False, show_preview=True, return_metadata=True, pca_components=None, clustering=False, cluster_k=None, cluster_feature_name='cluster_label', plots=None, selection_method='auto', selection_threshold=0.01)</code>","text":"<p>Perform automatic feature selection, engineering, and clustering in one unified function.</p> <p>This function is built for data scientists, analysts, or ML engineers who want to streamline the feature preprocessing pipeline with just one function call \u2014 while maintaining transparency, reproducibility, and full control over what happens to their features.</p>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset where each row is an observation and each column is a feature.     This is the only required argument \u2014 all others are optional and context-dependent.</p> str, optional <p>The name of the target column (i.e., label or output variable). This is necessary for supervised selection techniques like: - Mutual Information (<code>mutual_info</code>) - Model-based importance (<code>model</code>) - Lasso regression (<code>lasso</code>) - Recursive Feature Elimination (<code>rfe</code>)</p> <p>\u26a0\ufe0f If you don't pass this, selection methods requiring supervision will be skipped.</p> {'selection', 'engineering', 'both'}, default='both' <p>Controls what parts of the pipeline are executed: - 'selection': Only selects features based on some criteria. - 'engineering': Only adds new features (e.g., polynomial or PCA). - 'both': Runs selection first, then engineering.</p> <p>\ud83d\udc49 Tip: Use 'selection' during early cleanup. Use 'engineering' later before modeling.</p> {'regression', 'classification'}, optional <p>Type of ML problem. If not specified, it will be inferred from <code>target</code>: - Regression: If target is numeric with many unique values. - Classification: If target is binary or categorical.</p> <p>This controls scoring logic for feature selection (e.g., model choice, MI scoring).</p> bool, default=True <ul> <li>True \u2192 Apply selection/engineering steps directly to the data.</li> <li>False \u2192 Simulate the pipeline, only generate metadata and preview output.</li> </ul> <p>\u2705 Use <code>apply_changes=False</code> if you're testing or auditing steps first.</p> {'auto', 'manual'}, default='auto' <p>Controls engineering behavior: - 'auto': Automatically applies PolynomialFeatures and PCA if context is appropriate. - 'manual': Skip all engineering unless explicitly requested (e.g., you pass <code>pca_components</code>).</p> int, optional <p>Number of features to retain in methods like RFE or when displaying top-N importances.</p> <p>\ud83d\udccc Useful if you want to aggressively reduce feature dimensionality.</p> bool, default=False <p>Enables performance-safe mode: - Disables model fitting, plotting, clustering, and PCA. - Ideal for large datasets or batch scripts.</p> <p>\u26a0\ufe0f No importance plots or PCA will run if this is True.</p> bool, default=True <p>Displays a styled scrollable table showing: - Which features are kept - Which are engineered - Which were dropped</p> <p>\u2714\ufe0f Highly recommended in Jupyter/Colab for visual tracking.</p> bool, default=True <p>If True, returns a <code>_FeatureEngineeringSteps</code> object summarizing what happened.</p> <p>Contains: - <code>selected_features</code> - <code>dropped_features</code> - <code>created_features</code> - <code>transforms_applied</code></p> int, optional <p>Number of Principal Components to add. - Applies PCA on numeric features - Useful for dimensionality reduction and multicollinearity handling</p> <p>\ud83d\udcca If you add <code>\"pca\"</code> to <code>plots</code>, it shows a variance-explained curve.</p> bool, default=False <p>Enables KMeans clustering on numeric columns.</p> <p>\u2705 Adds a new column (<code>cluster_feature_name</code>) to represent the assigned cluster.</p> int, optional <p>Number of clusters (k) to use. - If None: shows elbow plot of SSE vs k. - If set: assigns clusters directly.</p> str, default='cluster_label' <p>Name of the column that stores the cluster number for each row. You can rename this if you\u2019re using multiple clustering passes.</p> list of str, optional <p>Visualizations to generate (optional, ignored in fast mode): - <code>\"importance\"</code>: Bar chart of top RandomForest importances - <code>\"pca\"</code>: Line plot showing cumulative variance from PCA - <code>\"clusters\"</code>: 2D PCA plot showing colored clusters</p> <p>\u2795 Add one or more based on your workflow.</p> str, optional <p>Method used for selecting features: - 'variance' \u2192 Drops constant or low-variance features - 'correlation' \u2192 Drops one of each highly correlated pair (based on threshold) - 'mutual_info' \u2192 Mutual Information score against the target - 'model' \u2192 Tree-based embedded feature importance (RandomForest) - 'lasso' \u2192 Uses coefficients from Lasso regression (regression only) - 'rfe' \u2192 Recursive Feature Elimination (Linear/Logistic) - 'auto' \u2192 Auto-selects based on task (classification/regression)</p> float, default=0.01 <p>Threshold for filtering-based methods: - If using <code>'correlation'</code>: Drop if abs(corr) &gt; threshold - If using <code>'mutual_info'</code>: Keep if MI score &gt; threshold</p> <p>\ud83d\udccc Lower values retain more features, higher values are stricter.</p>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering--returns","title":"Returns","text":"<p>pd.DataFrame     The transformed dataset with selected and engineered features.</p> <p>_FeatureEngineeringSteps (if return_metadata=True)     A rich summary of steps applied \u2014 for logs, reports, and audits.</p>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering--examples","title":"Examples","text":"<p>\u25b6\ufe0f Full pipeline:</p> <p>X, steps = feature_engineering(         df=raw_df,         target='target',         mode='both',         strategy='auto',         selection_method='model',         pca_components=3,         clustering=True,         cluster_k=5,         plots=['importance', 'pca', 'clusters']     )</p> <p>\u25b6\ufe0f Dry run (no changes, just audit):</p> <p>X, log = feature_engineering(df, target='target', apply_changes=False)</p> <p>\u25b6\ufe0f PCA for dimensionality reduction:</p> <p>X, meta = feature_engineering(df, mode='engineering', pca_components=2)</p> <p>\u25b6\ufe0f Lightweight filter-based selection:</p> <p>X, meta = feature_engineering(df, target='target', mode='selection', selection_method='mutual_info')</p>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering--notes","title":"Notes","text":"<ul> <li>Use <code>return_metadata=True</code> to track what changed \u2014 crucial for reproducibility.</li> <li>Feature selection happens before feature creation.</li> <li>All steps are logged and previewed before being applied.</li> <li>PCA and clustering only work on numeric columns.</li> </ul>"},{"location":"api/exploratory/#batwing_ml.feature_engineering.feature_engineering--see-also","title":"See Also","text":"<p>\u2022 feature_exploration() \u2013 For diagnostics before applying changes \u2022 preprocess_dataframe() \u2013 For imputation, encoding, and basic cleanup \u2022 summary_dataframe() \u2013 For an overview of raw features \u2022 evaluate_classification_model() \u2013 To measure final model quality</p>"},{"location":"api/multiclass_classification/","title":"Multiclass Classification API","text":"<p>This page documents the main entry points for multiclass classification workflows.</p> <ul> <li>Optuna-based hyperparameter tuning for multiclass</li> <li>Nested cross-validation</li> <li>Multiclass-aware evaluation utilities</li> </ul> <p>If imports fail during <code>mkdocs serve</code>, tweak the module paths in the <code>:::</code> directives to match your actual package structure.</p>"},{"location":"api/multiclass_classification/#hyperparameter-tuning-multiclass","title":"Hyperparameter tuning (multiclass)","text":"<pre><code>from batwing_ml import hyperparameter_tuning_multiclass_classification\n</code></pre>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification","title":"<code>hyperparameter_tuning_multiclass_classification(X=None, y=None, model_class=None, param_grid=None, scoring='accuracy', n_trials=50, cv_folds=5, stratified=True, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83c\udfaf Hyperparameter Tuning for Multiclass Classification (via Optuna)</p> <p>Optimize hyperparameters of any multiclass classification model using Optuna\u2019s efficient  search algorithm. This function supports cross-validation (with stratification), custom  or built-in scoring metrics, reproducibility, and flexible data sampling. Ideal for  experimentation, model leaderboard tuning, and automated pipelines.</p>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix of shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector with multiclass labels (3+ classes supported).</p> callable <p>A scikit-learn-style classifier class (e.g., <code>RandomForestClassifier</code>, <code>LogisticRegression</code>). Should be the class itself, not an instantiated object.</p> dict <p>Dictionary mapping hyperparameter names to Optuna search functions. Example:     {         \"C\": lambda t: t.suggest_float(\"C\", 0.01, 10, log=True),         \"solver\": lambda t: t.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\"])     }</p> str or callable, default='accuracy' <p>Evaluation metric to optimize. Can be: - Any valid sklearn scorer string (e.g., 'f1_macro', 'balanced_accuracy', 'neg_log_loss') - A custom scoring function with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of Optuna trials to run.</p> int, default=5 <p>Number of folds for cross-validation during evaluation.</p> bool, default=True <p>Whether to use StratifiedKFold (preserves class distribution across folds). Set to False to use standard KFold.</p> {'maximize', 'minimize'}, default='maximize' <p>Whether to maximize or minimize the scoring function.</p> bool, default=True <p>If True, prints Optuna progress, trial summary, and best results.</p> bool, default=True <p>If True, fits and returns the best model using the entire dataset.</p> int, default=42 <p>Seed for reproducibility (used in CV splitting and Optuna sampler).</p> float or None, optional <p>Randomly sample a fraction of the dataset (e.g., 0.1 = 10%).</p> int or None, optional <p>Limit dataset to first N rows (after fraction sampling, if both are set).</p> bool, default=False <p>If True:     - Limits trials to 10     - Disables verbose logs     - Useful for quick experimentation</p>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--returns","title":"Returns","text":"<p>dict     A dictionary containing:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.study.Study     - 'best_model' : trained model (if return_model=True)</p>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If required parameters are missing or scoring is misconfigured.</p>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--user-guide","title":"User Guide","text":"<p>\ud83c\udfaf When to Use: - You're tuning multiclass classification models (3+ classes). - You want intelligent hyperparameter search using Optuna instead of GridSearchCV. - You require macro/micro scoring for imbalanced datasets. - You need flexible sampling and control over reproducibility.</p> <p>\ud83d\udccc Core Concepts:</p> <p>\u2022 Multiclass Scoring:     Use <code>'f1_macro'</code>, <code>'balanced_accuracy'</code>, or other sklearn scoring strings.     You can also define a custom scorer that returns a float (e.g., macro recall).</p> <p>\u2022 Optuna Trials:     Each trial samples hyperparameters and evaluates them via cross-validation.     Optuna uses Tree Parzen Estimator (TPE) by default for efficient search.</p> <p>\u2022 param_grid:     A dictionary of Optuna sampling functions. Example:         {             'max_depth': lambda t: t.suggest_int('max_depth', 3, 10),             'min_samples_split': lambda t: t.suggest_int('min_samples_split', 2, 20)         }</p> <p>\u2022 StratifiedKFold:     Recommended when class distribution is skewed. Ensures representative folds.</p> <p>\u2022 fast_mode:     Enables a faster tuning mode by reducing trials and silencing logs. Great for early tests.</p> <p>\u2022 Sampling Subsets:     - <code>use_fraction=0.1</code>: randomly samples 10% of the data     - <code>use_n_samples=5000</code>: takes only first 5000 rows (after fraction)     - Combine both for rapid prototyping</p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import RandomForestClassifier results = hyperparameter_tuning_multiclass_classification(         X=X, y=y,         model_class=RandomForestClassifier,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 15),             \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),         },         scoring='f1_macro',         fast_mode=False,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) best_model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use <code>f1_macro</code> or <code>balanced_accuracy</code> when class distribution is uneven. - Set <code>fast_mode=True</code> when running many experiments in parallel. - Combine with <code>evaluate_multiclass_classification()</code> to analyze final performance. - Avoid using accuracy alone for imbalanced multiclass tasks \u2014 prefer macro metrics.</p>"},{"location":"api/multiclass_classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--see-also","title":"See Also","text":"<p>evaluate_multiclass_classification : Full evaluation utility for multiclass models. GridSearchCV : Slower, less flexible alternative. Optuna Docs : https://optuna.org</p>"},{"location":"api/multiclass_classification/#nested-cross-validation-multiclass","title":"Nested cross-validation (multiclass)","text":"<pre><code>from batwing_ml import run_nested_cv_multiclass_classification\n</code></pre> <p>Run nested cross-validation for multiclass classification models with internal tuning, scaling, and visualization.</p> <p>This function supports nested cross-validation for evaluating multiple classifiers on multiclass targets.  It performs hyperparameter tuning using either GridSearchCV or RandomizedSearchCV within inner folds and evaluates  model performance on outer folds. Additional features include smart sub-sampling, optional standardization, and  visual summary generation using matplotlib/seaborn.</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix (n_samples, n_features).</p> pd.Series or np.ndarray <p>Multiclass classification target variable (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn-compatible classifiers.</p> dict of str \u2192 dict <p>Dictionary mapping model names to their hyperparameter grids.</p> list of str or callable, default=['accuracy'] <p>List of scoring metrics for evaluation (e.g., ['accuracy', 'f1_macro', 'recall_macro']).</p> int, default=3 <p>Number of outer CV folds (used to estimate generalization performance).</p> int, default=3 <p>Number of inner CV folds (used for hyperparameter tuning).</p> int, default=42 <p>Random seed for reproducibility across resampling and CV.</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function that returns True/False depending on whether a model should use <code>StandardScaler</code>.</p> sklearn-compatible transformer, optional <p>Custom preprocessing pipeline. If provided, overrides automatic scaling logic.</p> {'grid', 'random'}, default='grid' <p>Search strategy to use:     - 'grid': exhaustive search using GridSearchCV     - 'random': random search using RandomizedSearchCV</p> int, default=10 <p>Number of parameter combinations to try (only used for random search).</p> float, optional <p>If provided, samples this fraction (e.g., 0.1 = 10%) of total data.</p> int, optional <p>If provided, caps total sample count to this number.</p> bool, default=False <p>If True, reduces CV splits to 2 and skips plots/logs for speed.</p> bool, default=True <p>Whether to print intermediate logs and tuning results.</p> bool, default=False <p>If True, returns dictionary of performance summaries, best parameters, and figures.</p> {'tabulate', 'rich'}, default='tabulate' <p>Controls formatting of logs \u2014 rich formatting vs plain ASCII tables.</p> bool, default=True <p>If False, disables visualization of model comparisons.</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--returns","title":"Returns","text":"<p>dict, optional     If <code>return_results=True</code>, returns:     - 'summary': DataFrame with final evaluation of all models/metrics     - 'results': Nested dictionary of raw scores and best params per model/metric     - 'best_params': Extracted best hyperparameters per model and metric     - 'figures': Optional matplotlib plots (if show_plots is True)</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If any required argument (<code>X</code>, <code>y</code>, <code>model_dict</code>, <code>param_grids</code>) is missing.</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--examples","title":"Examples","text":"<p>from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier</p> <p>models = { ...     'logistic': LogisticRegression(max_iter=200), ...     'rf': RandomForestClassifier() ... }</p> <p>grids = { ...     'logistic': {'C': [0.1, 1, 10]}, ...     'rf': {'n_estimators': [50, 100]} ... }</p> <p>results = run_nested_cv_multiclass_classification( ...     X=X, y=y, ...     model_dict=models, ...     param_grids=grids, ...     scoring_list=['accuracy', 'f1_macro'], ...     search_method='grid', ...     show_plots=True, ...     return_results=True ... )</p> <p>print(results['summary'])</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--notes","title":"Notes","text":"<p>\ud83d\udd01 What is Nested Cross-Validation?     Nested CV is used to provide an unbiased estimate of model performance while tuning hyperparameters.     The outer loop evaluates model generalization, while the inner loop tunes parameters.</p> <p>\ud83d\udcca Supported Metrics:     - accuracy, f1_macro, f1_weighted, precision_macro, recall_macro, etc.     - Custom scoring functions are also supported.</p> <p>\u2699\ufe0f Recommended Settings:     - Use grid search for small, well-bounded hyperparameter grids.     - Use random search with <code>n_iter</code> for wide or expensive search spaces.     - Set <code>fast_mode=True</code> for quick experiments or when plotting/logging isn\u2019t needed.</p> <p>\ud83d\udcc9 Memory-Saving Tips:     - Use <code>sample_frac</code> or <code>max_samples</code> for faster computation.     - Avoid plotting in low-memory environments by disabling <code>show_plots</code>.</p> <p>\ud83d\udce6 Outputs:     - Compact table of results across models/metrics     - Visual comparisons of bounded (0\u20131) and unbounded metrics     - Structured dictionary for pipeline integration or downstream analysis</p>"},{"location":"api/multiclass_classification/#batwing_ml.run_nested_cv_multiclass_classification--see-also","title":"See Also","text":"<ul> <li>GridSearchCV, RandomizedSearchCV (sklearn)</li> <li>evaluate_multiclass_classification_model: For detailed model diagnostics post-training</li> </ul>"},{"location":"api/multiclass_classification/#evaluation-utilities-multiclass","title":"Evaluation utilities (multiclass)","text":"<pre><code>from batwing_ml import evaluate_multiclass_classification\n</code></pre>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification","title":"<code>evaluate_multiclass_classification(model=None, X_train=None, y_train=None, X_test=None, y_test=None, cv=5, validation_params=None, scoring_curve='accuracy', verbose=True, return_dict=False, return_model_only=False, export_model=False, extra_plots=None, sample_fraction=None, sample_size=None, fast_mode=False)</code>","text":"<p>Evaluate a multiclass classification model with diagnostics, metrics, plots, and model handling.</p> <p>This function evaluates a scikit-learn-compatible classifier across multiple classes using both standard and advanced metrics. It supports unfitted or pretrained models, can train during evaluation, and provides visual diagnostics such as learning curves, calibration plots, and confusion matrices. It is ideal for model auditing, experiment evaluation, and production diagnostics in multiclass settings.</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn-compatible classifier. If not fitted, it will be trained using X_train and y_train.</p> array-like, optional <p>Training feature matrix. Required if the model is not already fitted.</p> array-like, optional <p>Training labels.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing labels.</p> int, default=5 <p>Number of cross-validation folds used for learning and validation curve plotting.</p> dict, optional <p>Dictionary of hyperparameter names mapped to value lists (e.g., {'max_depth': [2, 4, 6]}). Used for plotting validation curves.</p> str, default='accuracy' <p>Scoring metric used during learning and validation curve generation.</p> bool, default=True <p>If True, prints key metrics, classification reports, and shows plots.</p> bool, default=False <p>If True, returns a dictionary containing evaluation metrics and confusion matrix.</p> bool, default=False <p>If True, returns only the trained model. Ignores all metric outputs.</p> bool, default=False <p>If True and return_dict=True, includes the trained model in the output dictionary.</p> list of str, optional <p>Additional diagnostic plots to generate. Options:     - \"calibration\": Reliability plots per class (One-vs-Rest format)</p> float, optional <p>Fraction of the test set to use during evaluation (e.g., 0.1 = 10%).</p> int, optional <p>Absolute number of rows to use from the test set.</p> bool, default=False <p>If True, disables all visualizations and print outputs for fast evaluation.</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Fitted model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>return_dict=True</code> and <code>export_model=True</code></p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If required inputs are missing or invalid combinations of sample parameters are specified.</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--examples","title":"Examples","text":"<p>evaluate_multiclass_classification(         model=clf,         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         validation_params={'max_depth': [2, 4, 6]},         scoring_curve='f1_macro',         extra_plots=['calibration'],         return_dict=True     )</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--return-only-the-trained-model","title":"Return only the trained model","text":"<p>clf = evaluate_multiclass_classification(         model=RandomForestClassifier(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         return_model_only=True     )</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--use-pretrained-model-for-diagnostics","title":"Use pretrained model for diagnostics","text":"<p>from joblib import load model = load(\"clf.joblib\") evaluate_multiclass_classification(model=model, X_test=Xt, y_test=yt)</p>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You have a multiclass classifier (trained or not) and want a complete diagnostic report.     \u2022 You need reliable evaluation across multiple classes using macro-averaged metrics.     \u2022 You want visual feedback to compare models, understand performance, or debug issues.     \u2022 You want exportable results for dashboards, model cards, or batch audit pipelines.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 Accuracy:         \u2192 Overall proportion of correct predictions.         \u2192 Suitable for balanced class distributions.</p> <pre><code>\u2022 Macro Precision:\n    \u2192 Average precision across all classes.\n    \u2192 Treats all classes equally, regardless of support.\n\n\u2022 Macro Recall:\n    \u2192 Average recall across all classes.\n    \u2192 Useful when missing any class matters equally.\n\n\u2022 Macro F1 Score:\n    \u2192 Harmonic mean of macro precision and recall.\n    \u2192 Good for imbalanced multiclass classification.\n\n\u2022 ROC AUC (OVR):\n    \u2192 One-vs-Rest AUC computed across all classes.\n    \u2192 Reflects model's ability to separate each class from the rest.\n</code></pre> <p>\ud83d\udcc8 Optional Diagnostic Plots:     \u2022 Calibration Curve (<code>extra_plots=['calibration']</code>)         \u2192 For each class, compares predicted probabilities vs. true outcomes in One-vs-Rest fashion.         \u2192 Helps assess if predicted probabilities are well-calibrated.</p> <pre><code>\u2022 Learning Curve:\n    \u2192 Plots training/validation performance as a function of training set size.\n    \u2192 Helps detect underfitting, overfitting, and whether more data may help.\n\n\u2022 Validation Curve (`validation_params={'max_depth': [...]}`)\n    \u2192 Shows how model performance changes with different values of one hyperparameter.\n    \u2192 Useful for tuning complexity (e.g., tree depth, number of estimators).\n</code></pre> <p>\u26a0 Binary-Only Features Removed:     \u2022 Threshold tuning curves, KS statistic, DET curve, lift curve, and cost-sensitive loss are not supported       in multiclass context due to their reliance on binary decision boundaries.</p> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skips all visual output and logs. Great for large-scale evaluations or scripts.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Quickly fit and retrieve a model from the evaluation process.\n\n\u2022 export_model=True:\n    \u2192 Use in pipelines to return both evaluation metrics and model object in one step.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Useful when working with large datasets. Enables quick prototyping or evaluation subsets.\n</code></pre>"},{"location":"api/multiclass_classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--see-also","title":"See Also","text":"<ul> <li>preprocess_dataframe() : For feature cleaning and encoding</li> <li>run_nested_cv_classification() : For nested cross-validation and model comparison</li> <li>summary_dataframe(), summary_column() : For EDA and column profiling</li> <li>evaluate_classification_model() : Binary classification version</li> </ul>"},{"location":"api/regression/","title":"Regression API","text":"<p>This page documents the regression-side entry points in Batwing-ML.</p> <ul> <li>Optuna-based hyperparameter tuning</li> <li>Nested cross-validation for regression models</li> <li>Rich regression evaluation and diagnostics</li> </ul> <p>As before, if mkdocstrings complains about an import, adjust the dotted module paths below.</p>"},{"location":"api/regression/#hyperparameter-tuning-regression","title":"Hyperparameter tuning (regression)","text":"<pre><code>from batwing_ml import hyperparameter_tuning_regression\n</code></pre>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression","title":"<code>hyperparameter_tuning_regression(X=None, y=None, model_class=None, param_grid=None, scoring='r2', n_trials=50, cv_folds=5, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83d\udd27 Hyperparameter Tuning for Regression (via Optuna)</p> <p>Optimize hyperparameters of any regression model using Optuna\u2019s efficient  search. This function supports K-fold cross-validation, flexible scoring, sampling controls, reproducibility, and optionally returns the best fitted model.</p> <p>Ideal for leaderboard-style tuning, pipeline integration, and experimentation across research or production ML environments.</p>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix with shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector of regression outputs.</p> callable <p>A scikit-learn-style regressor class (e.g., <code>RandomForestRegressor</code>, <code>SVR</code>). Not an instance \u2013 must be the class itself.</p> dict <p>Dictionary mapping hyperparameter names to Optuna sampling functions. Example:     {         \"alpha\": lambda t: t.suggest_float(\"alpha\", 0.001, 10, log=True),         \"fit_intercept\": lambda t: t.suggest_categorical(\"fit_intercept\", [True, False])     }</p> str or callable, default='r2' <p>Scoring metric to optimize. Supports: - Any sklearn string (e.g., 'neg_root_mean_squared_error', 'r2', 'neg_mean_absolute_error') - A custom scoring function with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of Optuna optimization trials.</p> int, default=5 <p>Number of folds in K-Fold cross-validation for each trial.</p> {'maximize', 'minimize'}, default='maximize' <p>Optimization goal \u2014 e.g., maximize R\u00b2, or minimize RMSE.</p> bool, default=True <p>If True, logs model name, metric, best results, and parameter table.</p> bool, default=True <p>If True, trains and returns the best model on the entire dataset.</p> int, default=42 <p>Seed for reproducible splits and optimization behavior.</p> float or None, optional <p>If set, randomly samples a fraction of the data (e.g., 0.2 = 20%).</p> int or None, optional <p>If set, samples up to a fixed number of rows (e.g., 10000). Applied after <code>use_fraction</code>.</p> bool, default=False <p>If True, reduces number of trials to 10, disables print logs, and speeds up execution.</p>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--returns","title":"Returns","text":"<p>dict     Contains:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.Study     - 'best_model' : fitted model (if return_model=True)</p>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--raises","title":"Raises","text":"<p>ValueError     If scoring is neither a valid string nor a callable function.</p>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use: - You're optimizing regression models across different parameter sets. - You want to avoid grid search overhead with smarter sampling. - You\u2019re working with large datasets or want reproducible trials. - You prefer flexible, callable-based metric optimization.</p> <p>\ud83d\udccc Key Components:</p> <p>\u2022 param_grid:     Define hyperparameter ranges using Optuna\u2019s trial suggestions \u2014 more expressive than traditional grids.     Examples: <code>suggest_float</code>, <code>suggest_int</code>, <code>suggest_categorical</code>, <code>suggest_loguniform</code>.</p> <p>\u2022 scoring:     Use built-in sklearn scorers like 'r2', 'neg_root_mean_squared_error', or define your own function.</p> <p>\u2022 Subsampling:     Use <code>use_fraction</code> and/or <code>use_n_samples</code> to downsample large datasets while tuning.</p> <p>\u2022 fast_mode:     Cuts down <code>n_trials</code> to 10 and disables verbose printing. Great for initial testing.</p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import GradientBoostingRegressor results = hyperparameter_tuning_regression(         X=X, y=y,         model_class=GradientBoostingRegressor,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 10),             \"learning_rate\": lambda t: t.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),         },         scoring='neg_root_mean_squared_error',         fast_mode=True,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use log-scale for continuous values (e.g., learning rates, regularization strength). - Prefer <code>neg_root_mean_squared_error</code> or <code>neg_mean_absolute_error</code> for cost-aware regression. - Use <code>r2</code> when model interpretability or variance explanation is the goal.</p>"},{"location":"api/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--see-also","title":"See Also","text":"<p>evaluate_regression_model : For full regression metrics and plots after training. Optuna : https://optuna.org GridSearchCV : Classic alternative using exhaustive search.</p>"},{"location":"api/regression/#nested-cross-validation-regression","title":"Nested cross-validation (regression)","text":"<pre><code>from batwing_ml import run_nested_cv_regression\n</code></pre> <p>\ud83d\udd01 Nested Cross-Validation for Regression Models with Hyperparameter Tuning and Scoring Diagnostics</p> <p>Evaluate multiple regression models using nested cross-validation with internal hyperparameter tuning.  Supports both GridSearchCV and RandomizedSearchCV, optional scaling, rich or tabulate logging styles,  metric customization, sampling controls, and visual diagnostics. Useful for benchmarking, model selection,  and automated experimentation.</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix of shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Regression target values of shape (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn-compatible regressors.</p> dict of str \u2192 dict <p>Dictionary mapping model names to hyperparameter grids (sklearn format).</p> list of str or callables, default=['r2'] <p>List of scoring metrics to evaluate (e.g., 'r2', 'rmse', 'mae', or custom functions).</p> int, default=3 <p>Number of outer CV folds used for unbiased performance estimation.</p> int, default=3 <p>Number of inner CV folds used for hyperparameter tuning.</p> int, default=42 <p>Seed for reproducibility across cross-validation and sampling.</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function to determine whether a model should use standard scaling.</p> sklearn-compatible transformer or Pipeline, optional <p>Custom preprocessing steps. Overrides automatic scaling if provided.</p> {'grid', 'random'}, default='grid' <ul> <li>'grid': exhaustive grid search.</li> <li>'random': randomized search using <code>n_iter</code> samples from the grid.</li> </ul> int, default=10 <p>Number of iterations for random search (used only when <code>search_method='random'</code>).</p> float, optional <p>If provided, randomly samples a fraction of the data (e.g., 0.1 = 10%).</p> int, optional <p>If provided, limits total training size to this number of rows.</p> bool, default=False <p>If True, reduces CV folds and skips visuals/logs for faster experimentation.</p> bool, default=True <p>Whether to print progress, best parameters, and summary tables.</p> bool, default=False <p>If True, returns a dictionary with detailed results and summary plots.</p> {'tabulate', 'rich'}, default='tabulate' <p>Formatting style for printed summaries (console-friendly or rich-colored).</p> bool, default=True <p>If True, generates metric comparison plots using matplotlib/seaborn.</p> bool, default=False <p>If True, normalizes metric scales for side-by-side comparison across metrics.</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--returns","title":"Returns","text":"<p>dict, optional     Only if <code>return_results=True</code>:     - 'summary': pd.DataFrame summarizing all models and metrics     - 'results': full nested CV scores and best params     - 'best_params': dictionary of best params for each model/metric     - 'figures': matplotlib figure objects (if plots were generated)</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--raises","title":"Raises","text":"<p>ValueError     If required inputs (X, y, model_dict, param_grids) are missing.</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--examples","title":"Examples","text":"<p>models = {         'linear': LinearRegression(),         'rf': RandomForestRegressor()     }</p> <p>grids = {         'linear': {},         'rf': {'n_estimators': [50, 100], 'max_depth': [3, 5]}     }</p> <p>run_nested_cv_regression(         X=X, y=y,         model_dict=models,         param_grids=grids,         scoring_list=['r2', 'rmse', 'mae'],         search_method='random',         n_iter=5,         fast_mode=True,         show_plots=False     )</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--notes","title":"Notes","text":"<p>\ud83d\udd0d Scoring Options:     \u2022 Standard metrics: 'r2', 'adjusted_r2', 'mae', 'rmse', 'rmsle', 'mape'     \u2022 Custom scorers: pass a callable with (y_true, y_pred) \u2192 float     \u2022 Negative scores (e.g., RMSE) are automatically converted to positive.</p> <p>\ud83d\udcc8 Visualization:     - Score plots (e.g., R\u00b2, explained variance) are grouped separately from error plots (e.g., RMSE, MAE).     - Use <code>normalize=True</code> to enable relative comparison across all metrics in a single chart.</p> <p>\u26a1 Tips:     - Use <code>sample_frac</code> or <code>max_samples</code> for speed on large datasets.     - <code>fast_mode=True</code> is useful for test runs or CI/CD pipelines.     - Use <code>return_results=True</code> to integrate output into reports or notebooks.</p>"},{"location":"api/regression/#batwing_ml.run_nested_cv_regression--see-also","title":"See Also","text":"<ul> <li>run_nested_cv_classification : For binary/multiclass classification</li> <li>evaluate_regression_model : For detailed regression diagnostics</li> <li>GridSearchCV, RandomizedSearchCV : Sklearn tuning tools</li> </ul>"},{"location":"api/regression/#evaluation-utilities-regression","title":"Evaluation utilities (regression)","text":"<pre><code>from batwing_ml import evaluate_regression_model\n</code></pre> <p>Evaluate a regression model with metrics, diagnostics, visualization, and export handling.</p> <p>This function evaluates a scikit-learn-compatible regressor using a variety of performance metrics. It supports trained and untrained models, optionally fits them, and generates plots to analyze model performance, including learning curves, residuals, and error distributions.</p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn-compatible regressor. If unfitted, the function will train it using provided data.</p> array-like, optional <p>Training feature matrix. Required if model is not already fitted.</p> array-like, optional <p>Training target vector.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing target vector.</p> int, default=5 <p>Number of cross-validation folds used for learning and validation curves.</p> dict, optional <p>Dictionary of hyperparameter names to lists of values for validation curve plotting.</p> str, default='r2' <p>Scoring metric used for the learning and validation curves.</p> bool, default=True <p>If True, prints all metrics and visualizations.</p> bool, default=False <p>If True, returns computed metrics in a dictionary.</p> bool, default=False <p>If True, returns only the trained model, suppressing all metrics and plots.</p> bool, default=False <p>If True, returns a tuple of (metrics_dict, trained_model).</p> list of str, optional <p>Diagnostic visualizations to generate. Supported values:     - 'pred_vs_actual': Predicted vs Actual scatterplot     - 'residuals': Residuals vs Fitted plot     - 'error_dist': Histogram of prediction errors     - 'qq': Q-Q plot of residuals     - 'feature_importance': Feature importance bar plot (if supported)     - 'learning': Learning curve plot     - 'validation': Validation curve(s) for specified parameters</p> dict, optional <p>Dictionary of user-defined metric functions with format: {name: callable(y_true, y_pred)}.</p> bool, default=False <p>If True, log1p-transform both <code>y_test</code> and predictions before metric computation.</p> bool, default=False <p>If True, disables visual output and reduces verbosity for speed. Ideal for batch evaluations.</p> float, optional <p>If set, uses a random subset of the test data by fraction (e.g., 0.1 = 10%).</p> int, optional <p>If set, uses only the specified number of rows from the test data.</p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Trained model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>export_model=True</code></p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--raises","title":"Raises","text":"<p>ValueError     If both <code>sample_fraction</code> and <code>sample_size</code> are specified simultaneously.</p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--examples","title":"Examples","text":"<p>evaluate_regression_model(         model=LinearRegression(),         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         extra_plots=['pred_vs_actual', 'residuals', 'learning'],         return_dict=True     )</p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--save-and-export-model","title":"Save and export model","text":"<p>metrics, trained_model = evaluate_regression_model(         model=RandomForestRegressor(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         export_model=True     )</p>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You want a comprehensive evaluation of a regression model with minimal boilerplate.     \u2022 You need insight into prediction accuracy, residuals, and learning behavior.     \u2022 You're experimenting with different models and want standardized reports.     \u2022 You want to benchmark models quickly on subsets of large datasets.     \u2022 You wish to export trained models alongside their evaluation scores.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 R\u00b2 (R-squared):         \u2192 Proportion of variance in target explained by the model.         \u2192 Values closer to 1 indicate better performance.</p> <pre><code>\u2022 Adjusted R\u00b2:\n    \u2192 Penalized R\u00b2 for the number of predictors used. More robust to overfitting.\n\n\u2022 RMSE (Root Mean Squared Error):\n    \u2192 Measures average magnitude of error. Sensitive to outliers.\n\n\u2022 MAE (Mean Absolute Error):\n    \u2192 Measures average absolute deviation. Robust and interpretable.\n\n\u2022 MAPE (Mean Absolute Percentage Error):\n    \u2192 Scales MAE by the magnitude of the true value. Not defined for zero targets.\n\n\u2022 RMSLE (Root Mean Squared Log Error):\n    \u2192 Useful when dealing with targets across several orders of magnitude. Ignores underestimation penalties.\n\n\u2022 Custom Metrics:\n    \u2192 Pass any function with signature `func(y_true, y_pred)` for domain-specific scoring.\n</code></pre> <p>\ud83d\udcc8 Diagnostic Visualizations (enabled via <code>extra_plots</code>):     \u2022 Predicted vs Actual:         \u2192 Shows how closely predictions align with ground truth.</p> <pre><code>\u2022 Residual Plot:\n    \u2192 Detects heteroskedasticity or model bias across fitted values.\n\n\u2022 Error Distribution:\n    \u2192 Histogram of residuals, useful for checking skew.\n\n\u2022 Q-Q Plot:\n    \u2192 Checks if residuals follow normal distribution (key assumption in linear models).\n\n\u2022 Feature Importance:\n    \u2192 For tree models, displays relative contribution of each feature.\n\n\u2022 Learning Curve:\n    \u2192 Shows training vs validation performance across increasing sample sizes.\n\n\u2022 Validation Curve:\n    \u2192 Visualizes sensitivity to specific hyperparameters.\n</code></pre> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skips all visual output and logging. Best for CI jobs or looped experimentation.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Returns the fitted model only (no metrics or plots). Handy for pipelines.\n\n\u2022 export_model=True:\n    \u2192 Returns a tuple of (metrics_dict, trained_model) for downstream use.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Great for fast prototyping or large dataset downsampling.\n</code></pre>"},{"location":"api/regression/#batwing_ml.evaluate_regression_model--see-also","title":"See Also","text":"<ul> <li>sklearn.metrics : Reference for all built-in scoring functions</li> <li>run_nested_cv_regression : For comparing multiple regression models</li> <li>summary_dataframe(), preprocess_dataframe() : For EDA &amp; preprocessing utilities</li> </ul>"},{"location":"data-and-exploration/data-validation-etl/","title":"Data Validation &amp; ETL","text":"<p>High-level data cleaning and schema validation for tabular data.</p> <p>Batwing-ML gives you a single entry point to:</p> <ul> <li>Fix obvious schema issues</li> <li>Coerce dtypes</li> <li>Handle missing values in a consistent way</li> <li>Produce an audit trail of what changed</li> </ul> <pre><code>from batwing_ml import validate_and_clean_data\n</code></pre>"},{"location":"data-and-exploration/data-validation-etl/#api","title":"API","text":"<p>Validate and clean a raw Pandas DataFrame with schema enforcement, common ETL transformations,  and full audit logging \u2014 designed for production-ready ML pipelines and interactive data workflows.</p> <p>This function performs all critical preprocessing validations: - Ensures column names and data types are as expected - Removes duplicates, trims and cleans strings - Parses date columns, applies range/category checks - Tracks actions in a structured report - Optionally saves audit report as HTML and JSON</p>"},{"location":"data-and-exploration/data-validation-etl/#batwing_ml.validate_and_clean_data--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The raw input DataFrame to validate and clean. This should be the first step in your ML pipeline.</p> dict, optional <p>A dictionary mapping expected column names to their expected pandas-compatible data types (e.g., 'int', 'float'). Used to coerce types using <code>.astype()</code> and flag columns that don't match.</p> <p>Example:</p> <p>schema = {\"age\": \"int\", \"income\": \"float\", \"signup_date\": \"datetime64[ns]\"}</p> dict, optional <p>Dictionary to rename columns before validation. Useful when dealing with inconsistent column headers from different data sources.</p> <p>Example:</p> <p>rename_map = {\"SignUpDate\": \"signup_date\"}</p> bool, default=True <p>If True, removes duplicate rows using <code>df.drop_duplicates()</code>. Duplicates removed are logged in the report.</p> bool, default=True <p>If True, attempts to cast each column to the type specified in the <code>schema</code>. Safe conversions only; fails silently and skips invalid conversions.</p> bool, default=True <p>If True, automatically detects and parses columns that contain 'date' in their name using <code>pd.to_datetime()</code>.</p> dict, optional <p>A dictionary specifying numeric columns with acceptable (min, max) value ranges. Rows violating the range are not dropped but counted and reported.</p> <p>Example:</p> <p>enforce_ranges = {\"age\": (0, 120), \"income\": (0, 1_000_000)}</p> list of str, optional <p>Columns that must contain unique values (e.g., primary keys). Violations are logged, not dropped.</p> <p>Example:</p> <p>unique_columns = [\"email\", \"user_id\"]</p> dict, optional <p>Dictionary mapping column names to a list of allowed values. Invalid category entries are flagged in the report.</p> <p>Example:</p> <p>category_constraints = {\"gender\": [\"male\", \"female\", \"other\"]}</p> bool, default=True <p>If True, strips leading/trailing spaces and lowercases all object-type (string) columns.</p> bool, default=True <p>If True, converts all column names to <code>snake_case</code> for consistency across downstream ML workflows.</p> bool, default=True <p>If True, prints basic progress logs to console. Does not affect HTML output.</p> bool, default=True <p>If True, displays a beautiful scrollable HTML summary in notebooks and returns the full metadata dictionary.</p> str, optional <p>File path to save the report as an <code>.html</code> file. Uses the same style as your notebook HTML output.</p> <p>Example:</p> <p>report_path = \"validation_report.html\"</p> str, optional <p>If provided, saves the full validation report dictionary as a <code>.json</code> file for use in audit pipelines, logging systems, etc.</p> <p>Example:</p> <p>export_json = \"validation_metadata.json\"</p> list of str, optional <p>List of keywords (e.g., 'email', 'ssn', 'phone') to scan for in column names to flag potential PII fields.</p> <p>Example:</p> <p>pii_keywords = [\"email\", \"phone\", \"ssn\"]</p>"},{"location":"data-and-exploration/data-validation-etl/#batwing_ml.validate_and_clean_data--returns","title":"Returns","text":"<p>df_clean : pd.DataFrame     The cleaned and type-coerced DataFrame, safe for downstream modeling or transformation.</p> dict <p>A structured dictionary summarizing all validation checks, transformation steps, and column-level actions. Contains keys such as: 'missing_values', 'coerced_columns', 'pii_suspects', etc.</p>"},{"location":"data-and-exploration/data-validation-etl/#batwing_ml.validate_and_clean_data--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic usage with report:</p> <p>df_clean, report = validate_and_clean_data(df, schema={\"age\": \"int\", \"income\": \"float\"})</p> <p>\u25b6\ufe0f Add renaming and range checks:</p> <p>df_clean, report = validate_and_clean_data(         df,         rename_map={\"SignUpDate\": \"signup_date\"},         enforce_ranges={\"age\": (0, 100)}     )</p> <p>\u25b6\ufe0f Generate downloadable audit report:</p> <p>validate_and_clean_data(         df, schema=schema, report_path=\"validation.html\", export_json=\"report.json\"     )</p>"},{"location":"data-and-exploration/data-validation-etl/#batwing_ml.validate_and_clean_data--notes","title":"Notes","text":"<ul> <li>Columns not in schema are ignored during type coercion</li> <li>Violations are logged but data is not dropped unless explicitly enabled</li> <li>You can use this before any ML function like <code>preprocess_dataframe()</code> or <code>feature_engineering()</code></li> <li>The audit report helps ensure trust, governance, and repeatability in data flows</li> </ul>"},{"location":"data-and-exploration/data-validation-etl/#batwing_ml.validate_and_clean_data--related","title":"Related","text":"<p>\u2022 preprocess_dataframe() \u2013 for full-scale modeling prep \u2022 feature_exploration() \u2013 for checking feature quality \u2022 prepare_sample_split() \u2013 for reproducible sampling/splitting \u2022 pandas_profiling (external) \u2013 for deep statistical reports \u2022 great_expectations (optional) \u2013 for declarative expectations</p>"},{"location":"data-and-exploration/feature-engineering/","title":"Feature Engineering","text":"<p>High-level feature selection and transformation utilities.</p> <p>Feature engineering helps you:</p> <ul> <li>Remove low-signal or redundant features</li> <li>Create transformed or synthetic features</li> <li>Optionally apply dimensionality reduction</li> </ul> <pre><code>from batwing_ml import feature_engineering\n</code></pre>"},{"location":"data-and-exploration/feature-engineering/#api","title":"API","text":""},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering","title":"<code>feature_engineering(df, target=None, mode='both', task=None, apply_changes=True, strategy='auto', top_k=None, fast_mode=False, show_preview=True, return_metadata=True, pca_components=None, clustering=False, cluster_k=None, cluster_feature_name='cluster_label', plots=None, selection_method='auto', selection_threshold=0.01)</code>","text":"<p>Perform automatic feature selection, engineering, and clustering in one unified function.</p> <p>This function is built for data scientists, analysts, or ML engineers who want to streamline the feature preprocessing pipeline with just one function call \u2014 while maintaining transparency, reproducibility, and full control over what happens to their features.</p>"},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset where each row is an observation and each column is a feature.     This is the only required argument \u2014 all others are optional and context-dependent.</p> str, optional <p>The name of the target column (i.e., label or output variable). This is necessary for supervised selection techniques like: - Mutual Information (<code>mutual_info</code>) - Model-based importance (<code>model</code>) - Lasso regression (<code>lasso</code>) - Recursive Feature Elimination (<code>rfe</code>)</p> <p>\u26a0\ufe0f If you don't pass this, selection methods requiring supervision will be skipped.</p> {'selection', 'engineering', 'both'}, default='both' <p>Controls what parts of the pipeline are executed: - 'selection': Only selects features based on some criteria. - 'engineering': Only adds new features (e.g., polynomial or PCA). - 'both': Runs selection first, then engineering.</p> <p>\ud83d\udc49 Tip: Use 'selection' during early cleanup. Use 'engineering' later before modeling.</p> {'regression', 'classification'}, optional <p>Type of ML problem. If not specified, it will be inferred from <code>target</code>: - Regression: If target is numeric with many unique values. - Classification: If target is binary or categorical.</p> <p>This controls scoring logic for feature selection (e.g., model choice, MI scoring).</p> bool, default=True <ul> <li>True \u2192 Apply selection/engineering steps directly to the data.</li> <li>False \u2192 Simulate the pipeline, only generate metadata and preview output.</li> </ul> <p>\u2705 Use <code>apply_changes=False</code> if you're testing or auditing steps first.</p> {'auto', 'manual'}, default='auto' <p>Controls engineering behavior: - 'auto': Automatically applies PolynomialFeatures and PCA if context is appropriate. - 'manual': Skip all engineering unless explicitly requested (e.g., you pass <code>pca_components</code>).</p> int, optional <p>Number of features to retain in methods like RFE or when displaying top-N importances.</p> <p>\ud83d\udccc Useful if you want to aggressively reduce feature dimensionality.</p> bool, default=False <p>Enables performance-safe mode: - Disables model fitting, plotting, clustering, and PCA. - Ideal for large datasets or batch scripts.</p> <p>\u26a0\ufe0f No importance plots or PCA will run if this is True.</p> bool, default=True <p>Displays a styled scrollable table showing: - Which features are kept - Which are engineered - Which were dropped</p> <p>\u2714\ufe0f Highly recommended in Jupyter/Colab for visual tracking.</p> bool, default=True <p>If True, returns a <code>_FeatureEngineeringSteps</code> object summarizing what happened.</p> <p>Contains: - <code>selected_features</code> - <code>dropped_features</code> - <code>created_features</code> - <code>transforms_applied</code></p> int, optional <p>Number of Principal Components to add. - Applies PCA on numeric features - Useful for dimensionality reduction and multicollinearity handling</p> <p>\ud83d\udcca If you add <code>\"pca\"</code> to <code>plots</code>, it shows a variance-explained curve.</p> bool, default=False <p>Enables KMeans clustering on numeric columns.</p> <p>\u2705 Adds a new column (<code>cluster_feature_name</code>) to represent the assigned cluster.</p> int, optional <p>Number of clusters (k) to use. - If None: shows elbow plot of SSE vs k. - If set: assigns clusters directly.</p> str, default='cluster_label' <p>Name of the column that stores the cluster number for each row. You can rename this if you\u2019re using multiple clustering passes.</p> list of str, optional <p>Visualizations to generate (optional, ignored in fast mode): - <code>\"importance\"</code>: Bar chart of top RandomForest importances - <code>\"pca\"</code>: Line plot showing cumulative variance from PCA - <code>\"clusters\"</code>: 2D PCA plot showing colored clusters</p> <p>\u2795 Add one or more based on your workflow.</p> str, optional <p>Method used for selecting features: - 'variance' \u2192 Drops constant or low-variance features - 'correlation' \u2192 Drops one of each highly correlated pair (based on threshold) - 'mutual_info' \u2192 Mutual Information score against the target - 'model' \u2192 Tree-based embedded feature importance (RandomForest) - 'lasso' \u2192 Uses coefficients from Lasso regression (regression only) - 'rfe' \u2192 Recursive Feature Elimination (Linear/Logistic) - 'auto' \u2192 Auto-selects based on task (classification/regression)</p> float, default=0.01 <p>Threshold for filtering-based methods: - If using <code>'correlation'</code>: Drop if abs(corr) &gt; threshold - If using <code>'mutual_info'</code>: Keep if MI score &gt; threshold</p> <p>\ud83d\udccc Lower values retain more features, higher values are stricter.</p>"},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering--returns","title":"Returns","text":"<p>pd.DataFrame     The transformed dataset with selected and engineered features.</p> <p>_FeatureEngineeringSteps (if return_metadata=True)     A rich summary of steps applied \u2014 for logs, reports, and audits.</p>"},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering--examples","title":"Examples","text":"<p>\u25b6\ufe0f Full pipeline:</p> <p>X, steps = feature_engineering(         df=raw_df,         target='target',         mode='both',         strategy='auto',         selection_method='model',         pca_components=3,         clustering=True,         cluster_k=5,         plots=['importance', 'pca', 'clusters']     )</p> <p>\u25b6\ufe0f Dry run (no changes, just audit):</p> <p>X, log = feature_engineering(df, target='target', apply_changes=False)</p> <p>\u25b6\ufe0f PCA for dimensionality reduction:</p> <p>X, meta = feature_engineering(df, mode='engineering', pca_components=2)</p> <p>\u25b6\ufe0f Lightweight filter-based selection:</p> <p>X, meta = feature_engineering(df, target='target', mode='selection', selection_method='mutual_info')</p>"},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering--notes","title":"Notes","text":"<ul> <li>Use <code>return_metadata=True</code> to track what changed \u2014 crucial for reproducibility.</li> <li>Feature selection happens before feature creation.</li> <li>All steps are logged and previewed before being applied.</li> <li>PCA and clustering only work on numeric columns.</li> </ul>"},{"location":"data-and-exploration/feature-engineering/#batwing_ml.feature_engineering.feature_engineering--see-also","title":"See Also","text":"<p>\u2022 feature_exploration() \u2013 For diagnostics before applying changes \u2022 preprocess_dataframe() \u2013 For imputation, encoding, and basic cleanup \u2022 summary_dataframe() \u2013 For an overview of raw features \u2022 evaluate_classification_model() \u2013 To measure final model quality</p>"},{"location":"data-and-exploration/feature-exploration/","title":"Feature Exploration","text":"<p>Lightweight utilities to inspect relationships between features and the target.</p> <p>Use this step to:</p> <ul> <li>Identify potentially predictive features</li> <li>Spot obvious noise or leakage</li> <li>Decide what to keep, drop, or transform before engineering</li> </ul> <pre><code>from batwing_ml import feature_exploration\n</code></pre>"},{"location":"data-and-exploration/feature-exploration/#api","title":"API","text":""},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration","title":"<code>feature_exploration(df, target=None, task=None, top_n=20, corr_threshold=0.95, skew_threshold=1.0, sample_size=None, fast_mode=False, export_path=None, tree_importance=True, perm_importance=True, show_preview=True, return_summary=False, heavy_ops_sample=5000, plots=None)</code>","text":"<p>Quickly analyze the quality and behavior of your dataset's features, with rich statistical summaries, automated warnings, optional modeling, and beautiful visualizations.</p> <p>This function helps you discover: - What features matter most for prediction? - Which features are redundant or constant? - Which columns may require transformation (e.g., skewed)? - How do categorical features relate to the target?</p> <p>Ideal for data scientists, analysts, and ML engineers who want to explore datasets before modeling.</p>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset. Each column is treated as a potential feature.</p> <pre><code>\u2705 Required: Must be a clean rectangular DataFrame (no nested structures).\n</code></pre> str, optional <p>The name of the target (dependent) variable in your dataset.</p> <p>Required for supervised diagnostics like: - Mutual Information (MI) - Correlation with target - Tree-based feature importance - Permutation importance - Grouped statistics for categorical features</p> <p>Example:</p> <p>target = \"SalePrice\"</p> {'regression', 'classification', 'multiclass'}, optional <p>Type of ML problem: - 'regression': Predicting continuous values (e.g., prices, temperature) - 'classification': Binary prediction (e.g., spam or not spam) - 'multiclass': More than 2 classes (e.g., sentiment = low/medium/high)</p> <p>\u2705 If not passed, the function will auto-detect using target column data type and uniqueness.</p> int, default=20 <p>How many top features to display in: - Summary table - Importance plots - Skewed distribution visualizations</p> <p>Tip: Increase to 50 or 100 for wide datasets with many features.</p> float, default=0.95 <p>Threshold above which two numeric features are considered highly correlated (redundant).</p> <ul> <li>Used to flag potential multicollinearity in the summary</li> <li>Helps you decide which features might be dropped</li> </ul> <p>Example: - A corr of 0.98 between 'age' and 'years_since_birth' may signal redundancy</p> float, default=1.0 <p>Skewness threshold for numeric features.</p> <ul> <li>Columns with absolute skew &gt; threshold are flagged</li> <li>These may benefit from transformation like <code>log(x+1)</code> or power scaling</li> </ul> <p>Tip: Highly skewed features can reduce model performance (especially linear models).</p> int, optional <p>If set, the DataFrame is sampled down to this number of rows before computing summaries.</p> <p>Useful for: - Large datasets (100k+ rows) - Speeding up exploration - Limiting memory usage</p> <p>Example:</p> <p>sample_size = 10000</p> bool, default=False <p>Turns off all compute-intensive operations: - Mutual information - Tree-based model fitting - Permutation importance - Plots</p> <p>\u2705 Use this for quick diagnostics on very large datasets or inside pipelines.</p> str, optional <p>If set, saves the summary table as a CSV.</p> <p>Example:</p> <p>export_path = \"feature_summary.csv\"</p> bool, default=True <p>If enabled and target is provided: - Fits a RandomForest model - Extracts feature importances - Displays top-N most predictive features</p> <p>\u26a0\ufe0f Ignored if <code>fast_mode=True</code>.</p> bool, default=True <p>If enabled: - Computes permutation-based feature importance (model-agnostic) - More stable than tree importances</p> <p>\u26a0\ufe0f Slower. Use only for small to mid-sized datasets or when needed.</p> bool, default=True <p>Displays the output table in a scrollable, styled HTML block (dark-theme compatible).</p> <p>Recommended for: - Jupyter Notebook - Google Colab - VSCode notebooks</p> bool, default=False <p>If True, the summary table (a DataFrame) is returned.</p> <p>Useful when you want to: - Save to Excel/CSV manually - Merge with other reports - Visualize in another tool</p> int, default=5000 <p>For compute-heavy steps like model fitting, mutual info, or permutation: - Only this many rows are sampled from the DataFrame</p> <p>Keeps everything fast and memory-efficient.</p> list of str, optional <p>List of visualizations to include. Choose any of:</p> <ul> <li>'importance': Bar plot of top features by tree or permutation importance</li> <li>'correlation': Heatmap of correlation between numeric features</li> <li>'skewed': Histogram of skewed numeric columns</li> <li>'grouped': Bar plots of mean target by category (only for regression)</li> </ul> <p>Example:</p> <p>plots = [\"importance\", \"correlation\", \"skewed\"]</p>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--returns","title":"Returns","text":"<p>pd.DataFrame or None     - If <code>return_summary=True</code>: Returns a summary DataFrame with suggestions     - If <code>return_summary=False</code>: Displays summary and plots only</p>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic usage:</p> <p>feature_exploration(df, target=\"SalePrice\", task=\"regression\")</p> <p>\u25b6\ufe0f With visuals and full scoring:</p> <p>feature_exploration(         df,         target=\"target\",         task=\"regression\",         plots=[\"importance\", \"correlation\", \"skewed\", \"grouped\"]     )</p> <p>\u25b6\ufe0f Fast mode scan:</p> <p>feature_exploration(df, fast_mode=True)</p> <p>\u25b6\ufe0f Export results:</p> <p>feature_exploration(df, target=\"target\", export_path=\"summary.csv\")</p> <p>\u25b6\ufe0f Capture summary in a variable:</p> <p>summary_df = feature_exploration(df, return_summary=True)</p>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--when-to-use","title":"When to Use","text":"<p>\u2705 Before modeling: to identify top features, poor features, or potential issues \u2705 After cleaning: to detect skew, high cardinality, or multicollinearity \u2705 In pipelines: to auto-generate feature insight reports \u2705 In dashboards: to track data quality in production  </p>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--notes","title":"Notes","text":"<ul> <li>Correlation = Pearson for numeric features</li> <li>Skewness is calculated using SciPy\u2019s <code>skew()</code></li> <li>Importance plots use RandomForest or Permutation models</li> <li>Grouped stats only apply to regression targets and categorical features</li> </ul>"},{"location":"data-and-exploration/feature-exploration/#batwing_ml.feature_exploration.feature_exploration--related","title":"Related","text":"<p>\u2022 feature_engineering() \u2013 to act on features after diagnosing \u2022 preprocess_dataframe() \u2013 for cleaning before feature exploration \u2022 summary_dataframe() \u2013 to get statistical overview of the full DataFrame \u2022 evaluate_classification_model() \u2013 to inspect how features affect model accuracy</p>"},{"location":"data-and-exploration/preprocessing/","title":"Preprocessing","text":"<p>Turn a cleaned DataFrame into a modeling-ready feature matrix.</p> <p>This step handles:</p> <ul> <li>Missing values</li> <li>Encoding categoricals</li> <li>Scaling numeric features</li> <li>Dropping junk / constant / high-missing columns</li> </ul> <pre><code>from batwing_ml import preprocess_dataframe, preprocess_column\n</code></pre>"},{"location":"data-and-exploration/preprocessing/#dataframe-level-preprocessing","title":"DataFrame-level preprocessing","text":"<p>Preprocess a tabular dataset for machine learning using a streamlined, transparent pipeline.</p> <p>This function performs automatic and configurable preprocessing on a pandas DataFrame,  applying common transformations such as missing value imputation, categorical encoding,  numeric feature scaling, and filtering of low-information or problematic features.</p> <p>It is designed to make raw datasets \"model-ready\" with minimal manual effort, while still allowing for full control over each preprocessing step.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The raw input dataset to preprocess. Must be a standard, rectangular DataFrame.</p> bool, default=True <p>If True, fills missing values using the specified <code>numeric_method</code> and <code>categorical_method</code>.</p> {'mean', 'median', 'constant'}, default='mean' <p>Strategy to fill missing values in numeric columns: - 'mean': use column average - 'median': use column median - 'constant': fill with 0</p> {'mode', 'constant', 'drop'}, default='mode' <p>Strategy to fill missing values in categorical columns: - 'mode': fill with most frequent category - 'constant': fill with \"missing\" - 'drop': skip imputation for categoricals</p> float, default=0.3 <p>Drop columns with more than this proportion of missing values (e.g., 0.3 = 30%).</p> {'onehot', 'ordinal', None}, default='onehot' <p>How to encode categorical columns: - 'onehot': expand categories into binary columns (ideal for tree-based and linear models) - 'ordinal': convert categories into integers (compact but model-sensitive) - None: skip encoding</p> {'standard', 'minmax', 'robust', None}, default='standard' <p>Scaling method for numeric features: - 'standard': zero mean, unit variance (default for most ML models) - 'minmax': scales features to [0, 1] - 'robust': scales using median and IQR (more resistant to outliers) - None: skip scaling</p> bool, default=True <p>If True, drops columns where all values are the same \u2014 these provide no predictive value.</p> int or None, default=100 <p>If set, drops categorical columns with more than this many unique values. Useful to eliminate IDs or high-entropy features that aren't generalizable.</p> bool, default=False <p>If True, returns a second object (<code>steps</code>) summarizing the preprocessing pipeline steps. This is useful for auditing, documentation, or reproducing pipelines.</p> bool, default=False <p>If True, displays a styled scrollable HTML preview of the top rows of the transformed DataFrame. Intended for use inside notebooks (e.g., Jupyter, Colab).</p> bool, default=True <p>If True, prints human-readable logs describing each transformation as it is applied.</p> bool, default=False <p>If True, disables logging and previews to optimize performance for large datasets (1M+ rows). Recommended for production or batch processing pipelines.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--returns","title":"Returns","text":"<p>pd.DataFrame or Tuple[pd.DataFrame, dict]     - The transformed DataFrame, ready for use in ML models.     - If <code>return_steps=True</code>, also returns a dictionary-like object listing:         - dropped columns (due to high missingness, constant value, or high cardinality)         - columns encoded, scaled, or imputed         - final column names</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--examples","title":"Examples","text":"<p>df_clean = preprocess_dataframe(df)</p> <p>df_clean, steps = preprocess_dataframe(         df,         encode=\"ordinal\",         scale=\"robust\",         drop_missing_thresh=0.25,         return_steps=True,         preview=True     ) print(steps)</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--notes","title":"Notes","text":"<ul> <li>This function does not modify the input DataFrame (works on a copy).</li> <li>Designed to be flexible enough for prototyping, reproducible for experiments,   and fast enough for production workloads.</li> <li>Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops.</li> <li>For column-wise control, see <code>preprocess_column()</code>.</li> </ul>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--see-also","title":"See Also","text":"<p>preprocess_column : Clean and transform a single Series with similar options.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--user-guide","title":"User Guide","text":"<p>\ud83e\udded When Should You Use This Function? - You're starting with raw, messy tabular data that has missing values, mixed data types, or irrelevant columns. - You want to transform the dataset into a clean, numerical form ready for modeling \u2014 without hardcoding pipelines manually. - You're preparing data for machine learning algorithms (scikit-learn, XGBoost, LightGBM, etc.) and need a reproducible cleaning strategy. - You're in an experimentation phase and want fast iteration with logs, or you're moving toward deployment and need performance mode.</p> <p>\u2699\ufe0f Recommended Configurations (Use-Case Based)</p> <ol> <li>General-purpose ML modeling (balanced tabular data):    \u2192 Works well with logistic regression, SVMs, and shallow neural networks. <p>preprocess_dataframe(df, encode=\"onehot\", scale=\"standard\")</p> </li> </ol> <p>Why? One-hot encoding ensures categorical variables are treated independently. Standard scaling helps models converge.</p> <ol> <li>Tree-based models (RandomForest, XGBoost, LightGBM):    \u2192 These models handle ordinal input well and don\u2019t require scaling. <p>preprocess_dataframe(df, encode=\"ordinal\", scale=None)</p> </li> </ol> <p>Why? One-hot can add noise or bloat tree-based models. Ordinal + no scaling is faster and sufficient.</p> <ol> <li>High-cardinality datasets or sparse data (e.g., recommender systems): <p>preprocess_dataframe(df, encode=None, max_cardinality=50)</p> </li> </ol> <p>Why? Skip encoding and limit high-cardinality columns to avoid exploding the feature space.</p> <ol> <li>Production or big data batch preprocessing:    \u2192 Great for pipelines where performance &gt; visuals. <p>preprocess_dataframe(df, fast_mode=True)</p> </li> </ol> <p>Why? Disables all logging and display overhead \u2014 ideal for 1M+ rows.</p> <ol> <li>Auditing or debugging preprocessing behavior:    \u2192 You want to know exactly what was changed and why. <p>df_clean, steps = preprocess_dataframe(df, return_steps=True, verbose=True) print(steps)</p> </li> </ol> <p>Why? <code>steps</code> logs dropped columns, encoded features, and final output \u2014 great for versioning and reproducibility.</p> <p>\ud83d\udca1 Tips: - Use <code>preview=True</code> only in notebooks to visualize output cleanly. - Set <code>max_cardinality=None</code> to retain all categorical columns, even high-card ones. - <code>drop_constant=True</code> removes junk features automatically. - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - Use <code>preprocess_column()</code> for detailed tuning on a single feature.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_dataframe--related","title":"Related:","text":"<p>\u2022 preprocess_column(): Clean and transform a single Series with similar options. \u2022 summary_column(): View deep stats for a single column before preprocessing \u2022 summary_dataframe(): View deep stats for a DataFrame before preprocessing</p>"},{"location":"data-and-exploration/preprocessing/#column-level-preprocessing","title":"Column-level preprocessing","text":"<p>Preprocess a single column of data with smart transformations for modeling.</p> <p>This function allows targeted preprocessing of a single pandas Series \u2014 such as a feature column from a DataFrame \u2014 including missing value imputation, outlier treatment, encoding, scaling,  custom transformations, and optional inspection via plots or previews.</p> <p>It is useful for exploratory analysis, fine-tuned feature engineering, or inspecting column-specific cleaning steps outside of full-pipeline automation.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_column--parameters","title":"Parameters","text":"<p>col : pd.Series     The input column to clean. Can be numeric or categorical.</p> {'mean', 'median', 'mode', 'constant'} or scalar, optional <p>Strategy to fill missing values: - 'mean', 'median': numeric only - 'mode': most frequent value (works for both types) - 'constant': fill with 0 or \"missing\" - scalar: fill with a specific value (e.g., 0 or 'unknown')</p> {'standard', 'minmax', 'robust'}, optional <p>If provided, applies scaling (numeric only): - 'standard': zero mean, unit variance - 'minmax': rescale to [0, 1] - 'robust': scale based on median and IQR (for outliers)</p> {'ordinal', 'onehot'}, optional <p>If provided, applies encoding (categorical only): - 'ordinal': converts categories to integer codes - 'onehot': returns a new DataFrame with binary indicator columns</p> {'zscore', 'iqr'}, optional <p>Method for outlier detection and clipping (numeric only): - 'zscore': clips values with |Z| &gt; 3 - 'iqr': clips values outside 1.5 * IQR from Q1/Q3</p> tuple of float, default=(0.01, 0.99) <p>Lower and upper quantiles to cap values (a form of Winsorization). Applied only for numeric columns.</p> callable, optional <p>A custom function applied to every non-null element (e.g., <code>np.log1p</code>, <code>str.lower</code>). Useful for transformations like scaling, mapping, or normalization.</p> {'manual', 'auto'}, default='manual' <ul> <li>'manual': use the specified arguments only</li> <li>'auto': infer reasonable defaults based on column dtype and missing values</li> </ul> bool, default=False <p>If True, modifies the original Series inside a DataFrame. Otherwise works on a copy.</p> bool, default=False <p>If True, prints a small sample (head) of the cleaned column.</p> bool, default=False <p>If True, shows a histogram or bar plot (before/after if possible) to visualize the distribution.</p> bool, default=True <p>If True, logs steps taken (e.g., imputed with mean, encoded with ordinal).</p> bool, default=False <p>If True, disables plotting and previews for faster execution on large data.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_column--returns","title":"Returns","text":"<p>pd.Series or Tuple[pd.Series, dict]     - Cleaned Series (or one-hot encoded DataFrame if applicable).     - If used in unpacking (<code>col_clean, steps = ...</code>), also returns a dictionary       detailing the preprocessing actions taken.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_column--examples","title":"Examples","text":"<p>col_clean = preprocess_column(df[\"age\"], impute=\"mean\", scale=\"standard\")[0]</p> <p>cat_col, steps = preprocess_column(         df[\"gender\"],         impute=\"mode\",         encode=\"ordinal\",         preview=True,         verbose=True     ) print(steps)</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_column--user-guide","title":"User Guide","text":"<p>\ud83e\udded When Should You Use This? - You want fine-grained control over a single column in your dataset \u2014 e.g., apply different transformations for different features. - You\u2019re doing exploratory data analysis and want to inspect the impact of transformations before applying them in batch. - You\u2019re manually curating a feature set for a model and want to test encoding, scaling, or outlier treatment interactively. - You\u2019re building a custom preprocessing function per column for pipelines.</p> <p>\u2699\ufe0f Recommended Workflows (Based on Column Type)</p> <ol> <li>For numeric columns (e.g., age, income, price):    Apply standard cleaning + scaling + clipping: <p>col_clean, steps = preprocess_column(            df[\"income\"],            impute=\"mean\",            scale=\"standard\",            cap_outliers=\"zscore\",            cap_quantiles=(0.01, 0.99),            preview=True,            plot=True        )</p> </li> </ol> <p>Why? Numeric data benefits from scaling and outlier handling for better model convergence.</p> <ol> <li>For categorical columns (e.g., gender, city):    Apply imputation and ordinal encoding: <p>col_clean = preprocess_column(            df[\"gender\"],            impute=\"mode\",            encode=\"ordinal\"        )[0]</p> </li> </ol> <p>Why? Many models expect numeric input; ordinal encoding works well for tree models.</p> <ol> <li>When exploring transformation effects:    Use custom mapping functions or log transforms: <p>preprocess_column(df[\"price\"], custom_map=np.log1p, plot=True)</p> </li> </ol> <p>Why? Helps reduce skew in price-like data, which improves linear model performance.</p> <ol> <li>Quick automation:    Let the function auto-decide how to handle the column: <p>preprocess_column(df[\"feature_x\"], strategy=\"auto\")</p> </li> </ol> <p>Why? Saves time when you\u2019re cleaning many features quickly.</p> <p>\ud83d\udd0d Tips: - Use <code>preview=True</code> to view how the column looks after cleaning. - Use <code>plot=True</code> to visualize distributions before/after transformations. - Use <code>steps</code> output to document what happened to the column \u2014 especially useful in notebooks or reports. - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - One-hot encoding returns a DataFrame (not Series) \u2014 plan how you store or merge it back.</p>"},{"location":"data-and-exploration/preprocessing/#batwing_ml.preprocess_column--related","title":"Related:","text":"<p>\u2022 preprocess_dataframe(): Clean an entire DataFrame in one call \u2022 summary_column(): View deep stats for a single column before preprocessing \u2022 summary_dataframe(): View deep stats for a DataFrame before preprocessing</p>"},{"location":"data-and-exploration/sampling-and-splitting/","title":"Sampling &amp; Splitting","text":"<p>Helpers for sampling your dataset and creating reproducible train/test (and optional validation) splits.</p> <p>Use this when you want to:</p> <ul> <li>Subsample large datasets for faster iteration</li> <li>Create consistent train/test splits with clear parameters</li> <li>Avoid copy-pasted split logic across notebooks</li> </ul> <pre><code>from batwing_ml import prepare_sample_split\n</code></pre>"},{"location":"data-and-exploration/sampling-and-splitting/#api","title":"API","text":"<p>Sample and/or split your dataset for training workflows with optional stratification, reproducibility,  metadata tracking, and notebook-friendly summaries.</p> <p>This function is ideal for: - Efficiently working with large datasets by subsampling - Creating reproducible train/test splits with or without class balance - Getting audit-ready metadata (row counts, strategy used, seed) - Easily integrating into notebook-based data science pipelines</p>"},{"location":"data-and-exploration/sampling-and-splitting/#batwing_ml.prepare_sample_split--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The full dataset from which sampling and/or splitting is performed. Each row is an observation.</p> str, optional <p>Column name to use for stratification (classification problems). Required only if <code>stratify=True</code>. Can be ignored for regression tasks or unsupervised workflows.</p> <p>Example:</p> <p>target='label'</p> int, optional <p>Number of rows to sample from the dataset before splitting. If None, the entire dataset is used.</p> <p>Example:</p> <p>sample_size=10000</p> bool, default=False <p>Whether to split the sampled/full dataset into train and test sets.</p> <p>If True, you get <code>(train_df, test_df)</code> or <code>(train_df, test_df, metadata)</code>. If False, only a single DataFrame is returned.</p> float, default=0.2 <p>Proportion of test set if <code>split=True</code>.</p> <p>Example:</p> <p>test_size=0.3  # 70/30 split</p> bool, default=False <p>If True, ensures that train/test (or sample) retains class balance by stratifying using the <code>target</code>.</p> <p>Recommended for classification tasks.</p> int, default=42 <p>Sets the random state for both sampling and splitting to ensure full reproducibility.</p> <p>Example:</p> <p>random_seed=123</p> bool, default=False <p>If True, returns the list of original indices of the sampled DataFrame.</p> <p>Example:</p> <p>df_sampled, idx = prepare_sample_split(df, sample_size=5000, return_indices=True)</p> bool, default=False <p>If True, returns a detailed dictionary summarizing: - number of rows before/after sampling - stratification status - train/test split sizes (if split=True) - random seed used</p> <p>Metadata is also displayed in scrollable HTML format when using notebooks.</p> bool, default=False <p>If True, prints internal logging messages about the sampling/splitting process.</p> callable, optional <p>Optional custom logging function (e.g., <code>logger.info</code>) to capture messages in logs or dashboards.</p> <p>Example:</p> <p>prepare_sample_split(df, log_callback=my_logger)</p>"},{"location":"data-and-exploration/sampling-and-splitting/#batwing_ml.prepare_sample_split--returns","title":"Returns","text":"<p>pd.DataFrame or tuple     - If <code>split=False</code>: returns a sampled <code>pd.DataFrame</code>, or with indices/metadata if requested.     - If <code>split=True</code>: returns <code>(train_df, test_df)</code> or <code>(train_df, test_df, metadata)</code>.</p>"},{"location":"data-and-exploration/sampling-and-splitting/#batwing_ml.prepare_sample_split--examples","title":"Examples","text":"<p>\u25b6\ufe0f Basic sampling:</p> <p>df_sampled = prepare_sample_split(df, sample_size=5000)</p> <p>\u25b6\ufe0f Sampling + splitting:</p> <p>train_df, test_df = prepare_sample_split(df, sample_size=10000, split=True)</p> <p>\u25b6\ufe0f Stratified split with metadata:</p> <p>train_df, test_df, meta = prepare_sample_split(         df, target='label', stratify=True, split=True, return_metadata=True     )</p> <p>\u25b6\ufe0f Sample with reproducibility:</p> <p>df_sampled = prepare_sample_split(df, sample_size=3000, random_seed=123)</p> <p>\u25b6\ufe0f With logging hook:</p> <p>prepare_sample_split(df, log_callback=lambda msg: print(f\"[LOG]: {msg}\"))</p>"},{"location":"data-and-exploration/sampling-and-splitting/#batwing_ml.prepare_sample_split--notes","title":"Notes","text":"<ul> <li>Internally uses <code>sklearn.model_selection.train_test_split()</code> for splitting and stratification.</li> <li>Always resets index to prevent downstream issues with row alignment.</li> <li>Stratification only works with classification-style discrete targets.</li> </ul>"},{"location":"data-and-exploration/sampling-and-splitting/#batwing_ml.prepare_sample_split--related","title":"Related","text":"<p>\u2022 feature_exploration() \u2014 run after sampling to analyze feature quality \u2022 evaluate_classification_model() \u2014 use after train/test split for performance metrics \u2022 feature_engineering() \u2014 to transform features post sampling/split \u2022 preprocess_dataframe() \u2014 clean the data before modeling</p>"},{"location":"data-and-exploration/summaries-and-profiling/","title":"Summaries &amp; Profiling","text":"<p>Notebook-friendly summaries for quickly understanding your cleaned dataset.</p> <p>Use these utilities to inspect:</p> <ul> <li>Column types and missingness</li> <li>Basic statistics</li> <li>Potential junk or leakage columns</li> </ul> <pre><code>from batwing_ml import summary_dataframe, summary_column\n</code></pre>"},{"location":"data-and-exploration/summaries-and-profiling/#dataframe-summary","title":"DataFrame summary","text":"<p>Generates a detailed summary report of an entire DataFrame for exploratory data analysis (EDA).</p> <p>This function provides a transparent, column-by-column overview of your dataset, including data types, missing value patterns, uniqueness, cardinality, and optionally deeper statistical insights like skewness, kurtosis, entropy, and correlation structure.</p> <p>It helps you understand the shape and quality of your data before modeling, and supports structured auditing via optional DataFrame outputs for downstream usage.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input dataset to analyze. Must be a rectangular DataFrame with rows and columns.</p> bool, default=True <p>If True, prints human-readable summaries using styled tables, markdown, or HTML (in notebooks). If <code>fast_mode=True</code>, this will be ignored (no prints will be shown).</p> bool, default=False <p>If True, returns the core summary tables as pandas DataFrames for programmatic use.</p> bool, default=False <p>If True, enables additional statistics: - For numeric: skewness, kurtosis, and z-score-based outlier counts - For categorical: entropy (how evenly distributed the values are) - Also shows duplicate row and column analysis</p> bool, default=False <p>If True, computes a Pearson correlation matrix for all numeric features.</p> bool, default=False <p>If True, disables all visual and computationally expensive diagnostics: - Skips entropy, skewness, kurtosis, outliers, and duplicate detection - Skips correlation matrix computation - Skips verbose display/logging Use this mode when profiling large datasets (e.g., 1M+ rows) or in batch workflows.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--returns","title":"Returns","text":"<p>tuple of pd.DataFrame, optional     Only returned if <code>return_dataframes=True</code>. Includes:     - summary : Core metadata (dtype, missing %, unique %, etc.)     - desc_numeric : Descriptive stats for all numeric columns     - desc_categorical : Descriptive stats for categorical/object columns     - correlation_matrix : Numeric correlation matrix (only if <code>correlation_matrix=True</code>)</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--raises","title":"Raises","text":"<p>ValueError     If the input DataFrame is empty or not valid.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--examples","title":"Examples","text":"<p>summary_dataframe(df, detailing=True, correlation_matrix=True)</p> <p>summary, num_stats, cat_stats = summary_dataframe(df, return_dataframes=True, detailing=True)</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--notes","title":"Notes","text":"<ul> <li>The function is non-destructive: it reads from the input DataFrame without modifying it.</li> <li>If you're working with extremely large datasets, set <code>fast_mode=True</code> to avoid slow diagnostics.</li> <li>When <code>verbose=True</code>, this function uses IPython\u2019s HTML renderer for a notebook-friendly display.</li> </ul>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--see-also","title":"See Also","text":"<p>summary_column : Analyze a single column with detailed metrics and plots preprocess_dataframe : Prepare a dataset for modeling through scaling, encoding, and imputation preprocess_column : Clean a single column manually (e.g., outlier handling, transformation)</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.exploratory.summary_dataframe--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When Should You Use This? - At the start of a project to assess data readiness. - Before feature engineering to identify columns to drop, fix, or transform. - During EDA or notebook exploration to communicate data quality. - In automated pipelines where you need programmatic summary outputs.</p> <p>\ud83d\udccc What You'll Learn: - Which columns have high missingness, low variance, or high cardinality - How many numeric/categorical features exist - Skewness or entropy in features (if detailing=True) - Whether your dataset has duplicated rows or columns - Correlation patterns among numeric features (optional)</p> <p>\u2699\ufe0f Recommended Usage Patterns:</p> <ol> <li> <p>Full EDA diagnostic (notebooks):</p> <p>summary_dataframe(df, detailing=True, correlation_matrix=True)</p> </li> <li> <p>For dashboards or programmatic reporting:</p> <p>summary, num_stats, cat_stats = summary_dataframe(df, return_dataframes=True)</p> </li> <li> <p>Batch analysis or large files:</p> <p>summary_dataframe(df, fast_mode=True)</p> </li> <li> <p>Minimal quick check (CLI or scripts):</p> <p>summary_dataframe(df, detailing=False, verbose=True)</p> </li> </ol> <p>\ud83d\udca1 Tips: - Use with <code>preprocess_dataframe()</code> to act on low-quality features you identify here. - <code>entropy</code> close to 0 \u2192 one category dominates (low information) - High skew/kurtosis \u2192 consider log or robust transformations - Z-score outliers &gt;10 \u2192 column likely needs clipping or scaling - Use <code>fast_mode=True</code> when processing high-volume datasets or in production loops. - Use <code>detailing=False</code> for a quick overview of the dataset without deep stats. - Use <code>correlation_matrix=False</code> to skip the correlation matrix. - Use <code>return_dataframes=True</code> to export summaries to reports or ML audit logs</p>"},{"location":"data-and-exploration/summaries-and-profiling/#column-summary","title":"Column summary","text":"<p>Generates a detailed, human-readable summary of a single column in a DataFrame.</p> <p>This function helps you understand the nature of a specific column by providing summary metrics such as missingness, uniqueness, cardinality, entropy, skewness, and optional visualizations. It adapts intelligently to both numeric and categorical data and can highlight distribution issues, outliers, or missing trends over time.</p> <p>It is ideal for exploratory data analysis (EDA), column-wise diagnostics, and auditing feature quality before preprocessing or modeling.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame containing the column to summarize.</p> str <p>The name of the column to analyze.</p> int, default=10 <p>Number of most frequent values to display in the frequency table for categorical features.</p> bool, default=True <p>If True, prints all summaries in formatted tables with headers. If <code>fast_mode=True</code>, this is ignored.</p> bool, default=False <p>If True, returns: - summary_table : key metrics (missing %, unique %, etc.) - desc_stats : descriptive stats (mean, std, IQR, etc.) - freq_dist : frequency counts of top values</p> bool, default=True <p>If True, enables additional diagnostics: - For numeric: skewness, kurtosis, z-score outliers - For categorical: entropy - Also enables visualizations if <code>plots</code> is specified</p> str, optional <p>If specified, enables a missing-value trend chart over time using this datetime column. Useful for temporal datasets and time-series analysis.</p> list of str, optional <p>List of plots to display: - \"histogram\": for numeric distribution - \"bar\": for top category counts - \"missing_trend\": for missing rate over time (requires <code>time_column</code>)</p> bool, default=False <p>If True, skips all optional visualizations, skew/entropy calculations, and pretty print tables. Recommended when analyzing very large datasets or when integrating into production pipelines.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--returns","title":"Returns","text":"<p>tuple of pd.DataFrame, optional     If <code>return_dataframes=True</code>, returns:     - summary_table : base profile of the column     - desc_stats : statistical or categorical description     - freq_dist : top-N frequency breakdown</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--raises","title":"Raises","text":"<p>ValueError     If the specified column does not exist in the DataFrame.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--examples","title":"Examples","text":"<p>summary_column(df, \"salary\", detailing=True, plots=[\"histogram\"])</p> <p>col_stats, desc, top_vals = summary_column(         df,         \"product_category\",         detailing=True,         plots=[\"bar\"],         return_dataframes=True     )</p> <p>summary_column(df, \"discount\", fast_mode=True)</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--notes","title":"Notes","text":"<ul> <li>The function detects whether the column is numeric or categorical and adapts its metrics accordingly.</li> <li>Outlier detection (z-score) is only applied to numeric features with sufficient variance.</li> <li>Plots are automatically skipped when <code>fast_mode=True</code>.</li> </ul>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--see-also","title":"See Also","text":"<p>summary_dataframe : Summarizes all columns of a DataFrame at once. preprocess_column : Cleans and transforms a single column based on rules. preprocess_dataframe : End-to-end preprocessing pipeline for the entire DataFrame.</p>"},{"location":"data-and-exploration/summaries-and-profiling/#batwing_ml.summary_column--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When Should You Use This? - You want to audit or explore one column in depth. - You're deciding how to impute, encode, or drop a specific column. - You want to visualize category frequency or numeric distribution interactively. - You're building an automated column-report pipeline (with return_dataframes=True).</p> <p>\u2699\ufe0f Recommended Use Cases</p> <ol> <li> <p>Understand a numeric column with outliers:</p> <p>summary_column(df, \"loan_amount\", detailing=True, plots=[\"histogram\"])</p> </li> <li> <p>Explore a categorical feature for feature engineering:</p> <p>summary_column(df, \"device_type\", top_n=5, detailing=True, plots=[\"bar\"])</p> </li> <li> <p>Check for seasonal missingness (e.g., sensors or logs):</p> <p>summary_column(df, \"temperature\", time_column=\"timestamp\", plots=[\"missing_trend\"])</p> </li> <li> <p>Automation or fast analysis at scale:</p> <p>summary_column(df, \"user_age\", fast_mode=True)</p> </li> </ol> <p>\ud83d\udca1 Tips: - Use with <code>preprocess_column()</code> to apply encoding or transformation after diagnosis. - If <code>entropy</code> is very low, the column may have little signal or be constant. - use 'fast_mode'=True<code>for large datasets to skip slow diagnostics. - Use</code>detailing=False<code>for a quick overview of the column without deep stats. - For many zero-variance columns, use</code>summary_dataframe()<code>for batch detection. - Always use</code>return_dataframes=True` if building custom reports or logging stats.</p>"},{"location":"guides/classification-workflow/","title":"Tabular Classification Workflow","text":"<p>This guide shows the recommended end-to-end flow for classification problems using Batwing-ML.</p> <p>It assumes you already understand the core pieces from the Home and Getting Started pages and want a slightly more opinionated recipe for day\u2011to\u2011day work.</p>"},{"location":"guides/classification-workflow/#1-load-and-validate-your-data","title":"1. Load and validate your data","text":"<p>Start from a raw CSV or table and immediately bring it into a clean, auditable state.</p> <pre><code>import pandas as pd\nfrom batwing_ml import validate_and_clean_data\n\ndf = pd.read_csv(\"data.csv\")\n\n# Validate schema, fix dtypes, handle obvious issues\ndf_clean, audit = validate_and_clean_data(df)\n</code></pre> <p>Use <code>audit</code> (often a dict or DataFrame-like structure) to quickly see:</p> <ul> <li>Which columns were renamed or dropped</li> <li>What type conversions happened</li> <li>How missing values were handled</li> </ul> <p>In a notebook, just display <code>audit</code> to inspect it.</p>"},{"location":"guides/classification-workflow/#2-explore-the-cleaned-data","title":"2. Explore the cleaned data","text":"<p>Before you jump into models, get a feel for distributions, missingness, and basic relationships.</p> <pre><code>from batwing_ml import summary_dataframe, summary_column\n\nsummary = summary_dataframe(df_clean)\nsummary  # nice HTML in notebooks\n\n# Optional: deep dive into one column\nage_summary = summary_column(df_clean[\"age\"])\nage_summary\n</code></pre> <p>Typical questions to answer here:</p> <ul> <li>Are there obvious data quality issues left?</li> <li>Are there columns that are clearly junk / IDs / free text?</li> <li>Is the target heavily imbalanced?</li> </ul> <p>This is also a good place to manually decide which columns to keep or drop.</p>"},{"location":"guides/classification-workflow/#3-split-out-features-and-target","title":"3. Split out features and target","text":"<p>Keep the target separate from the start.</p> <pre><code>target_col = \"label\"\n\nX_raw = df_clean.drop(columns=[target_col])\ny = df_clean[target_col]\n</code></pre> <p>You\u2019ll feed <code>X_raw</code> into preprocessing and feature engineering next.</p>"},{"location":"guides/classification-workflow/#4-preprocess-features","title":"4. Preprocess features","text":"<p>Use Batwing-ML\u2019s preprocessing helper to handle missing values, encoding, scaling, and basic column cleanup.</p> <pre><code>from batwing_ml import preprocess_dataframe\n\nX_processed, prep_steps = preprocess_dataframe(\n    X_raw,\n    encode=\"onehot\",      # or \"ordinal\" depending on your use case\n    scale=\"standard\",     # or \"minmax\" / \"robust\" / None\n    drop_missing_thresh=0.3,\n    return_steps=True,\n)\n</code></pre> <p><code>prep_steps</code> will usually contain:</p> <ul> <li>Which columns were dropped and why (e.g., too many missing values, constant value)</li> <li>How numerical and categorical columns were treated</li> <li>Any encoding / scaling decisions</li> </ul> <p>This gives you a clean, numeric feature matrix ready for modeling.</p>"},{"location":"guides/classification-workflow/#5-feature-engineering","title":"5. Feature engineering","text":"<p>Once preprocessing is done, you can add more signal or reduce noise using <code>feature_engineering</code>.</p> <pre><code>from batwing_ml import feature_engineering\n\nX_fe, fe_meta = feature_engineering(\n    X_processed,\n    target=None,    # you can also pass the target name if needed by some strategies\n    mode=\"both\",   # e.g. \"selection\", \"generation\", or \"both\"\n)\n</code></pre> <p>Depending on your configuration, this step can:</p> <ul> <li>Drop low-variance or redundant features</li> <li>Use model\u2011based importance or mutual information to select features</li> <li>Add synthetic / transformed features</li> <li>Optionally apply dimensionality reduction (e.g., PCA)</li> </ul> <p><code>fe_meta</code> gives you a summary of what changed.</p> <p>Typically at this point you\u2019ll have:</p> <pre><code>X = X_fe  # final feature matrix\n</code></pre>"},{"location":"guides/classification-workflow/#6-trainvalidation-split","title":"6. Train/validation split","text":"<p>For quick iteration, use a standard train/test split. Nested CV will come later for more rigorous comparisons.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y,\n)\n</code></pre> <p><code>stratify=y</code> is strongly recommended for classification, especially with imbalanced classes.</p>"},{"location":"guides/classification-workflow/#7-hyperparameter-tuning-for-one-model-optuna","title":"7. Hyperparameter tuning for one model (Optuna)","text":"<p>Start with one solid baseline model and tune it with <code>hyperparameter_tuning_classification</code>.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom batwing_ml import hyperparameter_tuning_classification\n\nparam_grid = {\n    \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),\n    \"max_depth\":    lambda t: t.suggest_int(\"max_depth\", 3, 20),\n    \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),\n}\n\nresults = hyperparameter_tuning_classification(\n    X=X_train,\n    y=y_train,\n    model_class=RandomForestClassifier,\n    param_grid=param_grid,\n    scoring=\"f1_macro\",      # or \"roc_auc\" for binary, etc.\n    fast_mode=False,\n)\n\nbest_model = results[\"best_model\"]\nprint(results[\"best_params\"])\nprint(results[\"best_score\"])\n</code></pre> <p>Use <code>fast_mode=True</code> and/or <code>use_fraction</code> / <code>use_n_samples</code> when you want very fast feedback on large datasets.</p>"},{"location":"guides/classification-workflow/#8-evaluate-the-tuned-model","title":"8. Evaluate the tuned model","text":"<p>Once you have a tuned model, run a richer evaluation on the held\u2011out test set.</p> <pre><code>from batwing_ml import evaluate_classification_model\n\nmetrics = evaluate_classification_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    return_dict=True,\n)\n\nprint(metrics)\n</code></pre> <p>This typically gives you:</p> <ul> <li>Accuracy</li> <li>Precision, recall, F1</li> <li>ROC AUC (for binary)</li> <li>A classification report</li> <li>Confusion matrix and optional curves (depending on configuration)</li> </ul> <p>Use this to sanity\u2011check whether your model is sensible before you get fancy.</p>"},{"location":"guides/classification-workflow/#9-compare-multiple-models-with-nested-cv-optional-but-recommended","title":"9. Compare multiple models with nested CV (optional but recommended)","text":"<p>When you\u2019re ready to compare models more rigorously, use <code>run_nested_cv_classification</code>.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom batwing_ml import run_nested_cv_classification\n\nmodels = {\n    \"logistic\": LogisticRegression(max_iter=1000),\n    \"rf\": RandomForestClassifier(),\n    \"gboost\": GradientBoostingClassifier(),\n}\n\nparam_grids = {\n    \"logistic\": {\"C\": [0.1, 1, 10]},\n    \"rf\": {\"n_estimators\": [100, 300]},\n    \"gboost\": {\"learning_rate\": [0.01, 0.1], \"n_estimators\": [100, 200]},\n}\n\nnested = run_nested_cv_classification(\n    X=X,\n    y=y,\n    model_dict=models,\n    param_grids=param_grids,\n    scoring_list=[\"f1_macro\", \"roc_auc\"],\n    search_method=\"grid\",          # or \"random\"\n    return_results=True,\n)\n\nnested_summary = nested[\"summary\"]\nnested_summary\n</code></pre> <p>Nested CV helps you answer questions like:</p> <ul> <li>Which model family is actually best, once tuning is accounted for?</li> <li>How stable is performance across folds?</li> <li>Are you over\u2011tuning on one train/test split?</li> </ul>"},{"location":"guides/classification-workflow/#10-recommended-patterns","title":"10. Recommended patterns","text":"<p>A few practical tips when using this workflow:</p> <ul> <li>Always keep <code>X</code> and <code>y</code> separate. It\u2019s easier to avoid leakage.</li> <li>Log or save <code>audit</code>, <code>prep_steps</code>, and <code>fe_meta</code>. These make your experiments debuggable and reproducible.</li> <li>Use fast mode early. On big data, start with <code>fast_mode=True</code> + subsampling for quick feedback.</li> <li>Use nested CV when comparing model families. It\u2019s heavier, but much more honest.</li> </ul> <p>Once this workflow feels comfortable, you can apply the same structure to multiclass classification and regression with the corresponding Batwing-ML functions.</p>"},{"location":"guides/large-datasets-and-fast-mode/","title":"Working with Large Datasets &amp; Fast Mode","text":"<p>Batwing-ML includes several built\u2011in tools to help you move quickly when handling large datasets. This guide shows you the best practices for fast iteration, safe subsampling, and memory\u2011efficient experimentation.</p> <p>This is useful when:</p> <ul> <li>Your dataset has hundreds of thousands or millions of rows</li> <li>Hyperparameter tuning is too slow on full data</li> <li>Nested cross\u2011validation takes too long</li> <li>You\u2019re running experiments on a laptop or small VM</li> </ul>"},{"location":"guides/large-datasets-and-fast-mode/#1-the-philosophy-iterate-fast-refine-later","title":"1. The philosophy: iterate fast, refine later","text":"<p>For big tabular problems, the most productive workflow is:</p> <ol> <li>Subsample aggressively (1\u201320%)</li> <li>Use fast mode while tuning</li> <li>Use simplified model families early</li> <li>Scale up only when the pipeline is stable</li> </ol> <p>This lets you test ideas in seconds or minutes instead of hours.</p>"},{"location":"guides/large-datasets-and-fast-mode/#2-subsampling-your-dataset","title":"2. Subsampling your dataset","text":"<p>You can subsample before or during tuning.</p>"},{"location":"guides/large-datasets-and-fast-mode/#option-a-manual-subsample-before-the-pipeline","title":"Option A \u2014 Manual subsample before the pipeline","text":"<pre><code>df_sampled = df_clean.sample(frac=0.15, random_state=42)\n</code></pre> <p>Useful when:</p> <ul> <li>EDA is slow</li> <li>Preprocessing itself is heavy</li> </ul>"},{"location":"guides/large-datasets-and-fast-mode/#option-b-use-builtin-subsampling-in-the-tuning-utilities","title":"Option B \u2014 Use built\u2011in subsampling in the tuning utilities","text":"<p>All hyperparameter tuning functions support:</p> <ul> <li><code>use_fraction</code></li> <li><code>use_n_samples</code></li> <li><code>fast_mode</code></li> </ul> <p>Example:</p> <pre><code>results = hyperparameter_tuning_classification(\n    X=X,\n    y=y,\n    model_class=RandomForestClassifier,\n    param_grid=param_grid,\n    scoring=\"f1_macro\",\n    use_fraction=0.2,   # only 20% used for tuning\n)\n</code></pre> <p>Or:</p> <pre><code>results = hyperparameter_tuning_regression(\n    X=X,\n    y=y,\n    model_class=RandomForestRegressor,\n    param_grid=param_grid,\n    use_n_samples=20000,  # cap to 20K rows\n)\n</code></pre> <p>This keeps tuning fast even if the full dataset is huge.</p>"},{"location":"guides/large-datasets-and-fast-mode/#3-fast-mode-during-tuning","title":"3. Fast mode during tuning","text":"<p><code>fast_mode=True</code> is your shortcut for quick experimentation.</p> <pre><code>results = hyperparameter_tuning_classification(\n    X=X,\n    y=y,\n    model_class=RandomForestClassifier,\n    param_grid=param_grid,\n    fast_mode=True,\n)\n</code></pre> <p><code>fast_mode</code> usually:</p> <ul> <li>Reduces the number of Optuna trials</li> <li>Lowers CV folds</li> <li>Disables heavy plots or logs</li> <li>Reduces verbosity</li> </ul> <p>This is ideal when you want to test whether:</p> <ul> <li>The model type is appropriate</li> <li>The feature engineering strategy works</li> <li>Preprocessing looks correct</li> </ul> <p>Later, you can disable fast mode for the real search.</p>"},{"location":"guides/large-datasets-and-fast-mode/#4-fast-nested-cv","title":"4. Fast nested CV","text":"<p>Nested CV can be expensive because:</p> <ul> <li>Outer folds \u00d7 inner folds</li> <li>Each inner fold runs a full search</li> </ul> <p>For quick comparisons:</p> <pre><code>nested = run_nested_cv_classification(\n    X=X,\n    y=y,\n    model_dict=models,\n    param_grids=param_grids,\n    scoring_list=[\"f1_macro\"],\n    search_method=\"random\",   # faster than grid\n    max_samples=30000,          # limits per-fold data\n    fast_mode=True,\n    return_results=True,\n)\n</code></pre>"},{"location":"guides/large-datasets-and-fast-mode/#you-can-control-speed-with","title":"You can control speed with:","text":"<ul> <li><code>max_samples</code> \u2014 cap per-fold size</li> <li><code>fast_mode=True</code> \u2014 reduces folds + search depth</li> <li><code>search_method=\"random\"</code> \u2014 far lighter than grid</li> </ul> <p>These options let you compare model families in a reasonable time.</p>"},{"location":"guides/large-datasets-and-fast-mode/#5-downsampling-evaluation","title":"5. Downsampling evaluation","text":"<p>Sometimes even evaluation plots get slow on massive test sets.</p> <p>Batwing-ML\u2019s evaluation functions let you downsample:</p> <pre><code>metrics = evaluate_regression_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    sample_fraction=0.2,   # only 20% of test used\n    return_dict=True,\n)\n</code></pre> <p>Or:</p> <pre><code>metrics = evaluate_classification_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    sample_size=5000,      # use exactly 5000 rows\n)\n</code></pre> <p>This speeds up:</p> <ul> <li>Confusion matrix</li> <li>Residual plots</li> <li>ROC/PR curves</li> <li>Feature importance calculations</li> </ul>"},{"location":"guides/large-datasets-and-fast-mode/#6-tips-for-handling-truly-large-datasets","title":"6. Tips for handling truly large datasets","text":""},{"location":"guides/large-datasets-and-fast-mode/#use-categoricals-wisely","title":"\u2714\ufe0f Use categoricals wisely","text":"<p>High-cardinality features slow down one-hot encoding.</p> <ul> <li>Drop columns with extreme cardinality</li> <li>Switch to <code>encode=\"ordinal\"</code> when appropriate</li> </ul>"},{"location":"guides/large-datasets-and-fast-mode/#avoid-huge-dimensionality","title":"\u2714\ufe0f Avoid huge dimensionality","text":"<p>One-hot encoding on a million rows \u00d7 thousands of categories is expensive.</p> <p>Consider:</p> <ul> <li>Hashing</li> <li>PCA</li> <li>Feature selection</li> </ul>"},{"location":"guides/large-datasets-and-fast-mode/#start-with-simpler-models","title":"\u2714\ufe0f Start with simpler models","text":"<p>Tree-based models scale better early in experimentation.</p> <p>Use linear or simpler models when you want very fast iterations.</p>"},{"location":"guides/large-datasets-and-fast-mode/#use-small-earlystage-optuna-search","title":"\u2714\ufe0f Use small early\u2011stage Optuna search","text":"<pre><code>fast_mode=True\nuse_fraction=0.1\n</code></pre> <p>Then scale up once you\u2019re confident.</p>"},{"location":"guides/large-datasets-and-fast-mode/#7-recommended-quickiteration-recipe","title":"7. Recommended quick\u2011iteration recipe","text":"<p>For fast discovery:</p> <pre><code>results = hyperparameter_tuning_classification(\n    X=X,\n    y=y,\n    model_class=RandomForestClassifier,\n    param_grid=param_grid,\n    fast_mode=True,\n    use_fraction=0.1,\n)\n</code></pre> <p>Once your pipeline feels right:</p> <pre><code>results = hyperparameter_tuning_classification(\n    X=X,\n    y=y,\n    model_class=RandomForestClassifier,\n    param_grid=param_grid,\n    scoring=\"f1_macro\",\n    fast_mode=False,\n)\n</code></pre>"},{"location":"guides/large-datasets-and-fast-mode/#8-summary","title":"8. Summary","text":"<p>When working with large datasets in Batwing-ML:</p> <ul> <li>Subsample early, especially for EDA and prototyping</li> <li>Use fast_mode for tuning and nested CV</li> <li>Use random search methods instead of full grids</li> <li>Limit per-fold samples with max_samples</li> <li>Downsample evaluation with sample_size or sample_fraction</li> </ul> <p>This lets you stay productive while keeping your experiments honest and repeatable.</p>"},{"location":"guides/notebooks-and-visual-previews/","title":"Notebooks &amp; Visual Previews","text":"<p>Batwing-ML is designed to feel natural inside Jupyter Notebook, JupyterLab, and Google Colab. Many of its utilities return rich, styled HTML objects or generate diagnostic plots that display cleanly without extra setup.</p> <p>This guide walks you through how to get the best experience when using Batwing-ML inside interactive environments.</p>"},{"location":"guides/notebooks-and-visual-previews/#1-rich-html-summaries","title":"1. Rich HTML summaries","text":"<p>Functions like <code>summary_dataframe</code>, <code>summary_column</code>, and <code>validate_and_clean_data</code> generate objects that render as HTML when displayed in a notebook.</p>"},{"location":"guides/notebooks-and-visual-previews/#example","title":"Example","text":"<pre><code>from batwing_ml import summary_dataframe\nsummary = summary_dataframe(df_clean)\nsummary  # automatically displays styled HTML\n</code></pre> <p>These previews are:</p> <ul> <li>Scrollable</li> <li>Syntax-highlighted</li> <li>Theme-aware (light/dark)</li> <li>Designed to fit in notebook cells without overwhelming the screen</li> </ul>"},{"location":"guides/notebooks-and-visual-previews/#tip","title":"Tip","text":"<p>If you're not seeing HTML rendering, ensure you're not wrapping the output in <code>print()</code> \u2014 simply place the variable on the last line of a cell.</p>"},{"location":"guides/notebooks-and-visual-previews/#2-columnlevel-summaries","title":"2. Column\u2011level summaries","text":"<p>This is useful when exploring tricky features.</p> <pre><code>from batwing_ml import summary_column\nsummary_column(df_clean[\"age\"])\n</code></pre> <p>You\u2019ll typically see:</p> <ul> <li>Basic stats</li> <li>Distribution preview</li> <li>Missing value summary</li> <li>Type inference details</li> </ul> <p>These show up as compact, readable HTML blocks.</p>"},{"location":"guides/notebooks-and-visual-previews/#3-previewing-preprocessing-steps","title":"3. Previewing preprocessing steps","text":"<p>When you run <code>preprocess_dataframe</code>, the optional metadata (<code>prep_steps</code>) can also be displayed in notebooks.</p> <pre><code>X_processed, prep_steps = preprocess_dataframe(\n    df_clean.drop(columns=[\"label\"]),\n    encode=\"onehot\",\n    scale=\"standard\",\n    return_steps=True,\n)\n\nprep_steps  # view metadata in notebook\n</code></pre> <p>This helps you verify:</p> <ul> <li>Which columns were encoded</li> <li>Which were scaled</li> <li>Which were dropped (and why)</li> <li>How missing values were handled</li> </ul> <p>Great for debugging and reproducibility.</p>"},{"location":"guides/notebooks-and-visual-previews/#4-feature-engineering-previews","title":"4. Feature engineering previews","text":"<p>Feature engineering often modifies many columns. The metadata object returned (<code>fe_meta</code>) displays nicely.</p> <pre><code>X_fe, fe_meta = feature_engineering(X_processed)\nfe_meta\n</code></pre> <p>You\u2019ll typically see:</p> <ul> <li>Features added or removed</li> <li>Methods applied (variance, MI, model-based)</li> <li>Dimensionality-reduction notes</li> </ul> <p>This is extremely useful when experimenting.</p>"},{"location":"guides/notebooks-and-visual-previews/#5-plots-in-notebooks","title":"5. Plots in notebooks","text":"<p>Evaluation utilities produce plots automatically:</p> <ul> <li>ROC / PR curves (classification)</li> <li>Confusion matrices</li> <li>Residual plots (regression)</li> <li>Error distribution plots</li> <li>Predicted vs Actual scatter plots</li> </ul> <p>Just call the evaluation function \u2014 plots appear inline.</p>"},{"location":"guides/notebooks-and-visual-previews/#example_1","title":"Example","text":"<pre><code>from batwing_ml import evaluate_classification_model\nmetrics = evaluate_classification_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n)\n</code></pre> <p>If you're in Colab, you don\u2019t need special setup \u2014 matplotlib plots render automatically.</p>"},{"location":"guides/notebooks-and-visual-previews/#6-displaying-multiple-plots-cleanly","title":"6. Displaying multiple plots cleanly","text":"<p>Use additional notebook tools to manage layout:</p>"},{"location":"guides/notebooks-and-visual-previews/#jupyterlab","title":"JupyterLab","text":"<pre><code>%matplotlib inline\n</code></pre>"},{"location":"guides/notebooks-and-visual-previews/#sidebyside-plots","title":"Side\u2011by\u2011side plots","text":"<pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 5))\n</code></pre>"},{"location":"guides/notebooks-and-visual-previews/#collapsible-sections","title":"Collapsible sections","text":"<p>In JupyterLab/Notebook:</p> <pre><code>from IPython.display import HTML\nHTML('&lt;details&gt;&lt;summary&gt;Show plots&lt;/summary&gt; ... &lt;/details&gt;')\n</code></pre> <p>Useful for long evaluation runs.</p>"},{"location":"guides/notebooks-and-visual-previews/#7-using-batwing-ml-with-pandasprofiling-ydataprofiling","title":"7. Using Batwing-ML with pandas\u2011profiling / ydata\u2011profiling","text":"<p>Batwing-ML plays nicely with other notebook EDA tools.</p> <pre><code>from ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df_clean, minimal=True)\nprofile.to_widgets()\n</code></pre> <p>This complements the lightweight Batwing-ML summaries.</p>"},{"location":"guides/notebooks-and-visual-previews/#8-exporting-visuals","title":"8. Exporting visuals","text":"<p>Many evaluation functions return a dictionary that includes figures.</p> <p>You can save them manually:</p> <pre><code>fig = metrics[\"residual_plot\"]\nfig.savefig(\"residuals.png\", dpi=300)\n</code></pre> <p>Or save entire diagnostic sets per experiment.</p>"},{"location":"guides/notebooks-and-visual-previews/#9-debugging-inside-notebooks","title":"9. Debugging inside notebooks","text":"<p>Interactive environments make debugging easier.</p>"},{"location":"guides/notebooks-and-visual-previews/#check-shapes","title":"Check shapes","text":"<pre><code>X.shape, X_processed.shape, X_fe.shape\n</code></pre>"},{"location":"guides/notebooks-and-visual-previews/#inspect-encoding","title":"Inspect encoding","text":"<pre><code>prep_steps[\"encoded_columns\"]\n</code></pre>"},{"location":"guides/notebooks-and-visual-previews/#check-for-leakage","title":"Check for leakage","text":"<pre><code>corr = df_clean.corr(numeric_only=True)[target_col].sort_values()\ncorr.tail()\n</code></pre>"},{"location":"guides/notebooks-and-visual-previews/#preview-heavy-operations","title":"Preview heavy operations","text":"<p>If a step feels slow, test it on a smaller slice:</p> <pre><code>df_clean.head(1000)\n</code></pre> <p>This can give quick feedback before running the full pipeline.</p>"},{"location":"guides/notebooks-and-visual-previews/#10-tips-for-a-smooth-notebook-experience","title":"10. Tips for a smooth notebook experience","text":""},{"location":"guides/notebooks-and-visual-previews/#keep-metadata-objects-visible","title":"\u2714\ufe0f Keep metadata objects visible","text":"<p>They tell you exactly what preprocessing/engineering happened.</p>"},{"location":"guides/notebooks-and-visual-previews/#use-fast_mode-early","title":"\u2714\ufe0f Use <code>fast_mode</code> early","text":"<p>Notebook runs should be fast.</p>"},{"location":"guides/notebooks-and-visual-previews/#use-rich-display-not-print","title":"\u2714\ufe0f Use rich display, not print()","text":"<p>Let HTML objects render naturally.</p>"},{"location":"guides/notebooks-and-visual-previews/#use-colab-gputpu-only-if-needed","title":"\u2714\ufe0f Use Colab GPU/TPU only if needed","text":"<p>Most Batwing-ML tasks are CPU\u2011friendly.</p>"},{"location":"guides/notebooks-and-visual-previews/#re-run-cells-toptobottom-periodically","title":"\u2714\ufe0f Re-run cells top\u2011to\u2011bottom periodically","text":"<p>Ensures your pipeline is clean and reproducible.</p>"},{"location":"guides/notebooks-and-visual-previews/#summary","title":"Summary","text":"<p>Interactive notebooks are the best environment for experimenting with Batwing-ML. Use the built\u2011in HTML previews, plot-generating evaluators, and metadata objects to inspect, debug, and understand every step of your pipeline.</p> <p>Once the workflow is stable, you can move the code into scripts or a production environment knowing exactly how each step behaves.</p>"},{"location":"guides/regression-workflow/","title":"Regression Workflow","text":"<p>This workflow mirrors the same clean structure as the classification guide, but tailored for continuous targets. If you're predicting prices, demand, scores, revenue, or anything numeric, this is your go-to recipe.</p> <p>The flow stays consistent:</p> <p>validate \u2192 explore \u2192 preprocess \u2192 engineer \u2192 split \u2192 tune \u2192 evaluate \u2192 (optional) nested CV</p>"},{"location":"guides/regression-workflow/#1-load-and-validate-your-data","title":"1. Load and validate your data","text":"<pre><code>import pandas as pd\nfrom batwing_ml import validate_and_clean_data\n\ndf = pd.read_csv(\"data.csv\")\ndf_clean, audit = validate_and_clean_data(df)\n</code></pre> <p>Use <code>audit</code> to see type fixes, dropped columns, missing-handling decisions, and schema alignment.</p> <p>Common early checks for regression:</p> <ul> <li>Are extreme values real or data errors?</li> <li>Is the target column bounded or unbounded?</li> <li>Does the distribution look log-normal, skewed, or roughly Gaussian?</li> </ul>"},{"location":"guides/regression-workflow/#2-explore-your-dataset","title":"2. Explore your dataset","text":"<pre><code>from batwing_ml import summary_dataframe, summary_column\n\nsummary = summary_dataframe(df_clean)\nsummary\n</code></pre> <p>Explore the target too:</p> <pre><code>target_col = \"price\"\n\ntarget_summary = summary_column(df_clean[target_col])\ntarget_summary\n</code></pre> <p>Things to examine:</p> <ul> <li>Skew/outliers (important for MSE-based metrics)</li> <li>Missingness patterns</li> <li>Categorical vs numeric balance</li> <li>Potential leakage columns</li> </ul>"},{"location":"guides/regression-workflow/#3-split-features-and-target","title":"3. Split features and target","text":"<pre><code>target_col = \"price\"  # change for your dataset\n\nX_raw = df_clean.drop(columns=[target_col])\ny = df_clean[target_col]\n</code></pre> <p>Keep your target as a separate vector early to avoid accidental leakage in transformation steps.</p>"},{"location":"guides/regression-workflow/#4-preprocess-impute-encode-scale","title":"4. Preprocess: impute, encode, scale","text":"<p>All standard preprocessing steps for regression stay the same.</p> <pre><code>from batwing_ml import preprocess_dataframe\n\nX_processed, prep_steps = preprocess_dataframe(\n    X_raw,\n    encode=\"onehot\",            # often best for regression\n    scale=\"standard\",           # ensures fair weighting for linear models\n    drop_missing_thresh=0.3,\n    return_steps=True,\n)\n</code></pre> <p>What <code>prep_steps</code> usually captures:</p> <ul> <li>Columns removed due to too many missing values</li> <li>Categorical columns encoded, numeric columns scaled</li> <li>Imputation strategy used</li> </ul>"},{"location":"guides/regression-workflow/#5-feature-engineering","title":"5. Feature engineering","text":"<p>Feature engineering often matters more in regression than classification.</p> <pre><code>from batwing_ml import feature_engineering\n\nX_fe, fe_meta = feature_engineering(\n    X_processed,\n    target=None,\n    mode=\"both\",   # transformations + feature selection\n)\n</code></pre> <p>This may:</p> <ul> <li>Remove irrelevant/low-variance columns</li> <li>Add transformed versions of existing features</li> <li>Use mutual information or model-based methods to keep strong predictors</li> <li>Optionally include PCA or dimensionality reduction if configured</li> </ul> <p>After this step:</p> <pre><code>X = X_fe\n</code></pre>"},{"location":"guides/regression-workflow/#6-traintest-split","title":"6. Train/test split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n)\n</code></pre> <p>For regression, no stratify needed.</p>"},{"location":"guides/regression-workflow/#7-hyperparameter-tuning-for-one-model-optuna","title":"7. Hyperparameter tuning for one model (Optuna)","text":"<p>Start with a strong baseline like RandomForestRegressor, XGBoost, or GradientBoostingRegressor.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom batwing_ml import hyperparameter_tuning_regression\n\nparam_grid = {\n    \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),\n    \"max_depth\":    lambda t: t.suggest_int(\"max_depth\", 3, 20),\n    \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),\n}\n\nresults = hyperparameter_tuning_regression(\n    X=X_train,\n    y=y_train,\n    model_class=RandomForestRegressor,\n    param_grid=param_grid,\n    scoring=\"neg_rmse\",        # or \"neg_mae\", \"r2\" depending on your objective\n    fast_mode=False,\n)\n\nbest_model = results[\"best_model\"]\nprint(results[\"best_params\"])\nprint(results[\"best_score\"])\n</code></pre> <p>Tip: If your target is heavily skewed, consider log-transforming <code>y</code> before training.</p>"},{"location":"guides/regression-workflow/#8-evaluate-the-tuned-regressor","title":"8. Evaluate the tuned regressor","text":"<p>Use Batwing-ML\u2019s rich evaluation utilities.</p> <pre><code>from batwing_ml import evaluate_regression_model\n\nmetrics = evaluate_regression_model(\n    model=best_model,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    return_dict=True,\n)\n\nprint(metrics)\n</code></pre> <p>Typical outputs include:</p> <ul> <li>R\u00b2</li> <li>Adjusted R\u00b2</li> <li>RMSE</li> <li>MAE</li> <li>MAPE</li> <li>RMSLE</li> <li>Residual plots</li> <li>Error distribution</li> <li>Prediction vs Actual scatter</li> </ul> <p>Look for:</p> <ul> <li>Systematic under/over-prediction</li> <li>Heavy tails in residuals</li> <li>Patterns indicating missing features</li> </ul>"},{"location":"guides/regression-workflow/#9-compare-models-with-nested-cv-optional-but-recommended","title":"9. Compare models with nested CV (optional but recommended)","text":"<p>Once you want to compare model families honestly, use nested cross\u2011validation.</p> <pre><code>from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom batwing_ml import run_nested_cv_regression\n\nmodels = {\n    \"ridge\": Ridge(),\n    \"lasso\": Lasso(),\n    \"rf\": RandomForestRegressor(),\n}\n\nparam_grids = {\n    \"ridge\": {\"alpha\": [0.1, 1, 10]},\n    \"lasso\": {\"alpha\": [0.001, 0.01, 0.1]},\n    \"rf\": {\"n_estimators\": [100, 300]},\n}\n\nnested = run_nested_cv_regression(\n    X=X,\n    y=y,\n    model_dict=models,\n    param_grids=param_grids,\n    scoring_list=[\"neg_rmse\", \"r2\", \"neg_mae\"],\n    search_method=\"grid\",\n    return_results=True,\n)\n\nnested_summary = nested[\"summary\"]\nnested_summary\n</code></pre> <p>Nested CV is extremely helpful when:</p> <ul> <li>Two models look similar on a train/test split</li> <li>You want an unbiased metric for reporting</li> <li>You are selecting the final model family for deployment</li> </ul>"},{"location":"guides/regression-workflow/#10-recommended-regression-patterns","title":"10. Recommended regression patterns","text":"<p>A few practical considerations:</p>"},{"location":"guides/regression-workflow/#always-inspect-your-target-distribution","title":"\u2714\ufe0f Always inspect your target distribution","text":"<p>Transformations matter more here than in classification.</p>"},{"location":"guides/regression-workflow/#scale-your-features","title":"\u2714\ufe0f Scale your features","text":"<p>Especially important for linear models and distance-based methods.</p>"},{"location":"guides/regression-workflow/#beware-of-leakage-in-engineered-features","title":"\u2714\ufe0f Beware of leakage in engineered features","text":"<p>Don\u2019t accidentally encode the target into derived variables.</p>"},{"location":"guides/regression-workflow/#tune-maebased-and-rmsebased-metrics-separately","title":"\u2714\ufe0f Tune MAE\u2011based and RMSE\u2011based metrics separately","text":"<p>MAE is robust to outliers, RMSE is sensitive \u2014 both reveal different truths.</p>"},{"location":"guides/regression-workflow/#use-nested-cv-for-model-family-comparison","title":"\u2714\ufe0f Use nested CV for model family comparison","text":"<p>Especially when deciding between linear, tree\u2011based, and ensemble methods.</p> <p>Once this workflow feels natural, you can move on to multiclass classification or incorporate the same structure into production\u2011grade pipelines.</p>"},{"location":"modeling/binary-classification/","title":"Binary Classification","text":"<p>End-to-end support for binary classification problems: tuning, nested cross-validation, and evaluation.</p> <p>These functions assume you already have a preprocessed / engineered feature matrix <code>X</code> and target <code>y</code>.</p>"},{"location":"modeling/binary-classification/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<pre><code>from batwing_ml import hyperparameter_tuning_classification\n</code></pre>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification","title":"<code>hyperparameter_tuning_classification(X=None, y=None, model_class=None, param_grid=None, scoring='accuracy', n_trials=50, cv_folds=5, stratified=True, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83d\udd27 Hyperparameter Tuning for Classification (via Optuna)</p> <p>Optimize hyperparameters of any binary classification model using Optuna\u2019s efficient  sampling. This function supports cross-validation, custom scoring, stratified sampling,  reproducibility, and returns the best trial summary and optionally the best fitted model.</p> <p>Ideal for automated model selection pipelines, leaderboard tuning, or experimentation  in research and production ML workflows.</p>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix with shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector with binary labels (0 or 1).</p> callable <p>A scikit-learn-style classifier class (e.g., <code>RandomForestClassifier</code>, <code>LogisticRegression</code>). Not an instance \u2013 must be the class itself.</p> dict <p>Dictionary mapping hyperparameter names to Optuna sampling functions. Example:     {         \"C\": lambda t: t.suggest_float(\"C\", 0.01, 10, log=True),         \"penalty\": lambda t: t.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])     }</p> str or callable, default='accuracy' <p>Metric to optimize during cross-validation. Supports: - Any string from sklearn (e.g., 'f1', 'roc_auc', 'log_loss') - A custom callable with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of optimization trials to run.</p> int, default=5 <p>Number of folds in cross-validation to evaluate each hyperparameter setting.</p> bool, default=True <p>If True, use StratifiedKFold for cross-validation (preserves class balance).</p> {'maximize', 'minimize'}, default='maximize' <p>Whether to maximize or minimize the scoring function.</p> bool, default=True <p>If True, prints trial progress, parameter table, and best result summary.</p> bool, default=True <p>If True, fits and returns the best model using the entire dataset.</p> int, default=42 <p>Random seed for reproducible folds and results.</p> float or None, optional <p>If provided, samples a random fraction (e.g., 0.1 = 10%) of the dataset.</p> int or None, optional <p>If provided, uses only the first N rows of the data.</p> bool, default=False <p>If True, reduces <code>n_trials</code> to 10, disables logs, and optimizes speed. Use for quick tests or large-scale experiments.</p>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--returns","title":"Returns","text":"<p>dict     A dictionary containing:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.Study     - 'best_model' : fitted model (if return_model=True)</p>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--raises","title":"Raises","text":"<p>ValueError     If required arguments are missing or incompatible.</p>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use: - You're comparing models or trying to find optimal settings for one. - You want to replace GridSearchCV with faster, smarter search. - You want control over cross-validation, scoring, and sampling. - You\u2019re tuning on large datasets and want quick feedback via fast_mode or subsampling.</p> <p>\ud83d\udccc Core Concepts:</p> <p>\u2022 Optuna Trials:     Each trial samples a different set of parameters using Optuna\u2019s intelligent search strategy     (Tree Parzen Estimator) and evaluates them using cross-validation.</p> <p>\u2022 param_grid:     Define hyperparameter ranges via lambdas \u2014 much more flexible than grid search.     Supports <code>suggest_float</code>, <code>suggest_int</code>, <code>suggest_categorical</code>, and log-scale sampling.</p> <p>\u2022 scoring:     Choose a string (e.g., 'f1', 'accuracy', 'roc_auc') or define a custom scoring function     that returns a float. Examples:         - 'neg_log_loss': minimizes log loss (automatically handled)         - custom_cost(estimator, X, y): returns cost/loss based on predictions</p> <p>\u2022 StratifiedKFold:     Recommended for binary classification, especially if classes are imbalanced.</p> <p>\u2022 fast_mode:     Use fast_mode=True to reduce trials to 10, turn off prints, and return results quickly.     Ideal for initial tests or iterative tuning on large data.</p> <p>\u2022 Sampling Subsets:     - <code>use_fraction=0.1</code>: randomly sample 10% of the data     - <code>use_n_samples=5000</code>: use only the first 5000 rows     - You can pass both; <code>use_n_samples</code> is applied after <code>use_fraction</code></p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import RandomForestClassifier results = hyperparameter_tuning_classification(         X=X, y=y,         model_class=RandomForestClassifier,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 15),             \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),         },         scoring='f1',         n_trials=50,         stratified=True,         fast_mode=False,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) best_model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use fewer trials with small data or fast models (e.g., LogisticRegression). - Tune for <code>log_loss</code> when probabilistic accuracy matters (e.g., fraud detection). - Always use <code>return_model=True</code> in production workflows to get the fitted model. - Set <code>fast_mode=True</code> when running in a loop or with large datasets. - Pair with <code>evaluate_classification_model()</code> for post-tuning performance analysis.</p>"},{"location":"modeling/binary-classification/#batwing_ml.hyperparameter_tuning_classification.hyperparameter_tuning_classification--see-also","title":"See Also","text":"<p>evaluate_classification_model : Full evaluation suite for binary classifiers. GridSearchCV : Traditional brute-force alternative (slower, less efficient). Optuna : https://optuna.org</p>"},{"location":"modeling/binary-classification/#nested-cross-validation","title":"Nested cross-validation","text":"<pre><code>from batwing_ml import run_nested_cv_classification\n</code></pre> <p>Run nested cross-validation for multiple classification models with optional tuning strategies and scaling.</p> <p>This function evaluates one or more classification models using nested cross-validation \u2014  an approach that combines robust performance estimation with internal hyperparameter tuning.  It supports both GridSearchCV and RandomizedSearchCV, allows sub-sampling for large datasets,  and offers a simplified or silent fast mode for production or large-scale runs.</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix (n_samples, n_features). y : pd.Series or np.ndarray     Binary classification target variable (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn classifiers.</p> dict of str \u2192 dict <p>Dictionary mapping model names to hyperparameter grids in sklearn format.</p> list of str or callables, default=['accuracy'] <p>List of scoring metrics to evaluate during nested CV (e.g., ['accuracy', 'roc_auc']).</p> int, default=3 <p>Number of folds in the outer CV loop (used for final performance estimation).</p> int, default=3 <p>Number of folds in the inner CV loop (used for hyperparameter tuning).</p> {'grid', 'random'}, default='grid' <p>Search strategy to use for tuning: - 'grid': exhaustive parameter search (GridSearchCV) - 'random': random search with <code>n_iter</code> (RandomizedSearchCV)</p> int, optional <p>Only used when <code>search_method='random'</code>. Number of parameter settings sampled.</p> int, optional <p>If provided, limits the number of rows used for fitting to this count (e.g., 100_000).</p> float, optional <p>If provided, randomly samples this fraction of rows (e.g., 0.1 for 10%).</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function to determine if a given model should use <code>StandardScaler</code>.</p> sklearn-compatible transformer, optional <p>If provided, replaces built-in scaling logic with a user-defined transformer or pipeline.</p> bool, default=True <p>If True, prints progress, hyperparameter tables, and detailed summaries.</p> bool, default=False <p>If True, disables visual output and reduces outer CV folds to speed up runtime.</p> bool, default=False <p>If True, returns a dictionary with summaries, best parameters, and matplotlib figures.</p> {'tabulate', 'rich'}, default='tabulate' <p>Controls how summary tables are printed (colorful or ASCII).</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--returns","title":"Returns","text":"<p>dict, optional     Only if <code>return_results=True</code>. Includes:     - 'summary': pd.DataFrame of all model-metric combinations     - 'results': detailed CV results and best parameters     - 'best_params': dict of best params for each model and metric     - 'figures': optional comparison plots (if fast_mode=False)</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--raises","title":"Raises","text":"<p>ValueError     If any of the required inputs (<code>X</code>, <code>y</code>, <code>model_dict</code>, <code>param_grids</code>) are missing.</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--examples","title":"Examples","text":"<p>from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier</p> <p>models = {         'logistic': LogisticRegression(),         'rf': RandomForestClassifier()     }</p> <p>grids = {         'logistic': {'C': [0.1, 1, 10]},         'rf': {'n_estimators': [50, 100]}     }</p> <p>results = run_nested_cv_classification(         X=X, y=y,         model_dict=models,         param_grids=grids,         scoring_list=['accuracy', 'f1'],         fast_mode=True,         search_method='random',         n_iter=5,         max_samples=10000,         return_results=True     )</p> <p>print(results['summary'])</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--user-guide","title":"User Guide","text":"<p>\ud83e\uddea What is Nested CV?     - Nested cross-validation helps prevent overfitting in model selection by using an inner loop        for hyperparameter tuning and an outer loop for estimating generalization performance.</p> <p>\ud83d\udccc Common Use Cases:     - Benchmarking many classifiers fairly.     - Avoiding bias from tuning and testing on the same split.     - Handling imbalanced or high-cardinality feature spaces.</p> <p>\u2699\ufe0f When to Use Which Search Strategy:     - Use <code>'grid'</code> for small hyperparameter spaces where you want exact control.     - Use <code>'random'</code> when the space is large or you're optimizing speed vs. accuracy.</p> <p>\u26a1 When to Use <code>fast_mode=True</code>:     - During large-scale experiments (e.g., 10M+ rows).     - If visual plots or verbose logging are unnecessary.     - In production or headless environments.</p> <p>\ud83d\udd0d Interpreting the Output:     - Mean/Std Dev: Represents model stability across folds.     - Best Params: Optimal hyperparameters found by inner CV.     - Figures: Comparison plots for accuracy, F1, AUC, etc.</p> <p>\ud83d\udcc9 Scaling Rules:     - By default, tree-based models skip scaling.     - You can override this using <code>use_scaling()</code> or provide a custom <code>preprocessor</code>.</p> <p>\ud83d\udeab Memory Tips for Big Data:     - Use <code>max_samples</code> or <code>sample_frac</code> to downsample safely.     - Enable <code>fast_mode</code> to avoid expensive operations like seaborn/Matplotlib rendering.</p>"},{"location":"modeling/binary-classification/#batwing_ml.run_nested_cv_classification--see-also","title":"See Also","text":"<ul> <li>GridSearchCV, RandomizedSearchCV (from sklearn)</li> <li>evaluate_classification_model() for post-training model diagnostics</li> </ul>"},{"location":"modeling/binary-classification/#evaluation","title":"Evaluation","text":"<pre><code>from batwing_ml import evaluate_classification_model\n</code></pre>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model","title":"<code>evaluate_classification_model(model=None, X_train=None, y_train=None, X_test=None, y_test=None, cv=5, cost_fn=None, cost_fp=None, validation_params=None, scoring_curve='accuracy', verbose=True, return_dict=False, return_model_only=False, export_model=False, extra_plots=None, sample_fraction=None, sample_size=None, fast_mode=False)</code>","text":"<p>Evaluate a binary classification model with diagnostics, metrics, plots, and model handling.</p> <p>This function evaluates a scikit-learn-compatible classifier using standard and advanced metrics. It supports both pretrained and unfitted models, can train during evaluation, and provides plots for calibration, threshold tuning, learning/validation curves, and more. It's useful for audits, experimentation, and production diagnostics.</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn classifier. If unfitted, the function will train it using provided data.</p> array-like, optional <p>Training feature matrix. Required if model is not already fitted.</p> array-like, optional <p>Training labels.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing labels.</p> int, default=5 <p>Number of folds for cross-validation in learning/validation curves.</p> float, optional <p>Cost for false negatives in misclassification cost calculation.</p> float, optional <p>Cost for false positives.</p> dict, optional <p>Dictionary of hyperparameter names to lists of values for validation curve plotting.</p> str, default='accuracy' <p>Scoring metric used in learning and validation curves.</p> bool, default=True <p>If True, prints metrics and plots.</p> bool, default=False <p>If True, returns metrics as a dictionary.</p> bool, default=False <p>If True, returns the trained model only (no metrics).</p> bool, default=False <p>If True, returns (metrics_dict, trained_model) tuple.</p> list of str, optional <p>Options include:     - \"threshold\": Precision/Recall/F1 vs threshold     - \"calibration\": Reliability of predicted probabilities     - \"ks\": KS separation statistic     - \"lift\": Lift curve     - \"det\": Detection Error Tradeoff curve</p> float, optional <p>Fraction of data to use for train/test sets (e.g., 0.1 = 10%).</p> int, optional <p>Number of rows to use from training and testing sets.</p> bool, default=False <p>If True, skips plots and printouts for faster processing.</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Fitted model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>export_model=True</code></p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--raises","title":"Raises","text":"<p>ValueError     If necessary inputs are missing or misaligned.</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--examples","title":"Examples","text":"<p>evaluate_classification_model(         model=clf,         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         cost_fn=10, cost_fp=1,         validation_params={'max_depth': [2, 4, 6]},         scoring_curve='f1',         extra_plots=['threshold', 'ks'],         return_dict=True     )</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--save-a-newly-trained-model","title":"Save a newly trained model","text":"<p>model = evaluate_classification_model(         model=RandomForestClassifier(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         return_model_only=True     )</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--evaluate-a-pretrained-model","title":"Evaluate a pretrained model","text":"<p>from joblib import load model = load(\"model.joblib\") evaluate_classification_model(model=model, X_test=Xt, y_test=yt)</p>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You have a binary classifier (trained or not) and want a full evaluation report.     \u2022 You want to generate visual diagnostics to understand performance beyond raw metrics.     \u2022 You're comparing different models and need standardized summaries and visual feedback.     \u2022 You want to include cost-sensitive evaluation logic (e.g., FP/FN tradeoffs in fraud, medical diagnosis).     \u2022 You plan to export a trained model right after evaluation to reuse or save for deployment.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 Accuracy:         \u2192 Proportion of correct predictions overall.         \u2192 Works well when classes are balanced, but misleading with imbalanced classes.</p> <pre><code>\u2022 Precision:\n    \u2192 Of all predicted positives, how many were actually correct?\n    \u2192 Use when false positives are costly (e.g., spam detection).\n\n\u2022 Recall:\n    \u2192 Of all actual positives, how many did the model catch?\n    \u2192 Use when false negatives are costly (e.g., cancer diagnosis).\n\n\u2022 F1 Score:\n    \u2192 Harmonic mean of Precision and Recall.\n    \u2192 Best for imbalanced datasets where both FP and FN matter.\n\n\u2022 ROC AUC:\n    \u2192 Measures the model's ability to rank positives over negatives.\n    \u2192 Robust against imbalance. Closer to 1 = better.\n\n\u2022 Cost-sensitive Average Loss:\n    \u2192 Weighted loss calculation based on your domain-specific cost of False Positives and False Negatives.\n    \u2192 Useful in fraud detection, churn prediction, or medical triage where not all errors are equal.\n</code></pre> <p>\ud83d\udcc8 Optional Diagnostic Plots:     These are generated when <code>extra_plots</code> is set and <code>fast_mode=False</code>.</p> <pre><code>\u2022 Threshold Curve (`extra_plots=['threshold']`):\n    \u2192 Shows how Precision, Recall, and F1 change with different probability thresholds.\n    \u2192 Use it when you need to manually tune the decision boundary (e.g., prioritize recall over precision).\n\n\u2022 Calibration Curve (`extra_plots=['calibration']`):\n    \u2192 Compares predicted probabilities to observed outcomes.\n    \u2192 Helps determine if model outputs represent true probabilities (e.g., in credit scoring, risk modeling).\n\n\u2022 KS Statistic (`extra_plots=['ks']`):\n    \u2192 Plots the cumulative distribution of scores for each class and measures the maximum separation.\n    \u2192 A higher KS value (closer to 1) indicates good class separation.\n\n\u2022 Lift Curve (`extra_plots=['lift']`):\n    \u2192 Compares model performance against random guessing in terms of capturing true positives.\n    \u2192 Great for targeting top decile groups (e.g., marketing response modeling).\n\n\u2022 DET Curve (`extra_plots=['det']`):\n    \u2192 Plots False Positive Rate vs. False Negative Rate using logarithmic scale.\n    \u2192 Especially useful in imbalanced classification (e.g., rare disease detection, security event modeling).\n</code></pre> <p>\ud83d\udcda Learning &amp; Validation Curves (CV Required):     \u2022 Learning Curve:         \u2192 Plots model performance vs. training size.         \u2192 Helps detect underfitting (low train/val scores) or overfitting (train \u226b val).         \u2192 Useful to decide if more data will help your model.</p> <pre><code>\u2022 Validation Curve (`validation_params={'C': [...], 'max_depth': [...]}`):\n    \u2192 Shows how a single hyperparameter affects training and validation performance.\n    \u2192 Use to find optimal model complexity (e.g., tree depth, regularization).\n</code></pre> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skip all visual output and verbose logs. Ideal for CI/CD pipelines or large batch runs.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Use this if your model is not yet fitted and you want the trained object back after evaluation.\n\n\u2022 export_model=True:\n    \u2192 Returns both (metrics_dict, trained_model) to chain into pipelines, dashboards, or export routines.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Quickly prototype or test on large datasets without full evaluation time/cost.\n    \u2192 Useful when training/testing on full 10M+ rows is not practical.\n</code></pre>"},{"location":"modeling/binary-classification/#batwing_ml.evaluate_classification_model.evaluate_classification_model--see-also","title":"See Also","text":"<ul> <li>preprocess_dataframe() : For feature preprocessing</li> <li>summary_dataframe() : For data overview</li> <li>run_nested_cv_classification() : For model comparison</li> </ul>"},{"location":"modeling/multiclass-classification/","title":"Multiclass Classification","text":"<p>Support for multiclass classification tasks, including tuning, nested cross-validation, and evaluation.</p> <p>Use these when your target has more than two classes.</p>"},{"location":"modeling/multiclass-classification/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<pre><code>from batwing_ml import hyperparameter_tuning_multiclass_classification\n</code></pre>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification","title":"<code>hyperparameter_tuning_multiclass_classification(X=None, y=None, model_class=None, param_grid=None, scoring='accuracy', n_trials=50, cv_folds=5, stratified=True, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83c\udfaf Hyperparameter Tuning for Multiclass Classification (via Optuna)</p> <p>Optimize hyperparameters of any multiclass classification model using Optuna\u2019s efficient  search algorithm. This function supports cross-validation (with stratification), custom  or built-in scoring metrics, reproducibility, and flexible data sampling. Ideal for  experimentation, model leaderboard tuning, and automated pipelines.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix of shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector with multiclass labels (3+ classes supported).</p> callable <p>A scikit-learn-style classifier class (e.g., <code>RandomForestClassifier</code>, <code>LogisticRegression</code>). Should be the class itself, not an instantiated object.</p> dict <p>Dictionary mapping hyperparameter names to Optuna search functions. Example:     {         \"C\": lambda t: t.suggest_float(\"C\", 0.01, 10, log=True),         \"solver\": lambda t: t.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\"])     }</p> str or callable, default='accuracy' <p>Evaluation metric to optimize. Can be: - Any valid sklearn scorer string (e.g., 'f1_macro', 'balanced_accuracy', 'neg_log_loss') - A custom scoring function with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of Optuna trials to run.</p> int, default=5 <p>Number of folds for cross-validation during evaluation.</p> bool, default=True <p>Whether to use StratifiedKFold (preserves class distribution across folds). Set to False to use standard KFold.</p> {'maximize', 'minimize'}, default='maximize' <p>Whether to maximize or minimize the scoring function.</p> bool, default=True <p>If True, prints Optuna progress, trial summary, and best results.</p> bool, default=True <p>If True, fits and returns the best model using the entire dataset.</p> int, default=42 <p>Seed for reproducibility (used in CV splitting and Optuna sampler).</p> float or None, optional <p>Randomly sample a fraction of the dataset (e.g., 0.1 = 10%).</p> int or None, optional <p>Limit dataset to first N rows (after fraction sampling, if both are set).</p> bool, default=False <p>If True:     - Limits trials to 10     - Disables verbose logs     - Useful for quick experimentation</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--returns","title":"Returns","text":"<p>dict     A dictionary containing:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.study.Study     - 'best_model' : trained model (if return_model=True)</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If required parameters are missing or scoring is misconfigured.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--user-guide","title":"User Guide","text":"<p>\ud83c\udfaf When to Use: - You're tuning multiclass classification models (3+ classes). - You want intelligent hyperparameter search using Optuna instead of GridSearchCV. - You require macro/micro scoring for imbalanced datasets. - You need flexible sampling and control over reproducibility.</p> <p>\ud83d\udccc Core Concepts:</p> <p>\u2022 Multiclass Scoring:     Use <code>'f1_macro'</code>, <code>'balanced_accuracy'</code>, or other sklearn scoring strings.     You can also define a custom scorer that returns a float (e.g., macro recall).</p> <p>\u2022 Optuna Trials:     Each trial samples hyperparameters and evaluates them via cross-validation.     Optuna uses Tree Parzen Estimator (TPE) by default for efficient search.</p> <p>\u2022 param_grid:     A dictionary of Optuna sampling functions. Example:         {             'max_depth': lambda t: t.suggest_int('max_depth', 3, 10),             'min_samples_split': lambda t: t.suggest_int('min_samples_split', 2, 20)         }</p> <p>\u2022 StratifiedKFold:     Recommended when class distribution is skewed. Ensures representative folds.</p> <p>\u2022 fast_mode:     Enables a faster tuning mode by reducing trials and silencing logs. Great for early tests.</p> <p>\u2022 Sampling Subsets:     - <code>use_fraction=0.1</code>: randomly samples 10% of the data     - <code>use_n_samples=5000</code>: takes only first 5000 rows (after fraction)     - Combine both for rapid prototyping</p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import RandomForestClassifier results = hyperparameter_tuning_multiclass_classification(         X=X, y=y,         model_class=RandomForestClassifier,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 15),             \"min_samples_split\": lambda t: t.suggest_int(\"min_samples_split\", 2, 10),         },         scoring='f1_macro',         fast_mode=False,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) best_model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use <code>f1_macro</code> or <code>balanced_accuracy</code> when class distribution is uneven. - Set <code>fast_mode=True</code> when running many experiments in parallel. - Combine with <code>evaluate_multiclass_classification()</code> to analyze final performance. - Avoid using accuracy alone for imbalanced multiclass tasks \u2014 prefer macro metrics.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.hyperparameter_tuning_multiclass_classification.hyperparameter_tuning_multiclass_classification--see-also","title":"See Also","text":"<p>evaluate_multiclass_classification : Full evaluation utility for multiclass models. GridSearchCV : Slower, less flexible alternative. Optuna Docs : https://optuna.org</p>"},{"location":"modeling/multiclass-classification/#nested-cross-validation","title":"Nested cross-validation","text":"<pre><code>from batwing_ml import run_nested_cv_multiclass_classification\n</code></pre> <p>Run nested cross-validation for multiclass classification models with internal tuning, scaling, and visualization.</p> <p>This function supports nested cross-validation for evaluating multiple classifiers on multiclass targets.  It performs hyperparameter tuning using either GridSearchCV or RandomizedSearchCV within inner folds and evaluates  model performance on outer folds. Additional features include smart sub-sampling, optional standardization, and  visual summary generation using matplotlib/seaborn.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix (n_samples, n_features).</p> pd.Series or np.ndarray <p>Multiclass classification target variable (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn-compatible classifiers.</p> dict of str \u2192 dict <p>Dictionary mapping model names to their hyperparameter grids.</p> list of str or callable, default=['accuracy'] <p>List of scoring metrics for evaluation (e.g., ['accuracy', 'f1_macro', 'recall_macro']).</p> int, default=3 <p>Number of outer CV folds (used to estimate generalization performance).</p> int, default=3 <p>Number of inner CV folds (used for hyperparameter tuning).</p> int, default=42 <p>Random seed for reproducibility across resampling and CV.</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function that returns True/False depending on whether a model should use <code>StandardScaler</code>.</p> sklearn-compatible transformer, optional <p>Custom preprocessing pipeline. If provided, overrides automatic scaling logic.</p> {'grid', 'random'}, default='grid' <p>Search strategy to use:     - 'grid': exhaustive search using GridSearchCV     - 'random': random search using RandomizedSearchCV</p> int, default=10 <p>Number of parameter combinations to try (only used for random search).</p> float, optional <p>If provided, samples this fraction (e.g., 0.1 = 10%) of total data.</p> int, optional <p>If provided, caps total sample count to this number.</p> bool, default=False <p>If True, reduces CV splits to 2 and skips plots/logs for speed.</p> bool, default=True <p>Whether to print intermediate logs and tuning results.</p> bool, default=False <p>If True, returns dictionary of performance summaries, best parameters, and figures.</p> {'tabulate', 'rich'}, default='tabulate' <p>Controls formatting of logs \u2014 rich formatting vs plain ASCII tables.</p> bool, default=True <p>If False, disables visualization of model comparisons.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--returns","title":"Returns","text":"<p>dict, optional     If <code>return_results=True</code>, returns:     - 'summary': DataFrame with final evaluation of all models/metrics     - 'results': Nested dictionary of raw scores and best params per model/metric     - 'best_params': Extracted best hyperparameters per model and metric     - 'figures': Optional matplotlib plots (if show_plots is True)</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If any required argument (<code>X</code>, <code>y</code>, <code>model_dict</code>, <code>param_grids</code>) is missing.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--examples","title":"Examples","text":"<p>from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier</p> <p>models = { ...     'logistic': LogisticRegression(max_iter=200), ...     'rf': RandomForestClassifier() ... }</p> <p>grids = { ...     'logistic': {'C': [0.1, 1, 10]}, ...     'rf': {'n_estimators': [50, 100]} ... }</p> <p>results = run_nested_cv_multiclass_classification( ...     X=X, y=y, ...     model_dict=models, ...     param_grids=grids, ...     scoring_list=['accuracy', 'f1_macro'], ...     search_method='grid', ...     show_plots=True, ...     return_results=True ... )</p> <p>print(results['summary'])</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--notes","title":"Notes","text":"<p>\ud83d\udd01 What is Nested Cross-Validation?     Nested CV is used to provide an unbiased estimate of model performance while tuning hyperparameters.     The outer loop evaluates model generalization, while the inner loop tunes parameters.</p> <p>\ud83d\udcca Supported Metrics:     - accuracy, f1_macro, f1_weighted, precision_macro, recall_macro, etc.     - Custom scoring functions are also supported.</p> <p>\u2699\ufe0f Recommended Settings:     - Use grid search for small, well-bounded hyperparameter grids.     - Use random search with <code>n_iter</code> for wide or expensive search spaces.     - Set <code>fast_mode=True</code> for quick experiments or when plotting/logging isn\u2019t needed.</p> <p>\ud83d\udcc9 Memory-Saving Tips:     - Use <code>sample_frac</code> or <code>max_samples</code> for faster computation.     - Avoid plotting in low-memory environments by disabling <code>show_plots</code>.</p> <p>\ud83d\udce6 Outputs:     - Compact table of results across models/metrics     - Visual comparisons of bounded (0\u20131) and unbounded metrics     - Structured dictionary for pipeline integration or downstream analysis</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.run_nested_cv_multiclass_classification--see-also","title":"See Also","text":"<ul> <li>GridSearchCV, RandomizedSearchCV (sklearn)</li> <li>evaluate_multiclass_classification_model: For detailed model diagnostics post-training</li> </ul>"},{"location":"modeling/multiclass-classification/#evaluation","title":"Evaluation","text":"<pre><code>from batwing_ml import evaluate_multiclass_classification\n</code></pre>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification","title":"<code>evaluate_multiclass_classification(model=None, X_train=None, y_train=None, X_test=None, y_test=None, cv=5, validation_params=None, scoring_curve='accuracy', verbose=True, return_dict=False, return_model_only=False, export_model=False, extra_plots=None, sample_fraction=None, sample_size=None, fast_mode=False)</code>","text":"<p>Evaluate a multiclass classification model with diagnostics, metrics, plots, and model handling.</p> <p>This function evaluates a scikit-learn-compatible classifier across multiple classes using both standard and advanced metrics. It supports unfitted or pretrained models, can train during evaluation, and provides visual diagnostics such as learning curves, calibration plots, and confusion matrices. It is ideal for model auditing, experiment evaluation, and production diagnostics in multiclass settings.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn-compatible classifier. If not fitted, it will be trained using X_train and y_train.</p> array-like, optional <p>Training feature matrix. Required if the model is not already fitted.</p> array-like, optional <p>Training labels.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing labels.</p> int, default=5 <p>Number of cross-validation folds used for learning and validation curve plotting.</p> dict, optional <p>Dictionary of hyperparameter names mapped to value lists (e.g., {'max_depth': [2, 4, 6]}). Used for plotting validation curves.</p> str, default='accuracy' <p>Scoring metric used during learning and validation curve generation.</p> bool, default=True <p>If True, prints key metrics, classification reports, and shows plots.</p> bool, default=False <p>If True, returns a dictionary containing evaluation metrics and confusion matrix.</p> bool, default=False <p>If True, returns only the trained model. Ignores all metric outputs.</p> bool, default=False <p>If True and return_dict=True, includes the trained model in the output dictionary.</p> list of str, optional <p>Additional diagnostic plots to generate. Options:     - \"calibration\": Reliability plots per class (One-vs-Rest format)</p> float, optional <p>Fraction of the test set to use during evaluation (e.g., 0.1 = 10%).</p> int, optional <p>Absolute number of rows to use from the test set.</p> bool, default=False <p>If True, disables all visualizations and print outputs for fast evaluation.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Fitted model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>return_dict=True</code> and <code>export_model=True</code></p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--raises","title":"Raises","text":"<p>ValueError     If required inputs are missing or invalid combinations of sample parameters are specified.</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--examples","title":"Examples","text":"<p>evaluate_multiclass_classification(         model=clf,         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         validation_params={'max_depth': [2, 4, 6]},         scoring_curve='f1_macro',         extra_plots=['calibration'],         return_dict=True     )</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--return-only-the-trained-model","title":"Return only the trained model","text":"<p>clf = evaluate_multiclass_classification(         model=RandomForestClassifier(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         return_model_only=True     )</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--use-pretrained-model-for-diagnostics","title":"Use pretrained model for diagnostics","text":"<p>from joblib import load model = load(\"clf.joblib\") evaluate_multiclass_classification(model=model, X_test=Xt, y_test=yt)</p>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You have a multiclass classifier (trained or not) and want a complete diagnostic report.     \u2022 You need reliable evaluation across multiple classes using macro-averaged metrics.     \u2022 You want visual feedback to compare models, understand performance, or debug issues.     \u2022 You want exportable results for dashboards, model cards, or batch audit pipelines.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 Accuracy:         \u2192 Overall proportion of correct predictions.         \u2192 Suitable for balanced class distributions.</p> <pre><code>\u2022 Macro Precision:\n    \u2192 Average precision across all classes.\n    \u2192 Treats all classes equally, regardless of support.\n\n\u2022 Macro Recall:\n    \u2192 Average recall across all classes.\n    \u2192 Useful when missing any class matters equally.\n\n\u2022 Macro F1 Score:\n    \u2192 Harmonic mean of macro precision and recall.\n    \u2192 Good for imbalanced multiclass classification.\n\n\u2022 ROC AUC (OVR):\n    \u2192 One-vs-Rest AUC computed across all classes.\n    \u2192 Reflects model's ability to separate each class from the rest.\n</code></pre> <p>\ud83d\udcc8 Optional Diagnostic Plots:     \u2022 Calibration Curve (<code>extra_plots=['calibration']</code>)         \u2192 For each class, compares predicted probabilities vs. true outcomes in One-vs-Rest fashion.         \u2192 Helps assess if predicted probabilities are well-calibrated.</p> <pre><code>\u2022 Learning Curve:\n    \u2192 Plots training/validation performance as a function of training set size.\n    \u2192 Helps detect underfitting, overfitting, and whether more data may help.\n\n\u2022 Validation Curve (`validation_params={'max_depth': [...]}`)\n    \u2192 Shows how model performance changes with different values of one hyperparameter.\n    \u2192 Useful for tuning complexity (e.g., tree depth, number of estimators).\n</code></pre> <p>\u26a0 Binary-Only Features Removed:     \u2022 Threshold tuning curves, KS statistic, DET curve, lift curve, and cost-sensitive loss are not supported       in multiclass context due to their reliance on binary decision boundaries.</p> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skips all visual output and logs. Great for large-scale evaluations or scripts.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Quickly fit and retrieve a model from the evaluation process.\n\n\u2022 export_model=True:\n    \u2192 Use in pipelines to return both evaluation metrics and model object in one step.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Useful when working with large datasets. Enables quick prototyping or evaluation subsets.\n</code></pre>"},{"location":"modeling/multiclass-classification/#batwing_ml.evaluate_multiclass_classification.evaluate_multiclass_classification--see-also","title":"See Also","text":"<ul> <li>preprocess_dataframe() : For feature cleaning and encoding</li> <li>run_nested_cv_classification() : For nested cross-validation and model comparison</li> <li>summary_dataframe(), summary_column() : For EDA and column profiling</li> <li>evaluate_classification_model() : Binary classification version</li> </ul>"},{"location":"modeling/regression/","title":"Regression","text":"<p>Support for regression problems (continuous targets): tuning, nested cross-validation, and rich diagnostics.</p> <p>These utilities use your preprocessed / engineered <code>X</code> and numeric target <code>y</code>.</p>"},{"location":"modeling/regression/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<pre><code>from batwing_ml import hyperparameter_tuning_regression\n</code></pre>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression","title":"<code>hyperparameter_tuning_regression(X=None, y=None, model_class=None, param_grid=None, scoring='r2', n_trials=50, cv_folds=5, direction='maximize', verbose=True, return_model=True, random_state=42, use_fraction=None, use_n_samples=None, fast_mode=False)</code>","text":"<p>\ud83d\udd27 Hyperparameter Tuning for Regression (via Optuna)</p> <p>Optimize hyperparameters of any regression model using Optuna\u2019s efficient  search. This function supports K-fold cross-validation, flexible scoring, sampling controls, reproducibility, and optionally returns the best fitted model.</p> <p>Ideal for leaderboard-style tuning, pipeline integration, and experimentation across research or production ML environments.</p>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix with shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Target vector of regression outputs.</p> callable <p>A scikit-learn-style regressor class (e.g., <code>RandomForestRegressor</code>, <code>SVR</code>). Not an instance \u2013 must be the class itself.</p> dict <p>Dictionary mapping hyperparameter names to Optuna sampling functions. Example:     {         \"alpha\": lambda t: t.suggest_float(\"alpha\", 0.001, 10, log=True),         \"fit_intercept\": lambda t: t.suggest_categorical(\"fit_intercept\", [True, False])     }</p> str or callable, default='r2' <p>Scoring metric to optimize. Supports: - Any sklearn string (e.g., 'neg_root_mean_squared_error', 'r2', 'neg_mean_absolute_error') - A custom scoring function with signature: scorer(estimator, X_val, y_val)</p> int, default=50 <p>Number of Optuna optimization trials.</p> int, default=5 <p>Number of folds in K-Fold cross-validation for each trial.</p> {'maximize', 'minimize'}, default='maximize' <p>Optimization goal \u2014 e.g., maximize R\u00b2, or minimize RMSE.</p> bool, default=True <p>If True, logs model name, metric, best results, and parameter table.</p> bool, default=True <p>If True, trains and returns the best model on the entire dataset.</p> int, default=42 <p>Seed for reproducible splits and optimization behavior.</p> float or None, optional <p>If set, randomly samples a fraction of the data (e.g., 0.2 = 20%).</p> int or None, optional <p>If set, samples up to a fixed number of rows (e.g., 10000). Applied after <code>use_fraction</code>.</p> bool, default=False <p>If True, reduces number of trials to 10, disables print logs, and speeds up execution.</p>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--returns","title":"Returns","text":"<p>dict     Contains:     - 'best_score' : float     - 'best_params' : dict     - 'study' : optuna.Study     - 'best_model' : fitted model (if return_model=True)</p>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--raises","title":"Raises","text":"<p>ValueError     If scoring is neither a valid string nor a callable function.</p>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use: - You're optimizing regression models across different parameter sets. - You want to avoid grid search overhead with smarter sampling. - You\u2019re working with large datasets or want reproducible trials. - You prefer flexible, callable-based metric optimization.</p> <p>\ud83d\udccc Key Components:</p> <p>\u2022 param_grid:     Define hyperparameter ranges using Optuna\u2019s trial suggestions \u2014 more expressive than traditional grids.     Examples: <code>suggest_float</code>, <code>suggest_int</code>, <code>suggest_categorical</code>, <code>suggest_loguniform</code>.</p> <p>\u2022 scoring:     Use built-in sklearn scorers like 'r2', 'neg_root_mean_squared_error', or define your own function.</p> <p>\u2022 Subsampling:     Use <code>use_fraction</code> and/or <code>use_n_samples</code> to downsample large datasets while tuning.</p> <p>\u2022 fast_mode:     Cuts down <code>n_trials</code> to 10 and disables verbose printing. Great for initial testing.</p> <p>\ud83e\uddea Example Usage:</p> <p>from sklearn.ensemble import GradientBoostingRegressor results = hyperparameter_tuning_regression(         X=X, y=y,         model_class=GradientBoostingRegressor,         param_grid={             \"n_estimators\": lambda t: t.suggest_int(\"n_estimators\", 50, 300),             \"max_depth\": lambda t: t.suggest_int(\"max_depth\", 3, 10),             \"learning_rate\": lambda t: t.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),         },         scoring='neg_root_mean_squared_error',         fast_mode=True,         return_model=True     )</p> <p>print(results[\"best_params\"]) print(results[\"best_score\"]) model = results[\"best_model\"]</p> <p>\ud83d\udca1 Tips: - Use log-scale for continuous values (e.g., learning rates, regularization strength). - Prefer <code>neg_root_mean_squared_error</code> or <code>neg_mean_absolute_error</code> for cost-aware regression. - Use <code>r2</code> when model interpretability or variance explanation is the goal.</p>"},{"location":"modeling/regression/#batwing_ml.hyperparameter_tuning_regression.hyperparameter_tuning_regression--see-also","title":"See Also","text":"<p>evaluate_regression_model : For full regression metrics and plots after training. Optuna : https://optuna.org GridSearchCV : Classic alternative using exhaustive search.</p>"},{"location":"modeling/regression/#nested-cross-validation","title":"Nested cross-validation","text":"<pre><code>from batwing_ml import run_nested_cv_regression\n</code></pre> <p>\ud83d\udd01 Nested Cross-Validation for Regression Models with Hyperparameter Tuning and Scoring Diagnostics</p> <p>Evaluate multiple regression models using nested cross-validation with internal hyperparameter tuning.  Supports both GridSearchCV and RandomizedSearchCV, optional scaling, rich or tabulate logging styles,  metric customization, sampling controls, and visual diagnostics. Useful for benchmarking, model selection,  and automated experimentation.</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--parameters","title":"Parameters","text":"<p>X : pd.DataFrame or np.ndarray     Feature matrix of shape (n_samples, n_features).</p> pd.Series or np.ndarray <p>Regression target values of shape (n_samples,).</p> dict of str \u2192 estimator <p>Dictionary mapping model names to scikit-learn-compatible regressors.</p> dict of str \u2192 dict <p>Dictionary mapping model names to hyperparameter grids (sklearn format).</p> list of str or callables, default=['r2'] <p>List of scoring metrics to evaluate (e.g., 'r2', 'rmse', 'mae', or custom functions).</p> int, default=3 <p>Number of outer CV folds used for unbiased performance estimation.</p> int, default=3 <p>Number of inner CV folds used for hyperparameter tuning.</p> int, default=42 <p>Seed for reproducibility across cross-validation and sampling.</p> callable, default=lambda name: name not in ['random_forest', 'gboost'] <p>Function to determine whether a model should use standard scaling.</p> sklearn-compatible transformer or Pipeline, optional <p>Custom preprocessing steps. Overrides automatic scaling if provided.</p> {'grid', 'random'}, default='grid' <ul> <li>'grid': exhaustive grid search.</li> <li>'random': randomized search using <code>n_iter</code> samples from the grid.</li> </ul> int, default=10 <p>Number of iterations for random search (used only when <code>search_method='random'</code>).</p> float, optional <p>If provided, randomly samples a fraction of the data (e.g., 0.1 = 10%).</p> int, optional <p>If provided, limits total training size to this number of rows.</p> bool, default=False <p>If True, reduces CV folds and skips visuals/logs for faster experimentation.</p> bool, default=True <p>Whether to print progress, best parameters, and summary tables.</p> bool, default=False <p>If True, returns a dictionary with detailed results and summary plots.</p> {'tabulate', 'rich'}, default='tabulate' <p>Formatting style for printed summaries (console-friendly or rich-colored).</p> bool, default=True <p>If True, generates metric comparison plots using matplotlib/seaborn.</p> bool, default=False <p>If True, normalizes metric scales for side-by-side comparison across metrics.</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--returns","title":"Returns","text":"<p>dict, optional     Only if <code>return_results=True</code>:     - 'summary': pd.DataFrame summarizing all models and metrics     - 'results': full nested CV scores and best params     - 'best_params': dictionary of best params for each model/metric     - 'figures': matplotlib figure objects (if plots were generated)</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--raises","title":"Raises","text":"<p>ValueError     If required inputs (X, y, model_dict, param_grids) are missing.</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--examples","title":"Examples","text":"<p>models = {         'linear': LinearRegression(),         'rf': RandomForestRegressor()     }</p> <p>grids = {         'linear': {},         'rf': {'n_estimators': [50, 100], 'max_depth': [3, 5]}     }</p> <p>run_nested_cv_regression(         X=X, y=y,         model_dict=models,         param_grids=grids,         scoring_list=['r2', 'rmse', 'mae'],         search_method='random',         n_iter=5,         fast_mode=True,         show_plots=False     )</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--notes","title":"Notes","text":"<p>\ud83d\udd0d Scoring Options:     \u2022 Standard metrics: 'r2', 'adjusted_r2', 'mae', 'rmse', 'rmsle', 'mape'     \u2022 Custom scorers: pass a callable with (y_true, y_pred) \u2192 float     \u2022 Negative scores (e.g., RMSE) are automatically converted to positive.</p> <p>\ud83d\udcc8 Visualization:     - Score plots (e.g., R\u00b2, explained variance) are grouped separately from error plots (e.g., RMSE, MAE).     - Use <code>normalize=True</code> to enable relative comparison across all metrics in a single chart.</p> <p>\u26a1 Tips:     - Use <code>sample_frac</code> or <code>max_samples</code> for speed on large datasets.     - <code>fast_mode=True</code> is useful for test runs or CI/CD pipelines.     - Use <code>return_results=True</code> to integrate output into reports or notebooks.</p>"},{"location":"modeling/regression/#batwing_ml.run_nested_cv_regression--see-also","title":"See Also","text":"<ul> <li>run_nested_cv_classification : For binary/multiclass classification</li> <li>evaluate_regression_model : For detailed regression diagnostics</li> <li>GridSearchCV, RandomizedSearchCV : Sklearn tuning tools</li> </ul>"},{"location":"modeling/regression/#evaluation","title":"Evaluation","text":"<pre><code>from batwing_ml import evaluate_regression_model\n</code></pre> <p>Evaluate a regression model with metrics, diagnostics, visualization, and export handling.</p> <p>This function evaluates a scikit-learn-compatible regressor using a variety of performance metrics. It supports trained and untrained models, optionally fits them, and generates plots to analyze model performance, including learning curves, residuals, and error distributions.</p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--parameters","title":"Parameters","text":"<p>model : estimator object, optional     A scikit-learn-compatible regressor. If unfitted, the function will train it using provided data.</p> array-like, optional <p>Training feature matrix. Required if model is not already fitted.</p> array-like, optional <p>Training target vector.</p> array-like, optional <p>Testing feature matrix.</p> array-like, optional <p>Testing target vector.</p> int, default=5 <p>Number of cross-validation folds used for learning and validation curves.</p> dict, optional <p>Dictionary of hyperparameter names to lists of values for validation curve plotting.</p> str, default='r2' <p>Scoring metric used for the learning and validation curves.</p> bool, default=True <p>If True, prints all metrics and visualizations.</p> bool, default=False <p>If True, returns computed metrics in a dictionary.</p> bool, default=False <p>If True, returns only the trained model, suppressing all metrics and plots.</p> bool, default=False <p>If True, returns a tuple of (metrics_dict, trained_model).</p> list of str, optional <p>Diagnostic visualizations to generate. Supported values:     - 'pred_vs_actual': Predicted vs Actual scatterplot     - 'residuals': Residuals vs Fitted plot     - 'error_dist': Histogram of prediction errors     - 'qq': Q-Q plot of residuals     - 'feature_importance': Feature importance bar plot (if supported)     - 'learning': Learning curve plot     - 'validation': Validation curve(s) for specified parameters</p> dict, optional <p>Dictionary of user-defined metric functions with format: {name: callable(y_true, y_pred)}.</p> bool, default=False <p>If True, log1p-transform both <code>y_test</code> and predictions before metric computation.</p> bool, default=False <p>If True, disables visual output and reduces verbosity for speed. Ideal for batch evaluations.</p> float, optional <p>If set, uses a random subset of the test data by fraction (e.g., 0.1 = 10%).</p> int, optional <p>If set, uses only the specified number of rows from the test data.</p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--returns","title":"Returns","text":"<p>dict, estimator, or tuple     - Dictionary of metrics if <code>return_dict=True</code>     - Trained model if <code>return_model_only=True</code>     - (metrics_dict, model) if <code>export_model=True</code></p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--raises","title":"Raises","text":"<p>ValueError     If both <code>sample_fraction</code> and <code>sample_size</code> are specified simultaneously.</p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--examples","title":"Examples","text":"<p>evaluate_regression_model(         model=LinearRegression(),         X_train=X_train, y_train=y_train,         X_test=X_test, y_test=y_test,         extra_plots=['pred_vs_actual', 'residuals', 'learning'],         return_dict=True     )</p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--save-and-export-model","title":"Save and export model","text":"<p>metrics, trained_model = evaluate_regression_model(         model=RandomForestRegressor(),         X_train=X, y_train=y,         X_test=Xt, y_test=yt,         export_model=True     )</p>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--user-guide","title":"User Guide","text":"<p>\ud83e\udde0 When to Use This Function:     \u2022 You want a comprehensive evaluation of a regression model with minimal boilerplate.     \u2022 You need insight into prediction accuracy, residuals, and learning behavior.     \u2022 You're experimenting with different models and want standardized reports.     \u2022 You want to benchmark models quickly on subsets of large datasets.     \u2022 You wish to export trained models alongside their evaluation scores.</p> <p>\ud83d\udcca Core Metrics Explained:     \u2022 R\u00b2 (R-squared):         \u2192 Proportion of variance in target explained by the model.         \u2192 Values closer to 1 indicate better performance.</p> <pre><code>\u2022 Adjusted R\u00b2:\n    \u2192 Penalized R\u00b2 for the number of predictors used. More robust to overfitting.\n\n\u2022 RMSE (Root Mean Squared Error):\n    \u2192 Measures average magnitude of error. Sensitive to outliers.\n\n\u2022 MAE (Mean Absolute Error):\n    \u2192 Measures average absolute deviation. Robust and interpretable.\n\n\u2022 MAPE (Mean Absolute Percentage Error):\n    \u2192 Scales MAE by the magnitude of the true value. Not defined for zero targets.\n\n\u2022 RMSLE (Root Mean Squared Log Error):\n    \u2192 Useful when dealing with targets across several orders of magnitude. Ignores underestimation penalties.\n\n\u2022 Custom Metrics:\n    \u2192 Pass any function with signature `func(y_true, y_pred)` for domain-specific scoring.\n</code></pre> <p>\ud83d\udcc8 Diagnostic Visualizations (enabled via <code>extra_plots</code>):     \u2022 Predicted vs Actual:         \u2192 Shows how closely predictions align with ground truth.</p> <pre><code>\u2022 Residual Plot:\n    \u2192 Detects heteroskedasticity or model bias across fitted values.\n\n\u2022 Error Distribution:\n    \u2192 Histogram of residuals, useful for checking skew.\n\n\u2022 Q-Q Plot:\n    \u2192 Checks if residuals follow normal distribution (key assumption in linear models).\n\n\u2022 Feature Importance:\n    \u2192 For tree models, displays relative contribution of each feature.\n\n\u2022 Learning Curve:\n    \u2192 Shows training vs validation performance across increasing sample sizes.\n\n\u2022 Validation Curve:\n    \u2192 Visualizes sensitivity to specific hyperparameters.\n</code></pre> <p>\u2699 Runtime &amp; Usability Tips:     \u2022 fast_mode=True:         \u2192 Skips all visual output and logging. Best for CI jobs or looped experimentation.</p> <pre><code>\u2022 return_model_only=True:\n    \u2192 Returns the fitted model only (no metrics or plots). Handy for pipelines.\n\n\u2022 export_model=True:\n    \u2192 Returns a tuple of (metrics_dict, trained_model) for downstream use.\n\n\u2022 sample_fraction / sample_size:\n    \u2192 Great for fast prototyping or large dataset downsampling.\n</code></pre>"},{"location":"modeling/regression/#batwing_ml.evaluate_regression_model--see-also","title":"See Also","text":"<ul> <li>sklearn.metrics : Reference for all built-in scoring functions</li> <li>run_nested_cv_regression : For comparing multiple regression models</li> <li>summary_dataframe(), preprocess_dataframe() : For EDA &amp; preprocessing utilities</li> </ul>"}]}